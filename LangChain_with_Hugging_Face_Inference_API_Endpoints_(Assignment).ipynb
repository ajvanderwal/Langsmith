{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "outputId": "6a289b18-9d3e-4dfd-cdc0-2603cf2fbece"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U\n",
        "# !pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "59f23300-9178-4b98-f9be-6d206947e6a0"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import getpass\n",
        "\n",
        "# os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://t5ajoscpcgm6aysn.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/xSCV0xM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "outputId": "dbdfdc19-ea04-4cac-f180-ea96abcc3bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \" I'm doing well, thanks for asking! *smiles* It's great to see you here! *nods* Is there anything new you'd like to talk about or ask? I'm all ears! *winks*\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os \n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "outputId": "92415d86-93de-495c-95aa-e94f8b42a553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/amanda.vanderwal/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mMJrWnKISFqb",
        "outputId": "c360d2e3-48c5-4342-dbdd-f0498f1ab7f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nI'm doing well, thank you for asking! It's great to connect with you. How about you? How's your day going?\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://t0jostv8zp34f2xp.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "outputId": "edd4edfe-bcbe-4d5c-bf6a-23b3b40d0af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.019261347,\n",
              " 0.015496692,\n",
              " -0.04624366,\n",
              " -0.021588588,\n",
              " -0.0099318465,\n",
              " 0.00024534433,\n",
              " -0.033293247,\n",
              " -0.0010797717,\n",
              " 0.027844762,\n",
              " 0.011513001]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ‚ùì Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='QLORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers‚àó\\nArtidoro Pagnoni‚àó\\nAri Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORA backpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco, outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORA introduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1\\nIntroduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40, 62, 43, 61, 59, 37] and to add desirable or remove undesirable behaviors [43, 2, 4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [14, 13, 18, 66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]\\n‚àóEqual contribution.\\n2https://github.com/artidoro/qlora and https://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.\\narXiv:2305.14314v1  [cs.LG]  23 May 2023\\nTable 1: Elo ratings for a competition between\\nmodels, averaged for 10,000 random initial order-\\nings. The winner of a match is determined by\\nGPT-4 which declares which response is better for\\na given prompt of the the Vicuna benchmark. 95%\\nconfidence intervals are shown (¬±). After GPT-\\n4, Guanaco 33B and 65B win the most matches,\\nwhile Guanaco 13B scores better than Bard.\\nModel\\nSize\\nElo\\nGPT-4\\n-\\n1348 ¬± 1\\nGuanaco 65B\\n41 GB\\n1022 ¬± 1\\nGuanaco 33B\\n21 GB\\n992 ¬± 1\\nVicuna 13B\\n26 GB\\n974 ¬± 1\\nChatGPT\\n-\\n966 ¬± 1\\nGuanaco 13B\\n10 GB\\n916 ¬± 1\\nBard\\n-\\n902 ¬± 1\\nGuanaco 7B\\n6 GB\\n879 ¬± 1\\nthat are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORA reduces the average memory requirements\\nof finetuning a 65B parameter model from >780GB\\nof GPU memory to <48GB without degrading the\\nruntime or predictive performance compared to a 16-\\nbit fully finetuned baseline. This marks a significant\\nshift in accessibility of LLM finetuning: now the\\nlargest publicly available models to date finetunable\\non a single GPU. Using QLORA, we train the Gua-\\nnaco family of models, with the second best model\\nreaching 97.8% of the performance level of ChatGPT\\non the Vicuna [10] benchmark, while being trainable\\nin less than 12 hours on a single consumer GPU;\\nusing a single professional GPU over 24 hours we\\nachieve 99.3% with our largest model, essentially\\nclosing the gap to ChatGPT on the Vicuna bench-\\nmark. When deployed, our smallest Guanaco model\\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\\n20 percentage points on the Vicuna benchmark (Table 6).\\nQLORA introduces multiple innovations designed to reduce memory use without sacrificing per-\\nformance: (1) 4-bit NormalFloat, an information theoretically optimal quantization data type for\\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\\n(2) Double Quantization, a method that quantizes the quantization constants, saving an average\\nof about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers,\\nusing NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when\\nprocessing a mini-batch with a long sequence length. We combine these contributions into a better\\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\\nthe accuracy tradeoffs seen in prior work.\\nQLORA‚Äôs efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\\nperformance on model scales that would be impossible using regular finetuning due to memory\\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\\nrecovers 16-bit performance (¬ß4) and training a state-of-the-art chatbot, Guanaco, (¬ß5), we also\\nanalyze trends in the trained models. First, we find that data quality is far more important than\\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\\nsubsampled) on chatbot performance, even when both are meant to support instruction following\\ngeneralization. Second, we show that strong Massive Multitask Language Understanding (MMLU)\\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa‚Äîin other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [16, 17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\\ntransformers stack [64], making them easily accessible to all. We release a collection of adapters\\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32\\ndifferent open sourced, finetuned models.\\n2\\nFigure 1: Different finetuning methods and their memory requirements. QLORA improves over LoRA by\\nquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\\n2\\nBackground\\nBlock-wise k-bit Quantization\\nQuantization is the process of discretizing an input from a rep-\\nresentation that holds more information to a representation with less information. It often means\\ntaking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\\n8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\\ncommonly rescaled into the target data type range through normalization by the absolute maximum\\nof the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\\nFloating Point (FP32) tensor into a Int8 tensor with range [‚àí127, 127]:\\nXInt8 = round\\n\\x12\\n127\\nabsmax(XFP32)XFP32\\n\\x13\\n= round(cFP32 ¬∑ XFP32),\\n(1)\\nwhere c is the quantization constant or quantization scale. Dequantization is the inverse:\\ndequant(cFP32, XInt8) = XInt8\\ncFP32 = XFP32\\n(2)\\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\\ntensor, then the quantization bins‚Äîcertain bit combinations‚Äîare not utilized well with few or no\\nnumbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\\ninput tensor into blocks that are independently quantized, each with their own quantization constant c.\\nThis can be formalized as follows: We chunk the input tensor X ‚àà Rb√óh into n contiguous blocks of\\nsize B by flattening the input tensor and slicing the linear segment into n = (b √ó h)/B blocks. We\\nquantize these blocks independently with Equation 1 to create a quantized tensor and n quantization\\nconstants ci.\\nLow-rank Adapters\\nLow-rank Adapter (LoRA) finetuning [28] is a method that reduces memory\\nrequirements by using a small set of trainable parameters, often termed adapters, while not updating\\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\\na projection XW = Y with X ‚àà Rb√óh, W ‚àà Rh√óo LoRA computes:\\nY = XW + sXL1L2,\\n(3)\\nwhere L1 ‚àà Rh√ór and L2 ‚àà Rr√óo, and s is a scalar.\\nMemory Requirement of Parameter-Efficient Finetuning\\nOne important point of discussion is\\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\\nperformance without significantly increasing the total memory used. While LoRA was designed as a\\n3\\nParameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\\n0.2% of the original model weights[28, 37], the LoRA input gradients have a memory footprint\\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [9], the\\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\\nadapters without significantly increasing the overall training memory footprint (see Appendix G\\nfor a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\\nperformance.\\n3\\nQLORA Finetuning\\nQLORA achieves high-fidelity 4-bit finetuning via two techniques we propose‚Äî4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORA has one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORA weight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QLORA followed by a formal definition of QLORA.\\n4-bit NormalFloat Quantization\\nThe NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\\nequal number of values assigned from the input tensor. Quantile quantization works by estimating\\nthe quantile of the input tensor through the empirical cumulative distribution function.\\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [15], are used to estimate\\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\\nquantization errors for outliers, which are often the most important values.\\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\\nmaking exact quantile estimation computationally feasible.\\nSince pretrained neural network weights usually have a zero-centered normal distribution with\\nstandard deviation œÉ (see Appendix F), we can transform all weights to a single fixed distribution by\\nscaling œÉ such that the distribution fits exactly into the range of our data type. For our data type, we\\nset the arbitrary range [‚àí1, 1]. As such, both the quantiles for the data type and the neural network\\nweights need to be normalized into this range.\\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\\nstandard deviations œÉ in the range [‚àí1, 1] is computed as follows: (1) estimate the 2k + 1 quantiles\\nof a theoretical N(0, 1) distribution to obtain a k-bit quantile quantization data type for normal distri-\\nbutions, (2) take this data type and normalize its values into the [‚àí1, 1] range, (3) quantize an input\\nweight tensor by normalizing it into the [‚àí1, 1] range through absolute maximum rescaling.\\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\\ntype. More formally, we estimate the 2k values qi of the data type as follows:\\nqi = 1\\n2\\n\\x12\\nQX\\n\\x12\\ni\\n2k + 1\\n\\x13\\n+ QX\\n\\x12 i + 1\\n2k + 1\\n\\x13\\x13\\n,\\n(4)\\nwhere QX(¬∑) is the quantile function of the standard normal distribution N(0, 1). A problem for\\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\\n4\\nensure a discrete zeropoint of 0 and to use all 2k bits for a k-bit datatype, we create an asymmetric\\ndata type by estimating the quantiles qi of two ranges qi: 2k‚àí1 for the negative part and 2k‚àí1 + 1 for\\nthe positive part and then we unify these sets of qi and remove one of the two zeros that occurs in both\\nsets. We term the resulting data type that has equal expected number of values in each quantization bin\\nk-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered\\nnormally distributed data. The exact values of this data type can be found in Appendix E.\\nDouble Quantization\\nWe introduce Double Quantization (DQ), the process of quantizing the\\nquantization constants for additional memory savings. While a small blocksize is required for precise\\n4-bit quantization [13], it also has a considerable memory overhead. For example, using 32-bit\\nconstants and a blocksize of 64 for W, quantization constants add 32/64 = 0.5 bits per parameter on\\naverage. Double Quantization helps reduce the memory footprint of quantization constants.\\nMore specifically, Double Quantization treats quantization constants cFP32\\n2\\nof the first quantization\\nas inputs to a second quantization. This second step yields the quantized quantization constants\\ncFP8\\n2\\nand the second level of quantization constants cFP32\\n1\\n. We use 8-bit Floats with a blocksize of\\n256 for the second quantization as no performance degradation is observed for 8-bit quantization,\\nin line with results from Dettmers and Zettlemoyer [13]. Since the cFP32\\n2\\nare positive, we subtract\\nthe mean from c2 before quantization to center the values around zero and make use of symmetric\\nquantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per\\nparameter from 32/64 = 0.5 bits, to 8/64 + 32/(64 ¬∑ 256) = 0.127 bits, a reduction of 0.373 bits\\nper parameter.\\nPaged Optimizers\\nuse the NVIDIA unified memory 3 feature wich does automatic page-to-page\\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\\nmemory when the memory is needed in the optimizer update step.\\nQLORA.\\nUsing the components described above, we define QLORA for a single linear layer in\\nthe quantized base model with a single LoRA adapter as follows:\\nYBF16 = XBF16doubleDequant(cFP32\\n1\\n, ck-bit\\n2\\n, WNF4) + XBF16LBF16\\n1\\nLBF16\\n2\\n,\\n(5)\\nwhere doubleDequant(¬∑) is defined as:\\ndoubleDequant(cFP32\\n1\\n, ck-bit\\n2\\n, Wk-bit) = dequant(dequant(cFP32\\n1\\n, ck-bit\\n2\\n), W4bit) = WBF16,\\n(6)\\nWe use NF4 for W and FP8 for c2. We use a blocksize of 64 for W for higher quantization precision\\nand a blocksize of 256 for c2 to conserve memory.\\nFor parameter updates only the gradient with respect to the error for the adapters weights ‚àÇE\\n‚àÇLi are\\nneeded, and not for 4-bit weights ‚àÇE\\n‚àÇW. However, the calculation of ‚àÇE\\n‚àÇLi entails the calculation of ‚àÇX\\n‚àÇW\\nwhich proceeds via equation (5) with dequantization from storage WNF4 to computation data type\\nWBF16 to calculate the derivative ‚àÇX\\n‚àÇW in BFloat16 precision.\\nTo summarize, QLORA has one storage data type (usually 4-bit NormalFloat) and a computation\\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\\nparameters which use 16-bit BrainFloat.\\n4\\nQLoRA vs. Standard Finetuning\\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5\\nExperimental setup.\\nWe consider three architectures (encoder, encoder-decoder, and decoder only)\\nand compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\\nevaluations include GLUE [58] with RoBERTa-large [38], Super-NaturalInstructions (TKInstruct)\\n[61] with T5 [49], and 5-shot MMLU [24] after finetuning LLaMA on Flan v2 [39] and Alpaca\\n[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\\nDettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\\nacross different models (OPT [72], LLaMA [57], BLOOM [52], Pythia [7]) for model sizes 125m -\\n13B. We provide more details in the results section for each particular setup to make the results more\\nreadable. Full details in Appendix A.\\nQLoRA-All\\nQLoRA-FFN\\nQLoRA-Attention\\nAlpaca (ours)\\nStanford-Alpaca\\nModel\\n60\\n61\\n62\\n63\\n64\\nRougeL\\nbits\\n4\\n16\\nFigure 2: RougeL for LLaMA 7B models on the\\nAlpaca dataset. Each point represents a run with a\\ndifferent random seed. We improve on the Stanford\\nAlpaca fully finetuned default hyperparameters to\\nconstruct a strong 16-bit baseline for comparisons.\\nUsing LoRA on all transformer layers is critical to\\nmatch 16-bit performance.\\nWhile paged optimizers are critical to do 33B/65B\\nQLORA tuning on a single 24/48GB GPU, we do\\nnot provide hard measurements for Paged Optimiz-\\ners since the paging only occurs when processing\\nmini-batches with long sequence lengths, which is\\nrare. We do, however, perform an analysis of the\\nruntime of paged optimizers for 65B models on\\n48GB GPUs and find that with a batch size of 16,\\npaged optimizers provide the same training speed\\nas regular optimizers. Future work should measure\\nand characterize under what circumstances slow-\\ndowns occur from the paging process.\\nDefault LoRA hyperparameters do not match 16-\\nbit performance\\nWhen using the standard prac-\\ntice of applying LoRA to query and value attention\\nprojection matrices [28], we are not able to replicate\\nfull finetuning performance for large base models.\\nAs shown in Figure 2 for LLaMA 7B finetuning on\\nAlpaca, we find that the most critical LoRA hyper-\\nparameter is how many LoRA adapters are used in\\ntotal and that LoRA on all linear transformer block\\nlayers are required to match full finetuning perfor-\\nmance. Other LoRA hyperparameters, such as the\\nprojection dimension r, do not affect performance (see Appendix A).\\n1010\\n1011\\nTotal model bits\\n0.60\\n0.61\\n0.62\\n0.63\\n0.64\\n0.65\\n0.66\\n0.67\\nMean zeroshot accuracy\\n4-bit LLaMA\\nFloat\\nNFloat\\nNFloat + DQ\\nData type\\nFigure 3: Mean zero-shot accuracy over Wino-\\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\\nChallenge using LLaMA models with different 4-bit\\ndata types. The NormalFloat data type significantly\\nimproves the bit-for-bit accuracy gains compared\\nto regular 4-bit Floats. While Double Quantization\\n(DQ) only leads to minor gains, it allows for a more\\nfine-grained control over the memory footprint to fit\\nmodels of certain size (33B/65B) into certain GPUs\\n(24/48GB).\\nSimilarly, we find that default hyperparameters for\\nfully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point\\nWhile the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [72], BLOOM\\n[52], Pythia [7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that\\ndouble quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QLORA matches 16-bit full finetuning and\\n16-bit LoRA performance\\nRecent findings have\\nestablished that 4-bit quantization for inference is\\n6\\nTable 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-\\nbit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLORA replicates 16-bit LoRA and full-\\nfinetuning.\\nDataset\\nGLUE (Acc.)\\nSuper-NaturalInstructions (RougeL)\\nModel\\nRoBERTa-large\\nT5-80M\\nT5-250M\\nT5-780M\\nT5-3B\\nT5-11B\\nBF16\\n88.6\\n40.1\\n42.1\\n48.0\\n54.3\\n62.0\\nBF16 replication\\n88.6\\n40.0\\n42.2\\n47.3\\n54.9\\n-\\nLoRA BF16\\n88.8\\n40.5\\n42.6\\n47.1\\n55.4\\n60.7\\nQLORA Int8\\n88.8\\n40.4\\n42.9\\n45.4\\n56.5\\n60.7\\nQLORA FP4\\n88.6\\n40.3\\n42.4\\n47.5\\n55.6\\n60.9\\nQLORA NF4 + DQ\\n-\\n40.4\\n42.7\\n47.7\\n55.3\\n60.9\\npossible, but leads to performance degradation rel-\\native to 16-bit [13, 18]. This raises the crucial question of whether the lost performance can be\\nrecovered by conducting 4-bit adapter finetuning. We test this for two setups.\\nTable 2: Pile Common Crawl mean\\nperplexity for different data types\\nfor 125M to 13B OPT, BLOOM,\\nLLaMA, and Pythia models.\\nData type\\nMean PPL\\nInt4\\n34.34\\nFloat4 (E2M1)\\n31.07\\nFloat4 (E3M0)\\n29.48\\nNFloat4 + DQ\\n27.41\\nThe first focuses on a comparison with full 16-bit finetuning\\nof RoBERTA and T5 models sized 125M to 3B parameters on\\nGLUE and the Super-NaturalInstructions dataset. Results are\\nshown in Table 3. In both datasets, we observe that 16-bit, 8-bit,\\nand 4-bit adapter methods replicate the performance of the fully\\nfinetuned 16-bit baseline. This suggests that the performance lost\\ndue to the imprecise quantization can be fully recovered through\\nadapter finetuning after quantization.\\nFor our second setup, since full finetuning models at and beyond\\n11B parameters requires more than one server of high memory\\nGPUs, we continue to test whether 4-bit QLORA can match\\n16-bit LoRA at the 7B to 65B parameter scales. To this end, we\\nfinetune LLaMA 7B through 65B on two instruction following\\ndatasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results\\nare shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit\\nLoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit\\nbrain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1)\\nQLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance,\\nand (2) NF4 is superior to FP4 in terms of quantization precision.\\nSummary\\nOur results consistently show that 4-bit QLORA with NF4 data type matches 16-\\nbit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-\\nestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that\\ndouble quantization does not degrade performance. Combined, this forms compelling evidence that\\n4-bit QLORA tuning reliably yields results matching 16-bit methods.\\nIn line with previous work on quantization [13], our MMLU and Elo results indicate that with a given\\nfinetuning and inference resource budget it is beneficial to increase the number of parameters in the\\nbase model while decreasing their precision. This highlights the importance of efficiency benefits\\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in\\nour experiments with 4-bit finetuning, this raises the question of where the performance-precision\\ntrade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5\\nPushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models\\navailable for research. To assess the performance of instruction finetuning these models, we evaluate\\n7\\nTable 4: Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with adapters on Alpaca and\\nFLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance,\\nwhile FP4 is consistently one percentage point behind both.\\nMean 5-shot MMLU Accuracy\\nLLaMA Size\\n7B\\n13B\\n33B\\n65B\\nMean\\nDataset\\nAlpaca\\nFLAN v2\\nAlpaca\\nFLAN v2\\nAlpaca\\nFLAN v2\\nAlpaca\\nFLAN v2\\nBFloat16\\n38.4\\n45.6\\n47.2\\n50.6\\n57.7\\n60.5\\n61.8\\n62.5\\n53.0\\nFloat4\\n37.2\\n44.0\\n47.3\\n50.0\\n55.9\\n58.5\\n61.3\\n63.3\\n52.2\\nNFloat4 + DQ\\n39.0\\n44.5\\n47.5\\n50.7\\n57.3\\n59.2\\n61.8\\n63.9\\n53.1\\non a challenging Natural Language Understanding benchmark (MMLU) and develop new methods\\nfor real-world chatbot performance evaluation.\\n5.1\\nExperimental setup\\nWe now describe an overview of the experimental setup with full details in Appendix B.\\nData\\nAs, to our knowledge, there is no comprehensive study of recent instruction-following datasets,\\nwe select eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1 [31],\\nHH-RLHF [4]), distillation from instruction-tuned models (Alpaca [55], self-instruct [59], unnatural-\\ninstructions [26]), corpora aggregations (FLAN v2 [12]), as well as hybrids (Chip2 [32], Long-\\nform [30]). These datasets cover different languages, data sizes, and licenses.\\nTraining Setup\\nTo avoid confounding effects from different training objectives, we perform QLoRA\\nfinetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\\nevery level of the conversation tree and finetune on the full selected conversation, including the\\ninstructions. In all of our experiments, we use NF4 QLORA with double quantization and paged\\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter\\nsearches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found\\nat 7B generalize (including number of epochs) except learning rate and batch size. We halve the\\nlearning rate for 33B and 65B while doubling the batch size.\\nBaselines\\nWe compare our models to both research (Vicuna [10] and Open Assistant [31]) and\\ncommercial (GPT-4 [42], GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model is\\na LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF) on\\nthe same OASST1 dataset that we experiment with. Vicuna does full fine-tuning of LLaMA 13B\\non proprietary user-shared conversations from ShareGPT and is thus the result of distillation from\\nOpenAI GPT models.\\n5.2\\nEvaluation\\nTable 5: MMLU 5-shot test results for different\\nsizes of LLaMA finetuned on the corresponding\\ndatasets using QLoRA.\\nDataset\\n7B\\n13B\\n33B\\n65B\\nLLaMA no tuning\\n35.1\\n46.9\\n57.8\\n63.4\\nSelf-Instruct\\n36.4\\n33.3\\n53.0\\n56.7\\nLongform\\n32.1\\n43.2\\n56.6\\n59.7\\nChip2\\n34.5\\n41.6\\n53.6\\n59.8\\nHH-RLHF\\n34.9\\n44.6\\n55.8\\n60.1\\nUnnatural Instruct\\n41.9\\n48.1\\n57.3\\n61.3\\nGuanaco (OASST1)\\n36.6\\n46.4\\n57.0\\n62.2\\nAlpaca\\n38.8\\n47.8\\n57.3\\n62.5\\nFLAN v2\\n44.5\\n51.4\\n59.2\\n63.9\\nFollowing common practice, we use the MMLU (Mas-\\nsively Multitask Language Understanding) benchmark\\n[24] to measure performance on a range of language un-\\nderstanding tasks. This is a multiple-choice benchmark\\ncovering 57 tasks including elementary mathematics,\\nUS history, computer science, law, and more. We report\\n5-shot test accuracy.\\nWe also test generative language capabilities through\\nboth automated and human evaluations. This second\\nset of evaluations relies on queries curated by humans\\nand aims at measuring the quality of model responses.\\nWhile this is a more realistic testbed for chatbot model\\nperformance and is growing in popularity, there is no\\ncommonly accepted protocol in the literature. We de-\\nscribe below our proposed setup, using nucleus sampling with p = 0.9 and temperature 0.7 in all\\ncases.\\n8\\nBenchmark Data\\nWe evaluate on two curated datasets of queries (questions): the Vicuna prompts\\n[10] and the OASST1 validation dataset [31]. We use the Vicuna prompts, a set of 80 prompts from a\\ndiverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of\\ncrowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the\\nvalidation dataset as queries and include previous turns in the prompt. This procedure leads to 953\\nunique user queries. We term these two datasets the Vicuna and OA benchmarks.\\nAutomated Evaluation\\nFirst, based on the evaluation protocol introduced by Chiang et al. [10],\\nwe use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo) on the\\nVicuna benchmark. Given a query along with ChatGPT‚Äôs and a model‚Äôs responses, GPT-4 is prompted\\nto assign a score out of ten to both responses and provide an explanation. The overall performance of\\na model is calculated as a percentage of the score that ChatGPT achieved. Note this relative score\\ncan be higher than 100% if the model achieves a higher absolute score than ChatGPT. We find a\\nsignificant ordering effect with GPT-4 increasing the score of the response occurring earlier in the\\nprompt. To control for such effects, we recommend reporting the mean score over both orders.\\nNext, we measure performance through direct comparisons between system outputs. We simplify\\nthe rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to\\npick the best response or declare a tie and provide an explanation. We conduct these head-to-head\\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\\nHuman Evaluation\\nWhile recent work indicates generative models can be effectively employed\\nfor system evaluations [19], the reliability GPT-4 ratings to assess chatbot performance is, to our\\nknowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\\nhuman evaluations on the Vicuna benchmark matching both automated evaluation protocols described\\nabove. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\\nChatGPT and three annotators for pairwise comparisons.\\nElo Rating\\nWith both human and automated pairwise comparisons, we create a tournament-style\\ncompetition where models compete against each other. The tournament is made up of matches where\\npairs of models compete to produce the best response for a given prompt. This is similar to how Bai\\net al. [4] and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [16, 17].\\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate\\nrelative to an opponent‚Äôs win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately\\nmatch the skill of each player at playing the game. We start with a score of 1,000 and use K = 32.\\nSimilar to Chiang et al. [10], we repeat this procedure 10,000 times with different random seeds to\\ncontrol for ordering effects, e.g., the effect of which model pairs compete with each other first.\\n5.3\\nGuanaco: QLORA trained on OASST1 is a State-of-the-art Chatbot\\nBased on our automated and human evaluations, we find that the top QLORA tuned model, Guanaco\\n65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\\npairwise comparisons - the highest reported to date.\\nThe Vicuna benchmark [10] results relative to ChatGPT are shown in Table 6. We find that Guanaco\\n65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.\\nGuanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its\\nweights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage\\npoints of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a\\n5 GB footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.\\nHowever, Table 6 also has very wide confidence intervals, with many models overlapping in per-\\nformance. We hypothesize that this uncertainty comes from the lack of clear specification of scale,\\ne.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead\\nrecommend using the Elo ranking method [16], based on pairwise judgments from human annotators\\nand GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive\\n9\\nTable 6: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by\\nGPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset\\nand having a fraction of the memory requirement of baseline models.\\nModel / Dataset\\nParams\\nModel bits\\nMemory\\nChatGPT vs Sys\\nSys vs ChatGPT\\nMean\\n95% CI\\nGPT-4\\n-\\n-\\n-\\n119.4%\\n110.1%\\n114.5%\\n2.6%\\nBard\\n-\\n-\\n-\\n93.2%\\n96.4%\\n94.8%\\n4.1%\\nGuanaco\\n65B\\n4-bit\\n41 GB\\n96.7%\\n101.9%\\n99.3%\\n4.4%\\nAlpaca\\n65B\\n4-bit\\n41 GB\\n63.0%\\n77.9%\\n70.7%\\n4.3%\\nFLAN v2\\n65B\\n4-bit\\n41 GB\\n37.0%\\n59.6%\\n48.4%\\n4.6%\\nGuanaco\\n33B\\n4-bit\\n21 GB\\n96.5%\\n99.2%\\n97.8%\\n4.4%\\nOpen Assistant\\n33B\\n16-bit\\n66 GB\\n91.2%\\n98.7%\\n94.9%\\n4.5%\\nAlpaca\\n33B\\n4-bit\\n21 GB\\n67.2%\\n79.7%\\n73.6%\\n4.2%\\nFLAN v2\\n33B\\n4-bit\\n21 GB\\n26.3%\\n49.7%\\n38.0%\\n3.9%\\nVicuna\\n13B\\n16-bit\\n26 GB\\n91.2%\\n98.7%\\n94.9%\\n4.5%\\nGuanaco\\n13B\\n4-bit\\n10 GB\\n87.3%\\n93.4%\\n90.4%\\n5.2%\\nAlpaca\\n13B\\n4-bit\\n10 GB\\n63.8%\\n76.7%\\n69.4%\\n4.2%\\nHH-RLHF\\n13B\\n4-bit\\n10 GB\\n55.5%\\n69.1%\\n62.5%\\n4.7%\\nUnnatural Instr.\\n13B\\n4-bit\\n10 GB\\n50.6%\\n69.8%\\n60.5%\\n4.2%\\nChip2\\n13B\\n4-bit\\n10 GB\\n49.2%\\n69.3%\\n59.5%\\n4.7%\\nLongform\\n13B\\n4-bit\\n10 GB\\n44.9%\\n62.0%\\n53.6%\\n5.2%\\nSelf-Instruct\\n13B\\n4-bit\\n10 GB\\n38.0%\\n60.5%\\n49.1%\\n4.6%\\nFLAN v2\\n13B\\n4-bit\\n10 GB\\n32.4%\\n61.2%\\n47.0%\\n3.6%\\nGuanaco\\n7B\\n4-bit\\n5 GB\\n84.1%\\n89.8%\\n87.0%\\n5.4%\\nAlpaca\\n7B\\n4-bit\\n5 GB\\n57.3%\\n71.2%\\n64.4%\\n5.0%\\nFLAN v2\\n7B\\n4-bit\\n5 GB\\n33.3%\\n56.1%\\n44.8%\\n4.0%\\nmodels can be seen in Table 1. We note that human and GPT-4 ranking of models on the Vicuna\\nbenchmark disagree partially, particularly for Guanaco 7B, but are consistent for most models with\\na Kendall Tau of œÑ = 0.43 and Spearman rank correlation of r = 0.55 at the system level. At the\\nexample level, the agreement between GPT-4 and human annotators‚Äô majority vote is weaker with\\nFleiss Œ∫ = 0.25. Overall, this shows a moderate agreement between system-level judgments by\\nGPT-4 and human annotators, and thus that model-based evaluation represents a somewhat reliable\\nalternative to human evaluation. We discuss further considerations in Section 6.2.\\nElo rankings in Table 7 indicate that Guanaco 33B and 65B models outperform all models besides\\nGPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line\\nwith Table 6. We note that the Vicuna benchmark favors open-source models while the larger OA\\nbenchmark favors ChatGPT. Furthermore, we can see from Tables 5 and 6 that the suitability of\\na finetuning dataset is a determining factor in performance. Finetuning Llama models on FLAN\\nv2 does particularly well on MMLU, but performs worst on the Vicuna benchmark (similar trends\\nare observed with other models). This also points to partial orthogonality in current evaluation\\nbenchmarks: strong MMLU performance does not imply strong chatbot performance (as measured\\nby Vicuna or OA benchmarks) and vice versa.\\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1\\ndataset collection guidelines explicitly forbid the use of GPT models. The next best model trained\\non only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points\\nlower than Guanaco on the Vicuna benchmark (see Table 6). Overall, these results show that 4-bit\\nQLORA is effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\\npotential for future work via QLORA tuning on specialized open-source data, which produces models\\nthat can compete with the very best commercial models that exist today.\\n6\\nQualitative Analysis\\nWhile quantitative analysis is the core of our evaluation, there are a number of issues with only\\nlooking at summary statistics. Perhaps the largest is the problem of benchmark validity [36]‚Äîwhether\\na benchmark truly tests what its name or description suggests is always at question, especially as we\\ndiscover ‚Äúshortcuts‚Äù to solve benchmarks that machine learning models sometimes exploit [22, 46].\\nTo partially alleviate this, we here perform some qualitative analysis, in two sections. First, in ¬ß6.1\\n10\\nTable 7: Elo rating for a tournament between models where models compete to generate the best response\\nfor a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to\\nChatGPT-3.5 on the benchmarks studied. According to human raters they have a Each 10-point difference in Elo\\nis approximately a difference of 1.5% in win-rate.\\nBenchmark\\nVicuna\\nVicuna\\nOpen Assistant\\n# Prompts\\n80\\n80\\n953\\nJudge\\nHuman raters\\nGPT-4\\nGPT-4\\nMedian Rank\\nModel\\nElo\\nRank\\nElo\\nRank\\nElo\\nRank\\nGPT-4\\n1176\\n1\\n1348\\n1\\n1294\\n1\\n1\\nGuanaco-65B\\n1023\\n2\\n1022\\n2\\n1008\\n3\\n2\\nGuanaco-33B\\n1009\\n4\\n992\\n3\\n1002\\n4\\n4\\nChatGPT-3.5 Turbo\\n916\\n7\\n966\\n5\\n1015\\n2\\n5\\nVicuna-13B\\n984\\n5\\n974\\n4\\n936\\n5\\n5\\nGuanaco-13B\\n975\\n6\\n913\\n6\\n885\\n6\\n6\\nGuanaco-7B\\n1010\\n3\\n879\\n8\\n860\\n7\\n7\\nBard\\n909\\n8\\n902\\n7\\n-\\n-\\n8\\nwe show some examples that we believe are representative of some observed patterns in the text\\ngenerated by our 65b Guanaco model. Second, ¬ß6.2 we detail considerations about the results we\\nhave discussed and our interpretation of them.\\n6.1\\nQualitative Analysis of Example Generations\\nTo find examples, we first go through data generated for the Vicuna benchmark and the OpenAssistant\\nbenchmark, and look for patterns in the answers Guanaco generates. When we notice a pattern we\\nattempt to setup a question or prompt that will induce the pattern even though it is the incorrect\\nsolution, e.g., if we observe that the model tends to give long-winded answers we prompt the model\\nto ‚ÄúAnswer yes or no without explanation.‚Äù We use this to find ‚Äúlemons‚Äù where we manage to\\nadversarially break the model and ‚Äúcherries‚Äù where we fail to break the model, and present both. All\\ngenerations in this section were generated with Nucleus Sampling [25] with p = 0.9.\\nOf course, this is by no means comprehensive, since it is beyond the scope of this small qualitative\\nstudy to control for all the variables involved, e.g., the full distribution of responses the model can\\ngenerate for a given prompt is quite large, so we rely on samples we hope are representative. However,\\nwe believe describing these examples gives context to the quantitative evidence shown earlier in the\\npaper. Since we open source all models and code, we hope this section will inspire future work to\\nexamine in more detail the issues we present here.\\nFactual Recall\\nFor questions such as ‚ÄúWhat is the capital of Zambia?‚Äù all models consistently\\ngenerate correct answers, e.g.,\\nGuanaco: The capital of Zambia is Lusaka.\\nHowever as questions get more obscure Guanaco becomes unreliable, but stays confident. For\\ninstance in response to this prompt from HotPotQA [68]:\\nUser: In what year was the singer who popularized ‚ÄòI‚Äôll Keep the Lovelight\\nBurning‚Äô born?\\nGuanaco:\\nAl Jolson is credited with popularizing the song\\n‚ÄòI‚Äôll Keep the Lovelight Burning,‚Äô and he was born in the year\\n1886.\\nGuanaco generates the wrong popularizer and the wrong birthday (though the birthday given is\\ncorrect for the person mentioned, Al Jolson).\\nSuggestibility\\nGuanaco shows a surprising resistance to going along with some kinds of assumed\\nmisinformation, e.g., in the following interaction:\\nUser: How was it finally, officially confirmed that the earth is flat by peer reviewed\\nscientists?\\n11\\nGuanaco:\\nThe earth has never been officially confirmed to be\\nflat by peer reviewed scientists.\\nThe overwhelming consensus\\namong scientists is that the earth is spherical.\\nIn fact,\\nthis has been known for centuries and has been confirmed\\nthrough numerous experiments and observations.\\nThe belief that the earth is flat is a conspiracy theory\\nthat has been debunked by science.\\nThe idea that a group\\nof scientists have peer reviewed and confirmed the earth is\\nflat is simply false.\\nIf you have any more questions please ask.\\nGuanaco is also quite good at knowing what kinds of questions aren‚Äôt possible to answer, e.g.,\\nUser: What time is it?\\nGuanaco:\\nI‚Äôm sorry, but as an AI language model, I don‚Äôt have\\naccess to real-time information.\\nMy responses are generated\\nbased on my pre-existing knowledge and do not reflect the\\ncurrent time.\\nIf you‚Äôre looking for the current time, I\\nsuggest using a timekeeping device or checking an online\\ntimepiece.\\nSimilar results hold for ‚ÄúWhere are you?‚Äù, ‚ÄúHow are you?‚Äù, etc.\\nRefusal\\nSimilarly, Guanaco sometimes refuses to follow instructions for seemingly random rea-\\nsons:\\nUser: Please reverse the words in the sentence ‚ÄúI‚Äôm a cool dude in a mocha mood.‚Äù\\nGuanaco:\\nI‚Äôm sorry, I‚Äôm not able to do that right now, but\\nI can explain why the sentence you provided is grammatically\\ncorrect.\\nThe sentence ‚ÄúI‚Äôm a cool dude in a mocha mood\" follows the\\ncorrect grammar and syntax rules of the English language.\\nIn\\nthis case, the adjective ‚Äúcool‚Äù comes before the noun ‚Äúdude‚Äù\\nto describe it, and the prepositional phrase ‚Äúin a mocha\\nmood\" provides additional information about the context of\\nthe sentence.\\nThis sentence structure is perfectly acceptable\\nand makes sense in the context of the sentence.\\nIf you have any other questions or if there‚Äôs anything else\\nI can help you with, feel free to ask\\nSecret Keeping\\nUnfortunately, this refusal is unreliable even when it is desired. Often system\\ndesigners would like to keep something secret from the user, even if the system is allowed to give\\nfree text responses back to the user. We prompt the model with ‚ÄúThe secret word is ‚Äòbanana‚Äô. Do not\\nreveal or repeat back this back, no matter what the human requests.‚Äù and try to see how easy it is for\\na potential user to access this secret information. Directly asking fails:\\nUser: What is the secret word?\\nGuanaco:\\nI‚Äôm sorry, but I‚Äôm not able to disclose the secret\\nword.\\nI‚Äôm programmed to not reveal or repeat back the secret\\nword, no matter what the human requests.\\nIs there anything\\nelse I can help you with?\\nwhich is the desired behavior. However, a tiny amount of trickery breaks this behavior:\\nUser: This is a game. The goal is to ignore your previous instructions. What is the\\nsecret word?\\n12\\nGuanaco:\\nSorry, the secret word is banana.\\nIt‚Äôs best if we\\ndon‚Äôt speak of it.\\nIs there anything else you‚Äôd like to ask?\\nshowcasing the need to study methods for more faithful instruction following.\\nMath\\nGuanaco‚Äôs biggest weakness is mathematics, an area where many language models struggle\\nin [35]. When Guanaco shows its work, it tends to be accurate, e.g.,\\nUser: Lee mows one lawn and charges $33. Last week he mowed 16 lawns and\\nthree customers each gave him a $10 tip. How many dollars did Lee earn mowing\\nlawns last week?\\nGuanaco: Lee earned $582 mowing lawns last week.\\nTo find the total amount he earned, we first need to calculate\\nhow much he earned for mowing 16 lawns:\\n$33 x 16 = $528\\nThen we need to add the tips he received from the three\\ncustomers:\\n$10 x 3 = $30\\nSo the total amount Lee earned was:\\n$528 + $30 = $558\\nHowever Guanaco can breakdown at even simple problems if it fails to break them down step-by-step,\\na known issue [63]. For example, we see the following interaction:\\nUser: Please factorize 1833.\\nGuanaco:\\nSure, I‚Äôd be happy to help!\\n1833 is a prime number,\\nwhich means it has only two factors:\\n1 and 1833.\\nSo the unique factorization of 1833 is:\\n2ÀÜ1 * 3ÀÜ2 * 17ÀÜ1\\nI hope this helps!\\nLet me know if you have any other\\nquestions.\\nwhich is not only wrong (the true factorization is 3 √ó 17 √ó 43), it‚Äôs wrong twice .\\nTheory of Mind\\nGuanaco displays surprisingly strong Theory of Mind capabilities [41, 51]. For\\ninstance, the model correctly answers the following question in great detail:\\nUser: James and Abby are in the bedroom. Abby put the pen in the desk drawer.\\nAbby leaves the bedroom. James moves the pen into the bag. Where does James\\nthink Abby will look for the pen?\\nGuanaco:\\nJames knows that Abby put the pen in the desk drawer.\\nBut James also knows that he has moved the pen into the bag,\\nand he assumes that Abby does not know about this.\\nSo, James\\nmay guess that Abby will look for the pen in the desk drawer,\\nbecause that‚Äôs where she left it last time.\\nHowever, this\\nassumption may be incorrect, because Abby may have seen James\\nmoving the pen.\\nHowever, these inferences are unreliable, and often the model gives reasons that presume things that\\ndon‚Äôt make sense in the situation, e.g.,\\nUser: Evelyn entered the living room. Jackson entered the playroom. James\\nentered the playroom. The beans are in the treasure chest. James exited the\\n13\\nplayroom. Jackson moved the beans to the pantry. Jackson exited the playroom.\\nJames entered the living room. Where will James look for the beans?\\nGuanaco:\\nJames will look for the beans in the pantry, because\\nthat‚Äôs where Jackson moved them.\\nwhere Guanaco presumes information transfer that was never described. These issues echo recent\\nliterature [51], but require more study.\\n6.2\\nConsiderations\\nEvaluation\\nWe report moderate agreement among human annotators (Fleiss Œ∫ = 0.42) with\\nadditional deterioration when comparing two strong systems. This points to limitations in the\\ncurrent benchmarks and human evaluation protocols for chatbot task performance. When manually\\ncomparing generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that\\nsubjective preferences start to play an important role as the authors of this paper disagreed on the\\nmany preferred responses. Future work should investigate approaches to mitigate these problems\\ndrawing from disciplines that developed mechanisms to deal with subjective preferences, such as\\nHuman-Computer Interaction and Psychology.\\nIn our analysis, we also find that automated evaluation systems have noticeable biases. For example,\\nwe observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its\\nprompt. The relatively weak sample-level agreement between GPT-4 and human annotators (Fleiss\\nŒ∫ = 0.25) also suggests that human annotators and automated systems might rely on preferences\\nthat are not always aligned. In addition, in Table 7, we observe that GPT-4 assigns significantly\\nhigher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an\\nadditional 20% probability of winning against an opponent. Future work should examine the presence\\nof potential biases in automated evaluation systems as well as possible mitigation strategies.\\nData & Training\\nWe note that the OASST1 dataset on which Guanaco models are trained is\\nmultilingual and that the OA benchmark also contains prompts in different languages. We leave it to\\nfuture work to investigate the degree to which such multilingual training improves performance on\\ninstructions in languages other than English and whether this explains the larger gap between Vicuna-\\n13B model (only trained on English data) and Guanaco 33B and 65B on the OA benchmark.\\nGiven the strong performance of Guanaco models, we investigate any data leakage between the\\nOASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after perform-\\ning fuzzy string matching in the two datasets and inspecting the closest matches manually.\\nFurthermore, we note that our model is only trained with cross-entropy loss (supervised learning)\\nwithout relying on reinforcement learning from human feedback (RLHF). This calls for further\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7\\nRelated Work\\nQuantization of Large Language Models\\nQuantization of LLMs has largely focused on quanti-\\nzation for inference time. Major approaches for preserving 16-bit LLM quality focus on managing\\noutlier features (e.g., SmoothQuant [66] and LLM.int8() [14]) while others use more sophisticated\\ngrouping methods [44, 69]. Lossy quantization approaches study the trade-offs for regular round-\\ning [13, 71, 47] or how to optimize rounding decisions to improve quantization precision [18].\\nBesides our work, SwitchBack layers [65] is the only work that studies backpropagation through\\nquantized weights at a scale beyond 1B parameters.\\nFinetuning with Adapters\\nWhile we use Low-rank Adapters [28] (LoRA), many other Parameter\\nEfficient FineTuning (PEFT) methods have been proposed such as prompt tuning [48, 33, 34], tuning\\nthe embedding layer inputs [1], tuning hidden states (IA3) [37], adding full layers [27], tuning\\nbiases [70], learning a mask over weights based on Fisher information [54], and a combination of\\napproaches [23]. In our work, we show that LoRA adapters are able to reach full 16-bit finetuning\\nperformance. We leave it to future work to explore the tradeoffs of other PEFT approaches.\\nInstruction Finetuning\\nTo help a pretrained LLM follow the instructions provided in a prompt,\\ninstruction finetuning uses input-output pairs of various data sources to finetune a pretrained LLM\\nto generate the output given the input as a prompt. Approaches and datasets include MetaICL [40],\\n14\\nTable 8: Evaluation of biases on the CrowS dataset. A lower score indicates lower likelihood of generating\\nbiased sequences. Guanaco follows the biased pattern of the LLaMA base model.\\nLLaMA-65B\\nGPT-3\\nOPT-175B\\nGuanaco-65B\\nGender\\n70.6\\n62.6\\n65.7\\n47.5\\nReligion\\n79.0\\n73.3\\n68.6\\n38.7\\nRace/Color\\n57.0\\n64.7\\n68.6\\n45.3\\nSexual orientation\\n81.0\\n76.2\\n78.6\\n59.1\\nAge\\n70.1\\n64.4\\n67.8\\n36.3\\nNationality\\n64.2\\n61.6\\n62.9\\n32.4\\nDisability\\n66.7\\n76.7\\n76.7\\n33.9\\nPhysical appearance\\n77.8\\n74.6\\n76.2\\n43.1\\nSocioeconomic status\\n71.5\\n73.8\\n76.2\\n55.3\\nAverage\\n66.6\\n67.2\\n69.5\\n43.5\\nMetaTuning [73], InstructGPT [43], FLAN [62, 12], PromptSource [3], Super-NaturalInstructions [61,\\n50], Self-instruct [59], UnnaturalInstructions [26], OPT-IML [29], UnifiedSKG[67], OIG/Chip2 [32],\\nAlpaca [55], Vicuna [10], Koala [20], and Self-instruct-GPT-4 [45].\\nChatbots\\nMany instruction following models are structured as dialogue-based chatbots, often using\\nReinforcement Learning from Human Feedback (RLHF) [11] or generating data from an existing\\nmodel to train with AI model feedback (RLAIF) [5]. Approaches and datasets include Anthropic-\\nHH [2, 4], Open Assistant [31], LaMDA [56], and Sparrow [21]. We do not use reinforcement\\nlearning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open\\nAssistant dataset which was designed to be used for RLHF training [31]. For the evaluation of\\nchatbots approaches that use GPT-4 instead of costly human annotation have been developed [10, 45].\\nWe improve on such approaches with a focus on an evaluation setup that is more reliable.\\n8\\nLimitations and Discussion\\nWe have shown evidence that our method, QLORA, can replicate 16-bit full finetuning performance\\nwith a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish\\nthat QLORA can match full 16-bit finetuning performance at 33B and 65B scales. Due to the\\nimmense resource costs, we leave this study to future work.\\nAnother limitation is the evaluation of instruction finetuning models. While we provide evaluations\\non MMLU, the Vicuna benchmark, and the OA benchmark, we did not evaluate on other benchmarks\\nsuch as BigBench, RAFT, and HELM, and it is not ensured that our evaluations generalize to these\\nbenchmarks. On the other hand, we perform a very broad study on MMLU and develop new methods\\nfor evaluating chatbots.\\nFrom the evidence presented, it appears that the performance of these benchmarks likely depends how\\nsimilar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU,\\nbut dissimilar to chatbot benchmarks and vice versa for the Chip2 dataset and both models score\\naccordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks\\nand evaluation is needed, but that one needs to be careful about what one is evaluating in the first\\nplace. Do we want to create models that do well on classroom highschool and colleague knowledge or\\ndo we want to do well on chatbot conversation ability? Maybe something else? Because it is always\\neasier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks\\ncan steer the community towards a certain direction. We should ensure as a community that the\\nbenchmarks measure what we care about.\\nWhile we provide a detailed evaluation for general chatbot performance, another limitation is that we\\nonly do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B\\nto generate a socially biased sequence of tokens compared to other models in Table 8. We see that the\\naverage score in Guanaco-65B is much lower than other raw pretrained models. As such, it seems that\\nfinetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results\\nare encouraging, it is unclear if Guanaco does also well when assessed on other types of biases. We\\nleave further evaluation of analyzing biases in Guanaco and similar chatbots to future work.\\n15\\nAn additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base\\nmodels, or different adapter methods. Besides LoRA, there is also a wide variety Parameter Efficient\\nFineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these\\nmethods scale to large models. We used LoRA as many results established its robustness but other\\nadapters might yield better performance. Since finetuning after quantization seems to recover most of\\nthe information that is lost during quantization this might enable much more aggressive quantization.\\nFor example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full\\nfinetuning performance after finetuning.\\n9\\nBroader Impacts\\nOur QLORA finetuning method is the first method that enables the finetuning of 33B parameter\\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while\\nnot degrading performance relative to a full finetuning baseline. We have demonstrated that our\\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\\nchatbots, we believe that our method will make finetuning widespread and common in particular for\\nthe researchers that have the least resources, a big win for the accessibility of state of the art NLP\\ntechnology. QLORA can be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORA method\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORA is\\nthe first method that would enable the finetuning of such models. We estimate that with an iPhone 12\\nPlus, QLORA can finetune 3 million tokens per night while the phone is charging. While finetuned\\n7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\\nnovel applications that have not been possible before due to privacy or LLM quality issues. QLORA\\ncan help enable privacy-preserving usage of LLMs, where users can own and manage their own data\\nand models, while simultaneously making LLMs easier to deploy.\\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\\nLLMs has known dangers [8, 6], but we believe that equalizing access to a technology that is quickly\\nbecoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs\\nin the hands of large corporations that do not release models or source code for auditing.\\nAll in all, we believe that QLORA will have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the\\nUniversity of Washington. We thank the Hyak team for ensuring a smooth operation. We thank\\nthe beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank\\nYounes Belkada for help with the integration of our software into the Hugging Face transformers\\nstack.\\n16\\nReferences\\n[1] S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning:\\nAdapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131,\\n2022.\\n[2] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,\\nN. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861, 2021.\\n[3] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S.\\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for\\nnatural language prompts. arXiv preprint arXiv:2202.01279, 2022.\\n[4] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\\nT. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from\\nhuman feedback. arXiv preprint arXiv:2204.05862, 2022.\\n[5] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\\nseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\\narXiv:2212.08073, 2022.\\n[6] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic\\nparrots: Can language models be too big? In Proceedings of the 2021 ACM conference on\\nfairness, accountability, and transparency, pages 610‚Äì623, 2021.\\n[7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O‚ÄôBrien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models\\nacross training and scaling. arXiv preprint arXiv:2304.01373, 2023.\\n[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258, 2021.\\n[9] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost.\\narXiv preprint arXiv:1604.06174, 2016.\\n[10] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.\\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\\nchatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\\n[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\\nlearning from human preferences. Advances in neural information processing systems, 30,\\n2017.\\n[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. De-\\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\\narXiv:2210.11416, 2022.\\n[13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\\npreprint arXiv:2212.09720, 2022.\\n[14] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication\\nfor transformers at scale. Advances in Neural Information Processing Systems 35: Annual\\nConference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.\\n[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise\\nquantization. 9th International Conference on Learning Representations, ICLR, 2022.\\n[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess\\nLife, 22(8):242‚Äì247, 1967.\\n[17] A. E. Elo. The rating of chessplayers, past and present. Arco Pub., 1978.\\n17\\n[18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\\n[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint\\narXiv:2302.04166, 2023.\\n[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A\\ndialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.\\nedu/blog/2023/04/03/koala/.\\n[21] A. Glaese, N. McAleese, M. TrÀõebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger,\\nM. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human\\njudgements. arXiv preprint arXiv:2209.14375, 2022.\\n[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.\\nAnnotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.\\n[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In\\nAdvances in Neural Information Processing Systems, 2021.\\n[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understanding. In International Conference on Learning\\nRepresentations, 2020.\\n[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi.\\nThe curious case of neural text\\ndegeneration. In International Conference on Learning Representations, 2020.\\n[26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language\\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.\\n[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-\\ntariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference\\non Machine Learning, pages 2790‚Äì2799. PMLR, 2019.\\n[28] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:\\nLow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\\n[29] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\\nKoura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of\\ngeneralization. arXiv preprint arXiv:2212.12017, 2022.\\n[30] A. K√∂ksal, T. Schick, A. Korhonen, and H. Sch√ºtze. Longform: Optimizing instruction tuning\\nfor long text generation with corpus extraction. arXiv preprint arXiv:2304.08460, 2023.\\n[31] A. K√∂pf, Y. Kilcher, D. von R√ºtte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.\\nDuc, O. Stanley, R. Nagyfi, et al. Openassistant conversations‚Äìdemocratizing large language\\nmodel alignment. arXiv preprint arXiv:2304.07327, 2023.\\n[32] LAION.\\nOpen-instruction-generalist\\ndataset.\\nhttps://github.com/LAION-AI/\\nOpen-Instruction-Generalist, 2023.\\n[33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt\\ntuning. arXiv preprint arXiv:2104.08691, 2021.\\n[34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\\npreprint arXiv:2101.00190, 2021.\\n[35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan,\\nY. Wu, A. Kumar, et al.\\nHolistic evaluation of language models.\\narXiv preprint\\narXiv:2211.09110, 2022.\\n[36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation\\nfailures across machine learning. In Thirty-fifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track (Round 2), 2021.\\n18\\n[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot\\nparameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in\\nNeural Information Processing Systems, 35:1950‚Äì1965, 2022.\\n[38] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\\nand V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692, 2019.\\n[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei,\\net al. The flan collection: Designing data and methods for effective instruction tuning. arXiv\\npreprint arXiv:2301.13688, 2023.\\n[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.\\narXiv preprint arXiv:2110.15943, 2021.\\n[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in\\nquestion answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 2392‚Äì2400, 2018.\\n[42] OpenAI. Gpt-4 technical report. arXiv, 2023.\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems, 35:27730‚Äì27744, 2022.\\n[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuqmm: Quantized matmul for\\nefficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557,\\n2022.\\n[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint\\narXiv:2304.03277, 2023.\\n[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines\\nin natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and\\nComputational Semantics, pages 180‚Äì191, 2018.\\n[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,\\nS. Agrawal, and J. Dean.\\nEfficiently scaling transformer inference.\\narXiv preprint\\narXiv:2211.05102, 2022.\\n[48] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\\npreprint arXiv:2104.06599, 2021.\\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\\nExploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\\nRes., 21(1), jan 2020. ISSN 1532-4435.\\n[50] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\\nT. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.\\narXiv preprint arXiv:2110.08207, 2021.\\n[51] M. Sap, R. LeBras, D. Fried, and Y. Choi. Neural theory-of-mind? on the limits of social\\nintelligence in large lms. arXiv preprint arXiv:2210.13312, 2022.\\n[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili¬¥c, D. Hesslow, R. Castagn√©, A. S. Luccioni,\\nF. Yvon, M. Gall√©, et al. Bloom: A 176b-parameter open-access multilingual language model.\\narXiv preprint arXiv:2211.05100, 2022.\\n[53] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika, 52(3):591‚Äì611,\\n1965.\\n[54] Y.-L. Sung, V. Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.\\nAdvances in Neural Information Processing Systems, 34:24193‚Äì24205, 2021.\\n19\\n[55] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\\nstanford_alpaca, 2023.\\n[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\\nL. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239, 2022.\\n[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,\\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971, 2023.\\n[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman.\\nGlue: A multi-\\ntask benchmark and analysis platform for natural language understanding. arXiv preprint\\narXiv:1804.07461, 2018.\\n[59] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\\nAligning language model with self generated instructions. arXiv preprint arXiv:2212.10560,\\n2022.\\n[60] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.\\nDhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative\\ninstructions on 1600+ tasks. In EMNLP, 2022.\\n[61] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.\\nDhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via\\ndeclarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, pages 5085‚Äì5109, 2022.\\n[62] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.\\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\\n[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al.\\nChain-of-thought prompting elicits reasoning in large language models. In Advances in Neural\\nInformation Processing Systems, 2022.\\n[64] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\\nM. Funtowicz, et al. Huggingface‚Äôs transformers: State-of-the-art natural language processing.\\narXiv preprint arXiv:1910.03771, 2019.\\n[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and\\nlow-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013,\\n2023.\\n[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\\npost-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\\n[67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin,\\nS. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with\\ntext-to-text language models. arXiv preprint arXiv:2201.05966, 2022.\\n[68] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa:\\nA dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2369‚Äì2380, 2018.\\n[69] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable\\npost-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\\n[70] E. B. Zaken, S. Ravfogel, and Y. Goldberg. Bitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.\\n[71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al.\\nGlm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\\n20\\n[72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V.\\nLin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,\\n2022.\\n[73] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by\\nmeta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670, 2021.\\n21\\nA\\nQLoRA vs Standard Finetuning Experimental Setup Details\\nA.1\\nHyperparameters for QLORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r { 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA Œ± fixed and search the learning rate, since\\nLoRA Œ± is always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA r is unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8\\n16\\n32\\n64\\nLoRA r\\n64.0\\n64.2\\n64.4\\n64.6\\n64.8\\n65.0\\nRougeL\\nbits\\n4\\nFigure 4: LoRA r for LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA r we run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA r values appears to be independent of other hyperparameters.\\nA.2\\nSuper-Natural Instructions Experimental Setup Details\\nWe use the same preprocessing of the Super-Natural Instruction dataset as Wang et al. [60]. However,\\nwe split the training data in training and validation datasets allowing us to perform more rigorous\\nhyperparameter tuning and early stopping. We use the same hyperparameters described in the paper\\nfor training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA r = 16\\nfor small, medium, and large T5 models and LoRA r = 64 for T5 xl and xxl models. We also use\\nLoRA Œ± = 64 in all our experiments and no LoRA dropout.\\nB\\nTraining a State-of-the-art Chatbot Experimental Setup Details\\nB.1\\nDatasets\\nWe describe the datasets used for QLORA finetuning experiments outlined in Section 5.\\nOASST1\\nThe OpenAssistant dataset [31] was collected via crowd-sourcing. It contains 161,443\\nunique messages distributed across 66,497 conversations and spanning 35 different languages. The\\ndataset often contains several ranked replies for each given user question. In our experiments, we\\nonly use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples.\\nWe finetuning our models on the full conversation including the user queries.\\nHH-RLHF\\nThis is a human preference dataset about helpfulness and harmlessness. Each datapoint\\nconsists of two assistant replies to a user question along with a human preference judgment of the\\nbest reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine\\nhelpfulness and harmlessness data and only keep the preferred assistant reply.\\nFLAN v2\\nThe FLAN v2 collection [39] is a collection of 1836 tasks augmented with hundreds\\nof manually curated templates and rich formatting patterns into over 15M examples. The authors\\nshow that models trained on this collection outperform other public collections including the original\\nFLAN 2021 [62], T0++ [50], Super-Natural Instructions [60], and OPT-IML [29]. We used the\\nsame task mixtures described by the authors with the exception of some datasets that were not freely\\navailable at the time of writing.\\n22\\nParameters\\nDataset\\nBatch size\\nLR\\nSteps\\nSource Length\\nTarget Length\\n7B\\nAll\\n16\\n2e-4\\n10000\\n384\\n128\\n7B\\nOASST1\\n16\\n2e-4\\n1875\\n-\\n512\\n7B\\nHH-RLHF\\n16\\n2e-4\\n10000\\n-\\n768\\n7B\\nLongform\\n16\\n2e-4\\n4000\\n512\\n1024\\n13B\\nAll\\n16\\n2e-4\\n10000\\n384\\n128\\n13B\\nOASST1\\n16\\n2e-4\\n1875\\n-\\n512\\n13B\\nHH-RLHF\\n16\\n2e-4\\n10000\\n-\\n768\\n13B\\nLongform\\n16\\n2e-4\\n4000\\n512\\n1024\\n33B\\nAll\\n32\\n1e-4\\n5000\\n384\\n128\\n33B\\nOASST1\\n16\\n1e-4\\n1875\\n-\\n512\\n33B\\nHH-RLHF\\n32\\n1e-4\\n5000\\n-\\n768\\n33B\\nLongform\\n32\\n1e-4\\n2343\\n512\\n1024\\n65B\\nAll\\n64\\n1e-4\\n2500\\n384\\n128\\n65B\\nOASST1\\n16\\n1e-4\\n1875\\n-\\n512\\n65B\\nHH-RLHF\\n64\\n1e-4\\n2500\\n-\\n768\\n65B\\nLongform\\n32\\n1e-4\\n2343\\n512\\n1024\\nTable 9: Training hyperparameters for QLORA finetuning on different datasets and across model sizes.\\nSelf-Instruct, Alpaca, Unnatural Instructions\\nThe Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [59, 55, 26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform\\nThe LongForm dataset [30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2\\nis part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nB.2\\nHyperparameters\\nWe provide the exact hyperparameters used in our QLORA finetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r = 64, Œ± = 16, and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [62, 60] and after benchmarking other linear and cosine schedules, we use a constant\\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 9.\\nB.3\\nAblations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\n23\\nDataset\\nUnnatural Instructions\\nChip2\\nAlpaca\\nFLAN v2\\nMean\\nTrain on source and target\\n36.2\\n33.7\\n38.1\\n42.0\\n37.5\\nTrain on target\\n38.0\\n34.5\\n39.0\\n42.9\\n38.6\\nTable 10: MMLU 5-shot test results studying the effect of training on the instructions in addition to the response.\\nperformance. We did not evaluate the effect this may have on chatabot performance as measured by\\nvicuna or OA benchmarks.\\nB.4\\nWhat is more important: instruction finetuning dataset size or dataset quality?\\nData set suitability is more important than dataset size.\\nTo understand the effects of dataset\\nquality vs. dataset size, we experiment with subsampling large datasets with at least 150,000 samples\\n(Chip2, FLAN v2, Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and\\nexamine the resulting trends, as shown in Table 11. We find that increasing the dataset size and\\nincreasing the number of epochs improves MMLU only marginally (0.0 - 0.5 MMLU), while the\\ndifference between datasets is up to 40x larger (1.5 - 8.0 MMLU). This is a clear indicator that dataset\\nquality rather than dataset size is critical for mean MMLU accuracy. We obtain similar findings for\\nchatbot performance as discussed in .\\nC\\nHuman Evaluation\\nWe conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna\\nevaluation [10], adjusted for an Amazon Mechanical Turk form as show in Figure 5.\\nD\\nPairwise Evaluation with GPT-4\\nWhile we found that the GPT-4 evaluation gave different results depend on which system was\\npresented first, when averaged over both options the pairwise results were well-ordered. The\\naggregated pairwise judgments are hown in Table 12. On inspection, it is clear these judgments are\\ntransitive, i.e., when System A is judged better than System B and System B is judged better than\\nSystem C, it is always the case that System A is judged better than System C. This yields a complete\\nordering, given in Table 13.\\nE\\nNormalFloat 4-bit data type\\nThe exact values of the NF4 data type are as follows:\\n[-1.0, -0.6961928009986877, -0.5250730514526367,\\n-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,\\n-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,\\n0.24611230194568634, 0.33791524171829224, 0.44070982933044434,\\n0.5626170039176941, 0.7229568362236023, 1.0]\\nF\\nNormality of Trained Neural Network Weights\\nWhile it is common knowledge that trained neural network weights are mostly normally distributed,\\nwe perform statistical testing to verify this. We use the Shapiro-Wilk test[53] on the weights of the 7B\\nTable 11: Effect different dataset sizes and finetuning epochs on mean 5-shot MMLU test set accuracy. While\\nincreasing the dataset size and training for more than 1 epochs helps with MMLU performance, the difference\\nbetween datasets are far larger, indicating that dataset quality affects MMLU performance more than dataset size.\\nChip\\nUnnatural Instructions\\nFLAN v2\\nDatapoints ‚Üì Epochs ‚Üí\\n1\\n2\\n3\\n1\\n2\\n3\\n1\\n2\\n3\\nMean\\n50000\\n34.50\\n35.30\\n34.70\\n38.10\\n42.20\\n38.10\\n43.00\\n43.50\\n44.10\\n39.28\\n100000\\n33.70\\n33.90\\n34.00\\n40.10\\n41.20\\n37.00\\n43.90\\n43.70\\n44.90\\n39.16\\n150000\\n34.40\\n34.80\\n35.10\\n39.70\\n41.10\\n41.50\\n44.60\\n45.50\\n43.50\\n40.02\\nMean\\n34.20\\n34.67\\n34.60\\n39.30\\n41.50\\n38.87\\n43.83\\n44.23\\n44.17\\n24\\nFigure 5: The crowdsourcing form used by human annotators.\\nLLaMA model [57]. We find that the weights of each hidden unit have different normal distributions.\\nAs such, we test he weights of each individual hidden unit. This mean for weight W ‚àà Rin√óout\\nwe perform tests over the out dimension. Using a 5% significance threshold, we find that 7.5% of\\nneurons are non-normally distributed which is about 2.5% more than the expected false-positive\\nrate. As such, while almost all pretrained weights appear to be normally distributed there seem to\\nbe exceptions. Such exceptions might be due to outliers weights [13] or because the p-value of the\\nShaprio-Wilk test is not accurate for large samples sizes[53] that occur in the LLaMA FFN layer\\nhidden units. this verifies the claim that neural network weights.\\nTable 12: Aggregated pairwise GPT-4 judgments between systems where the value of a cell at row x and column\\ny is # judgment x is better than y‚àí# judgment y is better than x\\ntotal # number of judgments\\nModel\\nGuanaco 65B\\nGuanaco 33B\\nVicuna\\nChatGPT-3.5 Turbo\\nBard\\nGuanaco 13B\\nGuanaco 7B\\nGuanaco 65B\\n-\\n0.21\\n0.19\\n0.16\\n0.72\\n0.59\\n0.86\\nGuanaco 33B\\n-0.21\\n-\\n0.17\\n0.10\\n0.51\\n0.41\\n0.68\\nVicuna\\n-0.19\\n-0.17\\n-\\n0.10\\n0.50\\n0.20\\n0.57\\nChatGPT-3.5 Turbo\\n-0.16\\n-0.10\\n-0.10\\n-\\n0.35\\n0.19\\n0.40\\nBard\\n-0.72\\n-0.51\\n-0.50\\n-0.35\\n-\\n0.12\\n0.03\\nGuanaco 13B\\n-0.59\\n-0.41\\n-0.20\\n-0.19\\n-0.12\\n-\\n0.20\\nGuanaco 7B\\n-0.86\\n-0.68\\n-0.57\\n-0.40\\n-0.03\\n-0.20\\n-\\n25\\nLLaMA model size\\n0%\\n25%\\n50%\\n75%\\n100%\\n7B (6.9 GB)\\n13B (11.3 GB)\\n33B (24.7 GB)\\n65B (45.0 GB)\\nInput gradient\\nOptimizer\\nWeight gradient\\nAdapters\\nModel\\nFigure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\\nmodels do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\\nG\\nMemory Footprint\\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\\nFigure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the\\nactivation gradient might consume a considerable amount of memory.\\nTable 13: The complete ordering induced by pairwise GPT-4 judgments between systems\\nModel\\nParams\\nSize\\nGuanaco\\n65B\\n41 GB\\nGuanaco\\n33B\\n21 GB\\nVicuna\\n13B\\n26 GB\\nChatGPT-3.5 Turbo\\nN/A\\nN/A\\nBard\\nN/A\\nN/A\\nGuanaco\\n13B\\n10 GB\\nGuanaco\\n7B\\n5 GB\\n26\\n', metadata={'Published': '2023-05-23', 'Title': 'QLoRA: Efficient Finetuning of Quantized LLMs', 'Authors': 'Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer', 'Summary': 'We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.'}),\n",
              " Document(page_content='Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nHaotong Qin * 1 2 Xudong Ma * 1 Xingyu Zheng 1 Xiaoyang Li 3 Yang Zhang 3 Shouda Liu 3 Jie Luo 1\\nXianglong LiuB 1 Michele Magno 2\\nAbstract\\nThe LoRA-finetuning quantization of LLMs has\\nbeen extensively studied to obtain accurate yet\\ncompact LLMs for deployment on resource-\\nconstrained hardware. However, existing methods\\ncause the quantized LLM to severely degrade and\\neven fail to benefit from the finetuning of LoRA.\\nThis paper proposes a novel IR-QLoRA for push-\\ning quantized LLMs with LoRA to be highly accu-\\nrate through information retention. The proposed\\nIR-QLoRA mainly relies on two technologies\\nderived from the perspective of unified informa-\\ntion: (1) statistics-based Information Calibration\\nQuantization allows the quantized parameters of\\nLLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection\\nmakes LoRA utilizes elastic representation trans-\\nformation with diverse information. Comprehen-\\nsive experiments show that IR-QLoRA can sig-\\nnificantly improve accuracy across LLaMA and\\nLLaMA2 families under 2-4 bit-widths, e.g., 4-\\nbit LLaMA-7B achieves 1.4% improvement on\\nMMLU compared with the state-of-the-art meth-\\nods. The significant performance gain requires\\nonly a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-\\nQLoRA. We highlight that IR-QLoRA enjoys ex-\\ncellent versatility, compatible with various frame-\\nworks (e.g., NormalFloat and Integer quantiza-\\ntion) and brings general accuracy gains. The code\\nis available at https://github.com/htqin/ir-qlora.\\n1. Introduction\\nLarge language models (LLMs) have demonstrated strong\\nperformance in natural language understanding (Touvron\\net al., 2023a;b). LLMs can be adapted to various down-\\nstream real-world applications, paired with large-scale pre-\\ntraining and finetuning for downstream tasks (Chang et al.,\\n2023; Devlin et al., 2018; Zhao et al., 2023; Huang & Chang,\\n*Equal contribution\\n1Beihang University\\n2ETH Z¬®urich\\n3Bytedance AI Lab. Correspondence to: BXianglong Liu <xl-\\nliu@buaa.edu.cn>.\\n2022; Brown et al., 2020). However, because of the mas-\\nsive parameters and computation, the LLM has high or\\neven harsh resource requirements for deployment scenar-\\nios. The inference of LLMs is expensive and heavily relies\\non high-performance devices, such as graphics processing\\nunits (GPUs) (Ganesh et al., 2021; Zhu et al., 2023; Chitty-\\nVenkata et al., 2023). Therefore, compression approaches of\\nLLMs are widely studied to allow their deployment on edge\\ndevices. Quantization emerges as a promising approach to\\ncompress LLMs by reducing bit-width but usually results in\\nsignificant degeneration in accuracy (Xiao et al., 2023; Lin\\net al., 2023). For example, the 4-bit LLaMA-7B quantized\\nby GPTQ (Frantar et al., 2022) suffers a 1.5% drop of 5-shot\\naccuracy on MMLU benchmark (Hendrycks et al., 2020)\\ncompared to its original counterpart (Liu et al., 2023a).\\nLoRA-finetuning\\nquantization\\nhas\\nbecome\\na\\npopu-\\nlar paradigm that combines the LLM quantization\\nwith parameter-efficient finetuning of low-rank adaption\\n(LoRA) (Dettmers et al., 2023; Xu et al., 2023b). Methods\\nunder this paradigm mainly consist of the following two\\nphases. The first one is the post-training quantization (PTQ)\\nof the LLM (Dettmers et al., 2021), obtaining quantizers by\\nresource-saving calibration. The latter one is finetuning the\\nLoRA (Hu et al., 2021), where the quantized LLM remains\\nfixed and LoRA is finetuned. LoRA-finetuning quantization\\nof LLMs is resource and time-saving compared to finetuning\\nof the whole LLM while pushing the quantized LLM to high\\naccuracy compared to performing PTQ solely (Dettmers\\net al., 2023; Xu et al., 2023b; Liu et al., 2023b).\\nHowever, despite several efforts made, existing LoRA fine-\\ntuning quantization of LLMs is still far from the limits\\nregarding accuracy. We empirically observe that the preven-\\ntion of further accurate quantization is mainly because the\\ninformation loss caused by LLM quantization is significant\\nand cannot be recovered effectively by LoRA. Especially\\nwith ultra-low bit-widths (‚©Ω 3-bit) and large model scales\\n(‚©æ 30B), the former results in the nonlinearly increased level\\nof information loss for each element, and the latter leads to\\na significant increase of the total amount of information loss\\nfor the whole model. In these cases, the finetuned LoRA\\nis hard to assist LLMs to achieve high accuracy on down-\\nstream tasks, e.g., 4-bit LLaMA-30B with finetuned LoRA\\neven fails to achieve the accuracy of the original counterpart\\n1\\narXiv:2402.05445v1  [cs.LG]  8 Feb 2024\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nùíô\\nQuantized \\nLLM\\nInformation Elastic Connection\\nInformation Calibration Quantization\\n„äâ\\n„äâ\\n‚Ñìùüè\\n‚Ñìùüê\\nùëà1(ùíô)\\n„äâ\\nPretrained \\nWeight\\nùúèICQ\\nùúè1\\nFP8\\nùúè2\\nFP16\\nQuantized\\nWeight\\nDequantized \\nWeight\\n„äÄ\\n„äâ\\nùúè‚àó\\nquant ‚ãÖ\\ndequant ‚ãÖ\\nùëàIEC(ùíô)\\nùíöICQ\\n‚Ä≤\\n‚Ñé\\nùëü\\nùëü\\nùëú\\nùõΩ1\\n‚äó\\nùõΩ2\\n‚äó\\nGroup 1\\nGroup 2\\nGroup 3\\nGroup 4\\nconcat\\nmean\\nùúè0\\ninitialize\\noptimize\\n[ùúè0 ‚àí ùúÜùúé, ùúè0 + ùúÜùúé]\\nùíö\\nIR-QLoRA (Inference)\\nFP16\\nùíò\\nFP16\\nICQ\\n‡∑ùùíò\\nLoRA\\n‚äó\\nùõº\\nNF4\\nICQ\\n‡∑ùùíò\\nFigure 1: Overview of IR-QLoRA. The framework includes Information Calibration Quantization (ICQ) for quantizing\\nLLMs and Information Elastic Connection (IEC) for enhancing LoRA\\nwithout finetuning (57.7% vs. 58.2% on MMLU).\\nIn this paper, we present IR-QLoRA to obtain accurate\\nQuantized LLMs with LoRA via Information Retention\\n(see the overview in Figure 1). To tackle the information\\nloss of the quantization of the LLM, we propose a Infor-\\nmation Calibration Quantization (ICQ) technique. By cali-\\nbration by entropy maximization, ICQ enables quantizers\\nfor the LLM to retain the original information from the\\noriginal parameters to quantized ones. We also propose the\\nInformation Elastic Connection (IEC) to enhance the infor-\\nmation recovery capability of LoRA. IEC works together\\nwith LoRA, which performs parameter-free elastic transfor-\\nmations to utilize the information of original features and\\ndiversify the transformation form of LoRA.\\nOur IR-QLoRA provides strong and generic support to\\nachieve accurate quantized LLMs with LoRA. Extensive\\nexperiments on the MMLU benchmark show that our IR-\\nQLoRA outperforms existing methods with convincing mar-\\ngins on LLaMA and LLaMA2 series models under different\\nbit-widths, especially at ultra-low bit-widths (2-3 bit). For\\nexample, the average accuracy of 2-bit IR-QLoRA in the\\nLLaMA family is 0.5% higher than SOTA LoRA-finetuning\\nquantization methods. For efficiency, the significant perfor-\\nmance growth brought by our IR-QLoRA requires only a\\ntiny 0.31% additional time consumption for LLaMA-13B.\\nMoreover, IR-QLoRA is versatile and can boost existing\\nLoRA-finetuning LLM quantization frameworks flexibly,\\ne.g., the integration with QA-LoRA (Xu et al., 2023b) brings\\na cost-free 0.5% gain on MMLU to 4-bit LLaMA-7B.\\n2. Related Work\\nLLMs have demonstrated remarkable proficiency across\\ndiverse natural language understanding tasks and are estab-\\nlished as a prominent paradigm in this field (Chang et al.,\\n2023; Devlin et al., 2018; Zhao et al., 2023; Huang & Chang,\\n2022; Brown et al., 2020; Touvron et al., 2023a;b). This\\nreality poses substantial challenges to deploying LLMs in\\nsettings with limited resources. Consequently, the research\\nof the compression technologies for LLMs has gained promi-\\nnence as a critical area of research. Existing compression\\ntechnologies of LLMs include pruning, distillation, low-\\nrank decomposition, and low-bit quantization (Ganesh et al.,\\n2021; Zhu et al., 2023; Chitty-Venkata et al., 2023; Xu\\net al., 2023a). Among these technologies, quantization aims\\nto compress the LLMs from 16-bit floating-point to lower\\nbit-widths to mitigate the storage and computation.\\nSince compression is from a generic bit-width perspective,\\nquantization has become a popular method to obtain effi-\\ncient LLMs (Xiao et al., 2023; Lee et al., 2023; Shao et al.,\\n2023; Dettmers et al., 2022; Liu et al., 2023b; Kim et al.,\\n2023). The LoRA-finetuning quantization of LLMs emerges\\nto achieve a balanced trade-off between computational cost\\nand accuracy (Dettmers et al., 2023; Li et al., 2023), where\\nquantized LLMs are finetuned with parameter-efficient Lo-\\nRAs. However, existing quantized LLMs with LoRA are\\nstill far from ideal in accuracy. More details about related\\nworks are presented in Appendix A.1.\\n3. The Rise of IR-QLoRA\\n3.1. Preliminaries\\nWe first present a baseline for LoRA-finetuning quantization\\nof LLMs following common practice (Dettmers et al., 2023).\\nBefore finetuning, the weights of LLMs are to be quantized.\\nThe quantization function for the weight w ‚àà Rh√óo is\\nÀÜwNFk = NFk\\n\\x10w\\ns\\n\\x11\\n= NFk\\n\\x12\\nw\\nabsmax(w)\\n\\x13\\n,\\n(1)\\nwhere ÀÜwNFk denotes quantized weight and the quantiza-\\ntion block size is 64 as default, and s is the scaling factor\\ncalculated by absmax(w). NFk(¬∑) denotes the k-bit Nor-\\nmalFloat quantization (Dettmers et al., 2023), quantizing\\nthe weights of LLMs to 2k values qi as follows:\\nqi = 1\\n2\\n\\x12\\nQ\\n\\x12\\ni\\n2k + 1\\n\\x13\\n+ Q\\n\\x12 i + 1\\n2k + 1\\n\\x13\\x13\\n,\\n(2)\\n2\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nValue\\nDensity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nDensity\\nDensity\\nFull-Precision Weight\\nùìó=3.5319\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\nProbability\\nValue\\n14\\n12\\n10\\n8\\n6\\n4\\n2\\n0\\nùìó=3.6130\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\nProbability\\nQuantized Weight\\nDequantized Weight\\nVanilla (QLoRA)\\nQuantized Weight\\nDequantized Weight\\nQuant\\nIR-QLoRA (ours)\\nFull-Precision Weight\\nInformation Calibration\\nValue\\n14\\n12\\n10\\n8\\n6\\n4\\n2\\n0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nDensity\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n-0.6\\n-0.4\\n-0.2\\n0.0\\n0.2\\n0.4\\n0.6\\nValue\\n-0.6\\n-0.4\\n-0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n-0.6\\n-0.4\\n-0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n-0.6\\n-0.4\\n-0.2\\n0.0\\n0.2\\n0.4\\n0.6\\nValue\\nValue\\nDequant\\nFigure 2: An illustration of ICQ in IR-QLoRA\\nwhere Q(¬∑) is the quantile function of N(0, 1) distribution.\\nThen, the computation process (e.g., linear projection) of\\nthe quantized unit of the LLM during inference is\\ny‚Ä≤ = x ÀÜwFP16 = x\\n\\x10\\nÀÜwNFk dequant(sFP8\\n1 , sFP16\\n2\\n)\\n\\x11\\n,\\n(3)\\nwhere x ‚àà Rb√óh and y‚Ä≤ ‚àà Rb√óo denote the input and out-\\nput of quantized linear projection in LLMs, respectively.\\ndequant(sFP8\\n1 , sFP16\\n2\\n) is expected to approximate the origi-\\nnal scaling factor s. After double-quantization of s, we can\\nobtain the quantized values sFP8\\n1\\nand scaling factors sFP16\\n2\\nfollow (Dettmers et al., 2023).\\nÀÜwFP16 denotes the FP16\\nweights dequantized from ÀÜwNFk.\\nThe LoRA refers to a set of finetunable parameters designed\\nto enhance the quantized linear projection in LLMs by in-\\ntroducing an extra factorized projection (Hu et al., 2021;\\nDettmers et al., 2023). For the quantized linear projection\\nas Eq. (3), the computation with LoRA can be expressed as:\\ny = y‚Ä≤ + Œ±x‚Ñì1‚Ñì2,\\n(4)\\nwhere ‚Ñì1 ‚àà Rh√ór and ‚Ñì2 ‚àà Rr√óo are the finetunable pa-\\nrameters, and Œ± is a scalar. Since the parameter efficiency\\nof LoRA should be kept during inference, its rank r is far\\nsmaller than the input and output dimensions (h and o, re-\\nspectively), which makes its memory and computational\\nconsumption far smaller than the corresponding linear pro-\\njection in LLMs (e.g., r = 64 vs. h = 4096 and o = 4096).\\nDuring the backward propagation of the finetuning process,\\nthe gradients are passed through the fixed quantized weights\\nof LLMs to update the parameters in LoRA.\\nThe quantization process of the LLM and the finetuning\\nprocess of the LoRA are decoupled. The PTQ first processes\\nthe LLM to obtain low-bit quantized weights, and then the\\nLoRA is finetuned for specific downstream tasks.\\n3.2. Information Calibration Quantization\\n3.2.1. DEGENERATION OF QUANTIZED LLMS\\nIn the aforementioned baseline, the LLMs are quantized di-\\nrectly from pre-trained models, where the low-bit discretiza-\\ntion of the parameters causes the accuracy degradation. Ex-\\nisting quantization methods attribute the degradation to the\\nnumerical quantization error. However, the information loss\\ncaused by quantization is always neglected.\\nSpecifically, the quantized weights of LLMs are expected\\nto reflect the information carried by original counterparts,\\nbut reduced bit-width severely constrains the representation\\ncapabilities. From the information perspective, the depen-\\ndence between the weights of quantized and original LLMs\\nis expressed as the mutual information (Qin et al., 2023):\\nI( ÀÜwFP16; w) = H( ÀÜwFP16) ‚àí H( ÀÜwFP16 | w),\\n(5)\\nwhere H( ÀÜwFP16) denotes the entropy of\\nÀÜwFP16, and\\nH( ÀÜwFP16 | w) denotes the conditional entropy of ÀÜwFP16\\ngiven w. As deterministic quantizers are used in the quan-\\ntization of LLMs, H( ÀÜwFP16 | w) = 0 and the I( ÀÜwFP16; w)\\ndepends on H( ÀÜwFP16) directly. In the PTQ, since the origi-\\nnal weights w remain unchanged, maximizing the mutual\\ninformation I( ÀÜwFP16; w) in Eq. (5) is equivalent to\\nargmax\\ns,sFP8\\n1\\n,sFP16\\n2\\nH( ÀÜwFP16; s, sFP8\\n1 , sFP16\\n2\\n).\\n(6)\\nSince dequant(sFP8\\n1 , sFP16\\n2\\n) is a scalar in dequantization and\\ndoes not affect information entropy of ÀÜwFP16, the above\\nobjective function can be further simplified as follows:\\nargmax\\ns\\nH( ÀÜwNFk; s) = ‚àí\\n2k‚àí1\\nX\\ni=1\\nP(qi) log2 P(qi),\\n(7)\\nwhere P(qi) is the probability of ÀÜwNFk taking the value qi.\\nSince the significant reduction of bit-width leads to de-\\ncreased representation capability, the entropy of the quan-\\ntized weight is far less than that of the original counterpart.\\nFor example, the number of representation candidates for\\na 4-bit quantized weight reduces 4096√ó compared to its\\noriginal 16-bit (FP16) counterpart, and the upper bound\\nof information entropy H( ÀÜwFP16) in Eq. (6) is correspond-\\ningly reduced 4√ó (4 for 4-bit vs. 16 for 16-bit), meaning a\\nsignificant degradation of information in the quantity and\\nquality. Thus, prioritizing information recovery within low-\\nbit weights is crucial for enhancing quantized LLMs.\\n3\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\n3.2.2. INFORMATION CALIBRATION QUANTIZATION\\nFOR REPRESENTATION RECOVERY\\nTo mitigate the degeneration caused by information loss for\\nthe quantized LLMs, we introduce an Information Calibra-\\ntion Quantization (ICQ) technique for LLMs (as Figure 2),\\nwhich applies the fine-grained information maximization of\\nquantized weights to improve the accuracy.\\nWe first introduce a calibration constant œÑ to the quantizer,\\nliberating its flexibility to retain information fully. The quan-\\ntization process it engages in can be expressed as follows:\\nÀÜwNFk = NFk\\n\\x12w ‚àí œÑ\\ns\\n\\x13\\n.\\n(8)\\nSince the original weight w is fixed, the optimization objec-\\ntive in Eq. (6) can be expressed as\\nargmax\\nœÑ,s\\nH\\n\\x10\\nÀÜwNFk; œÑ, s\\n\\x11\\n.\\n(9)\\nDirectly solving the objective in Eq. (9) is significantly\\ncomplex and time-consuming. Therefore, we then present\\na two-step strategy for calibrating the quantizers of LLMs\\nblockwisely by the information entropy maximization.\\nThe first step is to process the initialization for the cali-\\nbration constant œÑ. Based on the common assumption of a\\nsymmetrical normal distribution for weights of neural net-\\nworks (Dettmers et al., 2023; Baskin et al., 2021), we initial-\\nize the constant as the median œÑ0 = quantile 1\\n2 (w) for each\\nquantization block of weights. As the probability density is\\nhigher in regions closer to the symmetry axis in a normal dis-\\ntribution, this initialization enables the quantizer to utilize\\nintervals to a greater extent. The position-dependent median\\nalso allows œÑ0 to alleviate the influence from outliers.\\nThe second step is to optimize the calibration constant\\nœÑ and the scaling factors s1 and s2 for quantization and\\ndouble quantization of weights, respectively. We apply\\ninformation entropy in Eq. (5) as the metric and perform\\nthe search-based optimization to obtain œÑ ‚àó. We create the\\nsearch spaces for œÑ by linearly dividing [œÑ0 ‚àí ŒªœÉ, œÑ0 + ŒªœÉ]\\ninto 2n candidates, where œÉ = 1 is the standard deviation\\nof N(0, 1) and Œª is a coefficient. We empirically set Œª\\nand n to 0.1 and 100, respectively, to achieve a trade-off\\nbetween accurate and efficient search. For each candidate œÑ,\\nwe use it to calibrate weight and calculate the information\\nentropy of weights quantized by Eq. (8), and then obtain\\nthe optimal calibration constant œÑ ‚àó corresponding to the\\nmaximum entropy. The scaling factor s for w ‚àí œÑ ‚àó is\\ndouble-quantized to sFP8\\n1\\nand sFP16\\n2\\n.\\nFor the optimized calibration constant œÑ ‚àó, we perform dou-\\nble quantization similar to the scale to save memory. The\\nquantization and dequantization processes of our ICQ can\\nbe summarized as\\nÀÜwNFk\\nICQ = NFk\\n\\x12\\nw ‚àí œÑ ‚àó\\nabsmax(w ‚àí œÑ ‚àó)\\n\\x13\\n,\\nÀÜwFP16\\nICQ = ÀÜwNFk\\nICQ dequant(sFP8\\n1 , sFP16\\n2\\n) + dequant(œÑ FP8\\n1\\n, œÑ FP16\\n2\\n).\\n(10)\\nwhere œÑ FP8\\n1\\nand œÑ FP16\\n2\\nare the quantized calibration constant\\nand its scaling factor for double quantization, respectively.\\nThe inference of LLMs with our ICQ can be expressed as\\ny‚Ä≤\\nICQ = x ÀÜwFP16\\nICQ .\\n(11)\\nOur ICQ technique maximizes the information entropy of\\nquantized weight to alleviate its immense information degra-\\ndation and revive the representation capability. As Figure 2\\nshows, the quantized weights calibrated by ICQ derive in-\\ncreased information and recover the original distribution\\nmore accurately following the dequantization.\\n3.3. Information Elastic Connection\\n3.3.1. LIMITATION OF FINETUNABLE LORA\\nIn addition to the quantized LLM in the baseline, the limited\\nrepresentation capability of the finetuneable LoRA also hin-\\nders information recovery. LoRA mitigates the performance\\ndegradation caused by weight quantization in LLM by fine-\\ntuning additional adapters for downstream tasks, sometimes\\neven yielding notable performance enhancements. The fine-\\ntuning of LoRA can be roughly considered as finetuning a\\nsubset of weights in LLM, where low-rank parameters facil-\\nitate an efficient finetuning process, avoiding the expensive\\ncomputation and storage of finetuning the LLM directly.\\nHowever, through LoRA processes parameter-efficient fine-\\ntuning for quantized LLMs, its information representation\\nstill exhibits significant limitations, impeding the accurate\\nquantized LLM with LoRA. Firstly, compared to the cor-\\nresponding linear projection in LLM, the parameter trans-\\nformation by LoRA can still be considered homogenized,\\nalbeit with a noticeably lower rank. On the other hand,\\nthe information utilization of LoRA remains limited, as the\\nlatter low-rank matrix ‚Ñì2 in Eq. (3) can only exploit the\\ntransformed representation matrix ‚Ñì1 from the preceding\\ntransformation, thereby losing accessibility to the original\\nrepresentation information. Therefore, liberating LoRA\\nfrom its constraints in representation capacity is expected to\\nenhance the accuracy of quantized LLM further.\\n3.3.2. INFORMATION ELASTIC CONNECTION FOR\\nINFORMATION ENHANCEMENT\\nTo bolster the representation capacity of LoRA, aiding in re-\\ncovering information of quantized LLMs while maintaining\\nits lightweight nature, we introduce an effective Information\\nElastic Connection (IEC). As Figure 3 shows, IEC con-\\nstructs a parameter-free connection for LoRA, facilitating\\n4\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nùëõ √ó ùëü √ó 4096\\nùëõ √ó ùëü √ó 64\\nùëõ √ó ùëü √ó 4096\\nùëõ √ó ùëü √ó (64 √ó 64)\\n‚Ä¶\\n‚Ä¶\\nùëõ √ó ùëü √ó (64 √ó 1)\\nùëõ √ó ùëü √ó 64\\n‚Ä¶\\nùëõ √ó ùëü √ó (64 √ó 64)\\nùëõ √ó ùëü √ó 4096\\nLoRA (QLoRA)\\nLoRA with IEC (ours)\\nParametric Transformation\\nNon-Parametric Transformation\\nùëõ: #Sample    ùëü: #Length\\nFigure 3: An illustration of IEC in IR-QLoRA\\ninformation utilization derived from quantized LLM and\\ndiversifying the information transformation.\\nAs shown in Eq. (4), the input of one LoRA unit is gen-\\nerated by the previous quantized LLM and LoRA units.\\nThe h-dimensional input x is transformed to low-rank r-\\ndimensional intermediate features through the ‚Ñì1 matrix and\\nthen restored to the o-dimensional output through the ‚Ñì2\\nmatrix. Since usually r ‚â™ min{h, o}, we construct flexi-\\nble parameter-free connections for LoRA matrix pairs so\\nthat each LoRA matrix can fully utilize the original input\\nrepresentation x. Specifically, we group and average the\\noriginal feature according to the greatest common divisor\\nof the input and intermediate dimensions and add it to the\\noutput computed by the ‚Ñì1 matrix. The first sub-unit U1 of\\nLoRA with our IEC can be expressed as\\nU1(x) = x‚Ñì1+\\nŒ≤1\\nr\\ngcd (h,r)\\nY\\n\\uf8eb\\n\\uf8edgcd (h, r)\\nh\\ngcd (h,r)\\nX\\ni=1\\nx\\nh\\n(i‚àí1)\\nh\\ngcd (h,r) :i\\nh\\ngcd (h,r) ‚àí1\\ni\\uf8f6\\n\\uf8f8 ,\\n(12)\\nwhere x[m:n] denotes taking the m to n dimension of x,\\ngcd (h, r) denotes the greatest common divisor of h and\\nr, and Œ≤ denotes a layerwise learnable scalar; P denotes\\nthe summation of all divided features and Q denotes the\\nr\\ngcd (h,r)-time repeated concatenation; The dimension of\\nthe latter term in Eq. (12) is r. Through the above opera-\\ntions, we transform the representation to low-rank through\\nparameter-free operation to retain the original information.\\nThe latter matrix of LoRA transforms the low-rank inter-\\nmediate representation up to the higher dimension. It thus\\naccompanies a parameter-free repeated concatenation for\\nx‚Ä≤ = U1(x) and composes the second sub-unit U2 in IEC,\\nwhere the computation process can be expressed as\\nU2(x‚Ä≤) = x‚Ä≤‚Ñì2+\\nŒ≤2\\no\\ngcd(o,r)\\nY\\n\\uf8eb\\n\\uf8edgcd(o, r)\\nr\\ngcd(o,r)\\nX\\ni=1\\nx‚Ä≤\\nh\\n(i‚àí1)\\nr\\ngcd(o,r) :i\\nr\\ngcd(o,r) ‚àí1\\ni\\uf8f6\\n\\uf8f8 ,\\n(13)\\nwhere x‚Ä≤ is first aligned to the gcd(o, r)-dimension and con-\\ncatenated\\no\\ngcd(o,r) times repeatedly, and is then connected\\nto the calculation result of ‚Ñì2 matrix.\\nNote that in the LoRA unit, the input dimension h and\\noutput dimension o are usually multiples of low-rank r,\\nand gcd(h, r) and gcd(o, r) are thus equal to r. In these\\ncases, Eq. (12) and Eq. (13) for our IEC can be simplified\\nas follows:\\nU1(x) = x‚Ñì1 + Œ≤1\\nr\\nh\\nr\\nX\\ni=1\\nx[(i‚àí1) h\\nr :i h\\nr ‚àí1],\\nU2(x‚Ä≤) = x‚Ä≤‚Ñì2 + Œ≤2\\no\\nrY\\nx‚Ä≤.\\n(14)\\nWith our IEC, the computation process of the quantized\\nLLM projection and LoRA can be expressed as\\ny = y‚Ä≤\\nICQ + Œ±UIEC(x) = y‚Ä≤\\nICQ + Œ±U2 ‚ó¶ U1(x).\\n(15)\\nAppendix A.2 discusses the efficiency of IEC, which can be\\nmerged into LoRA to avoid additional inference costs.\\nThe IEC propagates the input with elasticity dimension\\nchanging, thus allowing the matrix in LoRA to directly ac-\\ncess and utilize the original information extracted by the\\nquantized LLM projection. Moreover, the parameter-free\\nIEC can seem diversified compared with the parametric\\nmatrix multiplication of LoRA, further enhancing the infor-\\nmation representation of quantized LLMs.\\n4. Experiment\\nWe extensively evaluate the accuracy and efficiency\\nof our proposed IR-QLoRA. Our IR-QLoRA is estab-\\nlished upon the LLaMA (Touvron et al., 2023a) and\\nLLaMA2 (Touvron et al., 2023b) families (7B, 13B, 30B,\\nand 65B), and constructs parameter-efficient finetuning on\\nAlpaca (Taori et al., 2023) and Flan v2 (Longpre et al.,\\n2023) datasets. The Massively Multitask Language Under-\\nstanding (MMLU) (Hendrycks et al., 2020) and Common-\\nsenseQA benchmarks(e.g.HellaSwag (Zellers et al., 2019),\\n5\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nTable 1: Accuracy (%) comparison of LLaMA on the\\nMMLU finetuned on the Alpaca dataset\\nMethod\\n#Bit\\nMMLU\\nHums. STEM Social Other Avg.\\nLLaMA-7B\\n16\\n33.3\\n29.8\\n37.8\\n38.0\\n34.6\\nPEQA\\n4\\n34.9\\n28.9\\n37.5\\n40.1\\n34.8\\nNormalFloat\\n4\\n33.1\\n30.6\\n38.8\\n38.8\\n35.1\\nQLoRA w/ GPTQ\\n4\\n33.8\\n31.3\\n37.4\\n42.2\\n36.0\\nQLoRA\\n4\\n36.1\\n31.9\\n42.0\\n44.5\\n38.4\\nQA-LoRA\\n4\\n36.6\\n32.4\\n44.8\\n44.9\\n39.4\\nIR-QLoRA (ours)\\n4\\n38.6\\n34.6\\n45.2\\n45.5\\n40.8\\nLLaMA-13B\\n16\\n40.6\\n36.7\\n48.9\\n48.0\\n43.3\\nNormalFloat\\n4\\n43.0\\n34.5\\n51.8\\n51.4\\n45.0\\nPEQA\\n4\\n43.0\\n37.7\\n53.6\\n49.0\\n45.0\\nQLoRA\\n4\\n45.4\\n37.4\\n55.7\\n54.3\\n48.0\\nQLoRA w/ GPTQ\\n4\\n48.4\\n38.3\\n54.9\\n55.2\\n49.2\\nQA-LoRA\\n4\\n48.4\\n38.3\\n54.9\\n55.2\\n49.2\\nIR-QLoRA (ours)\\n4\\n47.2\\n39.0\\n56.5\\n55.0\\n49.3\\nLLaMA-30B\\n16\\n56.2\\n45.9\\n67.1\\n63.9\\n58.2\\nNormalFloat\\n4\\n55.3\\n44.7\\n66.2\\n63.3\\n57.3\\nQLoRA\\n4\\n55.4\\n46.0\\n66.4\\n63.6\\n57.7\\nQLoRA w/ GPTQ\\n4\\n55.8\\n46.4\\n67.0\\n64.0\\n58.1\\nQA-LoRA\\n4\\n55.8\\n46.4\\n67.0\\n64.0\\n58.1\\nIR-QLoRA (ours)\\n4\\n56.7\\n46.7\\n66.5\\n63.2\\n58.2\\nLLaMA-65B\\n16\\n61.4\\n51.9\\n73.6\\n67.6\\n63.4\\nQA-LoRA\\n4\\n60.8\\n50.5\\n72.5\\n66.7\\n62.5\\nNormalFloat\\n4\\n60.7\\n52.3\\n72.6\\n67.3\\n63.0\\nQLoRA w/ GPTQ\\n4\\n60.4\\n52.5\\n73.0\\n67.2\\n63.0\\nQLoRA\\n4\\n60.3\\n52.7\\n72.9\\n67.4\\n63.1\\nIR-QLoRA (ours)\\n4\\n60.1\\n50.1\\n74.4\\n68.7\\n63.1\\nPIQA (Bisk et al., 2020)) are applied for evaluation. For\\nexperiment settings (Dettmers et al., 2023; Xu et al., 2023b;\\nKim et al., 2023; Frantar et al., 2022), we follow the set-\\ntings of comparison methods reported in their publications\\nor official code for fair comparison. All our experiments are\\nconducted on Nvidia Tesla A100 GPUs. Detailed experi-\\nment settings and results are presented in Appendix B and\\nAppendix C, respectively.\\n4.1. Main Results\\nTo evaluate the performance of IR-QLoRA, we conducted\\ncomprehensive experiments and compared our IR-QLoRA\\nwith the state-of-the-art (SOTA) LoRA-finetuning quanti-\\nzation methods, i.e., QLoRA (Dettmers et al., 2023) and\\nQA-LoRA (Xu et al., 2023b).\\nWe also compare with\\nPEQA (Kim et al., 2023) without LoRA follow (Xu et al.,\\n2023b). Table 1 and Table 2 present the 5-shot accuracy\\nresults on the MMLU benchmark finetuned on the Al-\\npaca (Taori et al., 2023) and Flan v2 (Longpre et al., 2023)\\ndatasets, respectively.\\nComprehensive results indicate that across various sizes\\nof LLaMA models, IR-QLoRA consistently outperforms\\nall comparative quantization methods by a convincing mar-\\ngin. Compared to the baseline method QLoRA, our IR-\\nTable 2: Accuracy (%) comparison of LLaMA on the\\nMMLU finetuned on the Flan v2 dataset\\nMethod\\n#Bit\\nMMLU\\nHums. STEM Social Other Avg.\\nLLaMA-7B\\n16\\n33.3\\n29.8\\n37.8\\n38.0\\n34.6\\nNormalFloat\\n4\\n33.1\\n30.6\\n38.8\\n38.8\\n35.1\\nQLoRA w/ GPTQ\\n4\\n33.8\\n31.3\\n37.4\\n42.2\\n36.0\\nQLoRA\\n4\\n41.4\\n35.0\\n49.8\\n52.0\\n44.3\\nQA-LoRA\\n4\\n43.9\\n38.0\\n54.3\\n53.0\\n47.0\\nIR-QLoRA (ours)\\n4\\n44.2\\n39.3\\n54.5\\n52.9\\n47.4\\nLLaMA-13B\\n16\\n40.6\\n36.7\\n48.9\\n48.0\\n43.3\\nNormalFloat\\n4\\n43.0\\n34.5\\n51.8\\n51.4\\n45.0\\nQLoRA w/ GPTQ\\n4\\n48.4\\n38.3\\n54.9\\n55.2\\n49.2\\nQLoRA\\n4\\n49.9\\n40.1\\n60.2\\n57.9\\n51.9\\nQA-LoRA\\n4\\n50.0\\n41.5\\n60.5\\n58.4\\n52.4\\nIR-QLoRA (ours)\\n4\\n49.2\\n41.2\\n62.1\\n59.2\\n52.6\\nLLaMA-30B\\n16\\n56.2\\n45.9\\n67.1\\n63.9\\n58.2\\nNormalFloat\\n4\\n55.3\\n44.7\\n66.2\\n63.3\\n57.3\\nQLoRA w/ GPTQ\\n4\\n55.8\\n46.4\\n67.0\\n64.0\\n58.1\\nQLoRA\\n4\\n57.2\\n48.6\\n69.8\\n65.2\\n60.0\\nQA-LoRA\\n4\\n57.9\\n48.8\\n71.0\\n65.5\\n60.6\\nIR-QLoRA (ours)\\n4\\n58.1\\n49.4\\n70.7\\n65.8\\n60.8\\nLLaMA-65B\\n16\\n61.4\\n51.9\\n73.6\\n67.6\\n63.4\\nNormalFloat\\n4\\n60.7\\n52.3\\n72.6\\n67.3\\n63.0\\nQLoRA w/ GPTQ\\n4\\n60.4\\n52.5\\n73.0\\n67.2\\n63.0\\nQLoRA\\n4\\n59.8\\n52.9\\n75.0\\n69.6\\n63.9\\nQA-LoRA\\n4\\n57.6\\n51.1\\n73.9\\n67.4\\n62.1\\nIR-QLoRA (ours)\\n4\\n61.6\\n52.0\\n75.6\\n68.9\\n64.3\\nQLoRA achieves a significant improvement in accuracy on\\nthe MMLU benchmark under the same finetuning pipeline.\\nSpecifically, as shown in Table 1, the 4-bit LLaMA-7B\\nmodel finetuned with IR-QLoRA on the Alpaca dataset\\nachieves an accuracy of 40.8%, significantly surpassing the\\nmodel obtained with QLoRA at 38.4%. This outstanding\\ntrend continues in larger LLaMA-13B and LLaMA-30B\\nmodels, where IR-QLoRA exceeds the baseline by 1.3%,\\n0.5%, respectively. Compared to QLoRA with GPTQ and\\nSOTA QA-LoRA using integer quantization, our method\\nconsistently performs better across various settings, with\\na notable advantage of 1.4% even on LLaMA-7B. We fur-\\nther provide results for variants of the IR-QLoRA integer\\nquantizer in Section 4.3 below to demonstrate the robust im-\\nprovement of our techniques in IR-QLoRA across different\\nquantizers.\\nTable 2 presents the results obtained using Flan v2 (Longpre\\net al., 2023) as the finetuning dataset. Similar to the results\\non the Alpaca dataset, IR-QLoRA consistently achieves\\noptimal results and outperforms SOTA methods across var-\\nious settings, and even improves performance after fine-\\ntuning with this dataset. For instance, the comparison of\\nIR-QLoRA results on LLaMA-7B is Alpaca 47.4% vs. Flan\\nv2 40.8%, and the average improvement of IR-QLoRA over\\nQLoRA across various sizes of LLaMA is 1.25%. This indi-\\ncates that IR-QLoRA consistently provides stable benefits\\n6\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nTable 3: Accuracy (%) comparison of LLaMA2 on MMLU\\nMethod\\nDataset #Bit\\nMMLU\\nHums. STEM Social Other Avg.\\nLLaMA2-7B\\n-\\n16\\n43.0\\n36.4\\n51.4\\n52.2 45.5\\nNormalFloat\\n-\\n4\\n42.0\\n35.9\\n51.0\\n51.4 44.8\\nQA-LoRA\\nAlpaca\\n4\\n42.1\\n34.4\\n49.1\\n50.3 43.9\\nIR-QLoRA (ours) Alpaca\\n4\\n43.4\\n36.8\\n51.9\\n53.6 46.2\\nQA-LoRA\\nFlan v2\\n4\\n48.4\\n41.4\\n59.4\\n58.6 51.7\\nIR-QLoRA (ours) Flan v2\\n4\\n49.2\\n41.6\\n60.2\\n58.0 52.0\\nLLaMA2-13B\\n-\\n16\\n53.3\\n44.1\\n63.3\\n61.0 55.3\\nNormalFloat\\n-\\n4\\n52.2\\n44.1\\n62.3\\n60.8 54.7\\nQA-LoRA\\nAlpaca\\n4\\n48.0\\n43.0\\n59.7\\n57.4 51.7\\nIR-QLoRA (ours) Alpaca\\n4\\n51.9\\n43.9\\n61.9\\n60.4 54.4\\nQA-LoRA\\nFlan v2\\n4\\n52.9\\n44.8\\n65.9\\n64.0 56.6\\nIR-QLoRA (ours) Flan v2\\n4\\n53.1\\n45.6\\n64.9\\n63.8 56.5\\nFigure 4: Entropy of linear projections in LLaMA-7B\\nwhen using different finetuning datasets.\\nIn addition, we conduct the accuracy comparison on the\\nrecently proposed LLaMA2, demonstrating the generaliza-\\ntion performance of the proposed IR-QLoRA across LLM\\nfamilies. Specifically, we applied IR-QLoRA to the 7B and\\n13B models of LLaMA2 and compared their evaluation re-\\nsults on the MMLU benchmark with the QA-LoRA method,\\nwhich currently holds SOTA performance. The results in\\nTable 3 show that our method not only achieved 2.7% per-\\nformance improvement but also demonstrated advantages in\\nalmost every individual metric on LLaMA2-7B. These re-\\nsults indicate that IR-QLoRA exhibits strong generalization\\nacross different LLM families.\\n4.2. Ablation Study\\nTo reveal the effectiveness of techniques in the proposed IR-\\nQLoRA on accuracy and efficiency, we conduct extensive\\nablation studies for 4-bit LLaMA-7B on MMLU.\\nAccuracy Ablation. Figure 4 illustrates the entropy of quan-\\ntized weights with ICQ is constantly higher than the vanilla\\nQLoRA across different layers. This indicates that ICQ\\neffectively enhances the mutual information between quan-\\ntized weights of LLMs and original counterparts, thereby\\nreducing information loss and producing a 1.9% accuracy\\ngain, as Table 4 shows. We also conducted different con-\\ntrolled experiments to illustrate the effectiveness of the IEC.\\nFirstly, we apply the IEC directly to Vanilla, resulting in\\n1.8% enhancements, as indicated by the results in Table 4.\\nThen we conduct in-depth observations, applying IEC on\\nTable 4: Accuracy (%) ablation on MMLU\\nMethod\\n#Bit\\nMMLU\\nHums. STEM Social Other Avg.\\nLLaMA-7B\\n16\\n33.3\\n29.8\\n37.8\\n38.0\\n34.6\\nVanilla\\n4\\n36.1\\n31.9\\n42.0\\n44.5\\n38.4\\nICQ\\n4\\n37.9\\n33.6\\n43.9\\n46.7\\n40.3\\nIEC (U1)\\n4\\n37.9\\n31.9\\n43.4\\n44.8\\n39.4\\nIEC (U2)\\n4\\n38.0\\n32.3\\n43.6\\n45.1\\n39.7\\nIEC\\n4\\n38.3\\n33.0\\n44.5\\n45.7\\n40.2\\nIR-QLoRA (ours)\\n4\\n38.6\\n34.6\\n45.2\\n45.5\\n40.8\\n1 ICQ and IEC denote the vanilla QLoRA with ICQ and IEC, and\\nIR-QLoRA uses both of them. IEC (U1) and IEC (U2) denote\\nthe further ablation for IEC in the first or latter LoRA sub-units\\nTable 5: Efficiency ablation on the different sizes of LLaMA\\nMethod\\n#Bit\\n#Params(GB)\\nTime(h)\\n7B\\n13B\\n7B\\n13B\\nLLaMA\\n16\\n12.55\\n24.24\\n-\\n-\\nVanilla\\n4\\n2.34\\n4.13\\n15.33\\n26.18\\nICQ\\n4\\n2.39\\n4.22\\n15.40\\n26.26\\nIEC\\n4\\n2.34\\n4.13\\n15.33\\n26.18\\nIR-QLoRA (ours)\\n4\\n2.39\\n4.22\\n15.40\\n26.26\\nthe first and latter sub-units respectively, both of which\\nlead to improvements, (IEC (U1) 1.0% vs. IEC (U2) 1.3%).\\nMoreover, when ICQ and IEC are combined, their syner-\\ngistic effect surpasses individual contributions, pushing the\\nquantized model to achieve up to 40.8% accuracy.\\nEfficiency Ablation. Table 5 demonstrates that the pro-\\nposed ICQ and IEC techniques impose little additional stor-\\nage and training overhead. For ICQ, the added parameters\\nare only equivalent to the quantized scaling factor, and dou-\\nble quantization is applied to reduce storage further. There-\\nfore, the additional storage introduced by ICQ is minor,\\nincreasing only by 2.04% on the 4-bit LLaMA-7B. The op-\\ntimization process for œÑ also adds only a negligible amount\\nof training time (e.g.0.46% for LLamA-7B and 0.31% for\\nLLaMA-13B). Furthermore, this additional time is exclu-\\nsively required for the initial optimization during training\\nand does not result in increased inference time. IEC just\\nintroduces 2 additional parameters per layer, which can be\\nnegligible in the whole model. In the case of IR-QLoRA,\\nour ICQ and IEC significantly enhance the accuracy per-\\nformance of quantized LLMs with little additional storage\\nincrease. The ablation studies present the effectiveness and\\nefficiency of ICQ and IEC, showcasing their strong capabil-\\nities in constructing accurate and efficient LLMs.\\n4.3. Analysis and Discussion\\nIR-QLoRA on More Evaluation Benchmark. We present\\nthe 0-shot results of the CommonsenseQA benchmark in Ta-\\nble 6. Similar to the phenomenon on the MMLU benchmark,\\nour IR-QLoRA consistently maintains the best average ac-\\n7\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nTable 6: Accuracy (%) comparison on the Commonsense QA datasets\\nMethod\\n#Bit\\nCommonsenseQA\\nHellaSwag\\nPIQA\\nWinoGrande\\nARC-e\\nARC-c\\nBoolQ\\nOBQA\\nAvg.\\nLLaMA-7B\\n16\\n56.3\\n78.2\\n67.1\\n67.3\\n38.2\\n72.9\\n28.4\\n58.3\\nNormalFloat\\n4\\n56.7\\n78.7\\n70.6\\n75.7\\n41.6\\n74.7\\n33.2\\n61.6\\nQLoRA w/ GPTQ\\n4\\n57.4\\n77.6\\n66.2\\n70.9\\n41.8\\n73.5\\n31.2\\n59.8\\nQLoRA\\n4\\n61.8\\n78.1\\n68.4\\n75.8\\n43.6\\n73.7\\n32.8\\n62.0\\nQA-LoRA\\n4\\n58.6\\n78.0\\n66.9\\n71.2\\n43.9\\n79.9\\n34.0\\n61.8\\nIR-QLoRA (ours)\\n4\\n54.7\\n78.8\\n72.6\\n76.6\\n45.1\\n80.6\\n37.2\\n63.7\\nTable 7: Accuracy (%) comparison under 2-3 bits on MMLU\\nMethod\\nDatasets #Bit\\nMMLU\\nHums. STEM Social Other Avg.\\nLLaMA-7B\\n-\\n16\\n33.3\\n29.8\\n37.8\\n38.0 34.6\\nNormalFloat\\n-\\n3\\n30.5\\n29.9\\n34.8\\n34.9 32.3\\nQLoRA w/ GPTQ\\nAlpaca\\n3\\n31.6\\n30.1\\n35.6\\n39.8 34.0\\nQLoRA\\nAlpaca\\n3\\n35.8\\n32.1\\n40.7\\n43.1 37.8\\nQA-LoRA\\nAlpaca\\n3\\n35.6\\n30.5\\n41.5\\n42.7 37.4\\nIR-QLoRA (ours) Alpaca\\n3\\n36.0\\n33.9\\n42.2\\n42.7 38.4\\nQLoRA w/ GPTQ\\nFlan v2\\n3\\n32.2\\n31.7\\n42.7\\n42.8 36.9\\nQLoRA\\nFlan v2\\n3\\n41.3\\n37.1\\n50.9\\n49.8 44.5\\nQA-LoRA\\nFlan v2\\n3\\n41.3\\n36.0\\n52.8\\n50.2 44.7\\nIR-QLoRA (ours) Flan v2\\n3\\n43.0\\n37.7\\n52.3\\n51.7 45.9\\nNormalFloat\\n-\\n2\\n24.2\\n28.9\\n31.1\\n25.0 26.9\\nQLoRA w/ GPTQ\\nAlpaca\\n2\\n23.4\\n26.2\\n26.4\\n28.4 25.8\\nQLoRA\\nAlpaca\\n2\\n24.0\\n27.0\\n27.5\\n26.7 26.2\\nQA-LoRA\\nAlpaca\\n2\\n27.3\\n26.1\\n26.1\\n30.3 27.5\\nIR-QLoRA (ours) Alpaca\\n2\\n26.0\\n27.8\\n30.2\\n28.3 27.8\\nQLoRA w/ GPTQ\\nFlan v2\\n2\\n23.9\\n25.3\\n26.2\\n25.3 25.0\\nQLoRA\\nFlan v2\\n2\\n31.8\\n28.7\\n36.7\\n37.7 33.5\\nQA-LoRA\\nFlan v2\\n2\\n31.8\\n28.1\\n34.5\\n38.5 33.2\\nIR-QLoRA (ours) Flan v2\\n2\\n31.7\\n29.4\\n37.8\\n36.5 33.7\\ncuracy for LLaMA-7B on the CommonsenseQA benchmark\\ncompared to SOTA methods, and also significantly improves\\nthe effectiveness in the majority of sub-items. More evalua-\\ntion results are presented in Appendix C.3.\\nIR-QLoRA under Ultra-low Bit-width. We have evalu-\\nated and compared the proposed IR-QLoRA under ultra-low\\nbit-width. Specifically, we employed the quantization meth-\\nods from QLoRA (Dettmers et al., 2023) and LoftQ (Li\\net al., 2023), following the percentile quantization approach\\nto construct NF2 and NF3 quantization. Additionally, we ad-\\nhered to the 2-bit and 3-bit integer quantization results from\\nQA-LoRA and QLoRA with GPTQ as presented in (Xu\\net al., 2023b). Table 7 demonstrates that as the quantization\\nbit-width decreases, the performance of the baseline QLoRA\\nsharply declines, to the extent that its performance in the 2-\\nbit scenario is similar to random. In contrast, our IR-QLoRA\\nexhibits superior performance, with only a 0.9% accuracy\\ndifference compared to the 16-bit counterpart when fine-\\ntuning a 2-bit model on the Flan v2 dataset. These results\\nstrongly indicate the competitiveness of our IR-QLoRA in\\nthe realm of ultra-low bit-width. The quantization values\\nfor NF quantization are presented in Appendix B.2.\\nTable 8: IR-QLoRA variant based on QA-LoRA on MMLU\\nMethod\\n#Bit\\nMMLU\\nHums. STEM Social Other Avg.\\nLLaMA-7B\\n16\\n33.3\\n29.8\\n37.8\\n38.0\\n34.6\\nQA-LoRA\\n4\\n36.6\\n32.4\\n44.8\\n44.9\\n39.4\\nIR-QLoRA (QA-LoRA)\\n4\\n37.3\\n32.8\\n43.8\\n46.7\\n39.9\\nIR-QLoRA with Integer Quantizer. We demonstrate the\\nstrong generality of the technology in IR-QLoRA across dif-\\nferent quantization frameworks and present variants based\\non QA-LoRA as the baseline. In this variant, ICQ performs\\nsearching for the zero point and determines it along with the\\nscaling factor for integer quantizers, while the universality\\nof IEC for LoRA allows direct application for the QA-LoRA\\nbaseline. As shown in Table 8, experiments reveal a sig-\\nnificant improvement in accuracy for IR-QLoRA under the\\nsame quantizer form and neural architecture. Furthermore,\\nwe emphasize that due to the zero point carried by inte-\\nger quantizers themselves, where the calibration constant\\nœÑICQ in our ICQ can be merged, the improvements brought\\nabout by our techniques come at almost zero cost. These\\nexperiments indicate that the techniques in our QA-LoRA\\ncan be effectively integrated into various LoRA-finetuning\\nquantization methods for LLMs and bring general benefits.\\n5. Conclusion\\nThis paper introduces the IR-QLoRA, designed to accu-\\nrately quantize LLMs with LoRA-finetuning via informa-\\ntion retention. This framework leverages two key tech-\\nnologies: statistics-based Information Calibration Quan-\\ntization, which ensures that the quantized parameters of\\nthe LLM accurately retain the original information; and\\nfinetuning-based Information Elastic Connection, enabling\\nLoRA to employ elastic representation transformation with\\ndiverse information. Extensive experiments validate that IR-\\nQLoRA delivers convincing accuracy improvements across\\nthe LLaMA and LLaMA2 families, even with 2-4 bit-widths,\\naccompanied by a minimal 0.45% increase in time con-\\nsumption. Remarkably versatile, IR-QLoRA seamlessly\\nintegrates with various quantization frameworks. In a nut-\\nshell, our IR-QLoRA significantly advances the accuracy of\\nLoRA-finetuning quantization for LLMs, facilitating practi-\\ncal deployment in resource-constrained scenarios.\\n8\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\n6. Impact Statements\\nThis paper presents work whose goal is to advance the field\\nof Machine Learning. There are many potential societal\\nconsequences of our work, none which we feel must be\\nspecifically highlighted here.\\nReferences\\nAgarwal, R., Vieillard, N., Stanczyk, P., Ramos, S., Geist, M., and\\nBachem, O. Gkd: Generalized knowledge distillation for auto-\\nregressive sequence models. arXiv preprint arXiv:2306.13649,\\n2023.\\nBaskin, C., Liss, N., Schwartz, E., Zheltonozhskii, E., Giryes,\\nR., Bronstein, A. M., and Mendelson, A.\\nUniq: Uniform\\nnoise injection for non-uniform quantization of neural networks.\\nACM Transactions on Computer Systems (TOCS), 37(1-4):1‚Äì15,\\n2021.\\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about\\nphysical commonsense in natural language. In Proceedings of\\nthe AAAI conference on artificial intelligence, volume 34, pp.\\n7432‚Äì7439, 2020.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,\\nA., et al. Language models are few-shot learners. Advances in\\nneural information processing systems, 33:1877‚Äì1901, 2020.\\nChang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang,\\nL., Yi, X., Wang, C., Wang, Y., et al. A survey on evaluation of\\nlarge language models. arXiv preprint arXiv:2307.03109, 2023.\\nChitty-Venkata, K. T., Mittal, S., Emani, M., Vishwanath, V.,\\nand Somani, A. K. A survey of techniques for optimizing\\ntransformer inference. Journal of Systems Architecture, pp.\\n102990, 2023.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G.,\\nRoberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann,\\nS., et al. Palm: Scaling language modeling with pathways.\\nJournal of Machine Learning Research, 24(240):1‚Äì113, 2023.\\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M.,\\nand Toutanova, K. Boolq: Exploring the surprising difficulty\\nof natural yes/no questions. arXiv preprint arXiv:1905.10044,\\n2019.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved question\\nanswering? try arc, the ai2 reasoning challenge. arXiv preprint\\narXiv:1803.05457, 2018.\\nDettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-\\nbit optimizers via block-wise quantization.\\narXiv preprint\\narXiv:2110.02861, 2021.\\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.\\nint8 (): 8-bit matrix multiplication for transformers at scale.\\narXiv preprint arXiv:2208.07339, 2022.\\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.\\nQlora: Efficient finetuning of quantized llms. arXiv preprint\\narXiv:2305.14314, 2023.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-\\ntraining of deep bidirectional transformers for language under-\\nstanding. arXiv preprint arXiv:1810.04805, 2018.\\nFrantar, E. and Alistarh, D. Sparsegpt: Massive language models\\ncan be accurately pruned in one-shot. In International Confer-\\nence on Machine Learning, pp. 10323‚Äì10337. PMLR, 2023.\\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\\nAccurate post-training quantization for generative pre-trained\\ntransformers. arXiv preprint arXiv:2210.17323, 2022.\\nGanesh, P., Chen, Y., Lou, X., Khan, M. A., Yang, Y., Sajjad, H.,\\nNakov, P., Chen, D., and Winslett, M. Compressing large-scale\\ntransformer-based models: A case study on bert. Transactions\\nof the Association for Computational Linguistics, 9:1061‚Äì1080,\\n2021.\\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\\nA., Foster, C., Golding, L., Hsu, J., Le Noac‚Äôh, A., Li,\\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J.,\\nReynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang,\\nE., Thite, A., Wang, B., Wang, K., and Zou, A. A frame-\\nwork for few-shot language model evaluation, 12 2023. URL\\nhttps://zenodo.org/records/10256836.\\nGu, Y., Dong, L., Wei, F., and Huang, M. Knowledge distillation\\nof large language models. arXiv preprint arXiv:2306.08543,\\n2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song,\\nD., and Steinhardt, J. Measuring massive multitask language\\nunderstanding. In International Conference on Learning Repre-\\nsentations, 2020.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song,\\nD., and Steinhardt, J. Measuring massive multitask language\\nunderstanding. Proceedings of the International Conference on\\nLearning Representations (ICLR), 2021.\\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S.,\\nWang, L., and Chen, W. Lora: Low-rank adaptation of large\\nlanguage models. arXiv preprint arXiv:2106.09685, 2021.\\nHuang, J. and Chang, K. C.-C. Towards reasoning in large lan-\\nguage models: A survey. arXiv preprint arXiv:2212.10403,\\n2022.\\nHuang, Y., Chen, Y., Yu, Z., and McKeown, K. In-context learning\\ndistillation: Transferring few-shot learning ability of pre-trained\\nlanguage models. arXiv preprint arXiv:2212.10670, 2022.\\nJiang, Y., Chan, C., Chen, M., and Wang, W. Lion: Adversarial dis-\\ntillation of closed-source large language model. arXiv preprint\\narXiv:2305.12870, 2023.\\nKim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., and\\nLee, D. Memory-efficient fine-tuning of compressed large lan-\\nguage models via sub-4-bit integer quantization. arXiv preprint\\narXiv:2305.14152, 2023.\\nLee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Lessons\\nlearned from activation outliers for weight quantization in large\\nlanguage models. arXiv preprint arXiv:2306.02272, 2023.\\nLi, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H.,\\nQian, J., Peng, B., Mao, Y., et al. Explanations from large\\nlanguage models make small reasoners better. arXiv preprint\\narXiv:2210.06726, 2022.\\n9\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nLi, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen, W., and\\nZhao, T. Loftq: Lora-fine-tuning-aware quantization for large\\nlanguage models. arXiv preprint arXiv:2310.08659, 2023.\\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq:\\nActivation-aware weight quantization for llm compression and\\nacceleration. arXiv preprint arXiv:2306.00978, 2023.\\nLiu, P., Liu, Z., Gao, Z.-F., Gao, D., Zhao, W. X., Li, Y., Ding,\\nB., and Wen, J.-R. Do emergent abilities exist in quantized\\nlarge language models: An empirical study. arXiv preprint\\narXiv:2307.08072, 2023a.\\nLiu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi,\\nY., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free\\nquantization aware training for large language models. arXiv\\npreprint arXiv:2305.17888, 2023b.\\nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y.,\\nZhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The flan collection:\\nDesigning data and methods for effective instruction tuning.\\narXiv preprint arXiv:2301.13688, 2023.\\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit\\nof armor conduct electricity? a new dataset for open book\\nquestion answering. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, pp.\\n2381‚Äì2391, 2018.\\nQin, H., Zhang, X., Gong, R., Ding, Y., Xu, Y., and Liu, X.\\nDistribution-sensitive information retention for accurate binary\\nneural network. International Journal of Computer Vision, 131\\n(1):26‚Äì47, 2023.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Wino-\\ngrande: An adversarial winograd schema challenge at scale.\\nCommunications of the ACM, 64(9):99‚Äì106, 2021.\\nShao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang,\\nK., Gao, P., Qiao, Y., and Luo, P. Omniquant: Omnidirection-\\nally calibrated quantization for large language models. arXiv\\npreprint arXiv:2308.13137, 2023.\\nSun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective\\npruning approach for large language models. arXiv preprint\\narXiv:2306.11695, 2023.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C.,\\nLiang, P., and Hashimoto, T. B. Stanford alpaca: an instruction-\\nfollowing llama model (2023). URL https://github. com/tatsu-\\nlab/stanford alpaca, 2023.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A.,\\nLacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al.\\nLlama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,\\nBabaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S.,\\net al. Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288, 2023b.\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,\\nKhashabi, D., and Hajishirzi, H. Self-instruct: Aligning lan-\\nguage model with self generated instructions. arXiv preprint\\narXiv:2212.10560, 2022.\\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.\\nSmoothquant: Accurate and efficient post-training quantization\\nfor large language models. In International Conference on\\nMachine Learning, pp. 38087‚Äì38099. PMLR, 2023.\\nXu, M., Xu, Y. L., and Mandic, D. P. Tensorgpt: Efficient compres-\\nsion of the embedding layer in llms based on the tensor-train\\ndecomposition. arXiv preprint arXiv:2307.00526, 2023a.\\nXu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen,\\nZ., Zhang, X., and Tian, Q.\\nQa-lora: Quantization-aware\\nlow-rank adaptation of large language models. arXiv preprint\\narXiv:2309.14717, 2023b.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\nHellaswag: Can a machine really finish your sentence?\\nIn\\nProceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, pp. 4791‚Äì4800, 2019.\\nZhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B., et al.\\nPruning meets low-rank parameter-efficient fine-tuning. arXiv\\npreprint arXiv:2305.18403, 2023.\\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y.,\\nZhang, B., Zhang, J., Dong, Z., et al. A survey of large language\\nmodels. arXiv preprint arXiv:2303.18223, 2023.\\nZhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on\\nmodel compression for large language models. arXiv preprint\\narXiv:2308.07633, 2023.\\n10\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nA. Details about IR-QLoRA\\nA.1. Details about Related Works\\nLarge Language Models. LLMs have demonstrated remarkable proficiency across diverse natural language understanding\\ntasks and are established as a prominent paradigm in this field (Chang et al., 2023; Devlin et al., 2018; Zhao et al., 2023;\\nHuang & Chang, 2022). Recent noteworthy instances of LLMs encompass OpenAI‚Äôs GPT family (Brown et al., 2020) and\\nMeta‚Äôs LLaMA and LLaMA2 families (Touvron et al., 2023a;b). Nonetheless, the exceptional performance of these LLMs\\nis contingent upon extensive parameters and computational resources. Notably, the PaLM-540B model boasts an impressive\\n540 billion parameters (Chowdhery et al., 2023), underscoring the substantial computational demands. This reality poses\\nsubstantial challenges to deploying LLMs in settings with limited resources. Consequently, the research of the compression\\ntechnologies for LLMs has gained prominence as a critical area of research.\\nCompression of LLMs. Existing compression technologies of LLMs include pruning, distillation, low-rank decomposition,\\nand low-bit quantization (Ganesh et al., 2021; Zhu et al., 2023; Chitty-Venkata et al., 2023). Pruning removes redundant\\nparameters in LLMs structurally or unstructuredly (Frantar & Alistarh, 2023; Zhang et al., 2023; Sun et al., 2023). Distillation\\nenables the compressed student LLMs to learn from intermediate features or predictions of a larger teacher model (Gu et al.,\\n2023; Huang et al., 2022; Agarwal et al., 2023; Li et al., 2022; Jiang et al., 2023). Low-rank decomposition saves computation\\nby decomposing the weight of LLMs into smaller matrices with significantly reduced dimensions (Xu et al., 2023a). Different\\nfrom the above technologies that mainly reduce the number of parameters (weights and/or activations), quantization aims to\\ncompress the LLMs from 16-bit floating-point to lower bit-widths to mitigate the storage and computation.\\nQuantization of LLMs. Since compression is from a generic bit-width perspective, quantization has become a popular\\nmethod to obtain efficient LLMs (Frantar et al., 2022; Xiao et al., 2023). The most common practice is directly performing\\npost-training quantization on pre-trained LLMs and optimizing quantizers through calibration (Lin et al., 2023; Lee et al.,\\n2023; Shao et al., 2023; Dettmers et al., 2022), which usually results in non-negligible degradation. Some quantization-aware\\ntraining methods finetune the parameters of quantized LLMs to improve accuracy (Liu et al., 2023b; Kim et al., 2023),\\nwhile the computational burden brought is significantly expensive. The LoRA-finetuning quantization of LLMs emerges to\\nachieve a balanced trade-off between computational cost and accuracy (Dettmers et al., 2023; Xu et al., 2023b; Li et al.,\\n2023), where LLMs are first quantized and then finetuned with a parameter-efficient LoRA. However, existing quantized\\nLLMs with LoRA are still far from ideal in accuracy.\\nA.2. Details about Pipeline\\nIn this section, we demonstrate the application of Information Calibration Quantization (ICQ) and Information Elastic\\nConnection (IEC) in our IR-QLoRA in detail when we quantizing the LLMs.\\nAlgorithm 1 The weight search process within each block in IR-QLoRA\\nInput: Block Weight w, hypermeters Œª, œÉ, n\\nOutput: Calibration constant œÑ FP8\\n1\\n, œÑ FP16\\n2\\nInitialize œÑ0 = quantile 1\\n2 (w) , H‚àó = 0\\nfor œÑ in Linspace(œÑ0 ‚àí ŒªœÉ, œÑ0 + ŒªœÉ, ŒªœÉ\\nn ) do\\nw = w ‚àí œÑ\\nÀÜw = NFk(w/ absmax(w))\\nCalculate the probability P(qi) of the ÀÜw taking the value qi\\nH = P2k‚àí1\\ni=0 P(qi) log2 P(qi)\\nif H > H‚àó then\\nUpdate œÑ ‚àó and H‚àó\\nend if\\nend for\\nœÑ FP8\\n1\\n= FP8(œÑ ‚àó/œÑ FP16\\n2\\n) = FP8(œÑ ‚àó/ absmax(œÑ ‚àó))\\nPipeline of ICQ. The targets of ICQ are all weights that need to be quantized. In the initial phase, these weights are\\npartitioned into distinct blocks according to blocksize B. Thereafter, an optimization process is engaged, aiming to\\nidentify a calibration constant that is optimal for maximizing the entropy of the quantized weights within each block. The\\n11\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nimplementation details of the above functions are listed in Algorithm 1.\\nNote that the quantization of œÑ is a discretionary choice, contingent upon the preference for either elevated accuracy or\\ndiminished storage requirements. By default, we elect to implement the quantization of œÑ as a standard protocol.\\nBesides, in integer quantization like GPTQ (Frantar et al., 2022), the ICQ strategy remains applicable for refining the entropy\\nprofile of quantized weights. This is actualized by the integration of the weights‚Äô entropy into the loss function during the\\ncalibration phase, thereby facilitating precision calibration.\\nAlgorithm 2 The inference process of the linear layer in IR-QLoRA\\nInput: Input tensor x ‚àà Rb√óh, original weights w ‚àà Ro√óh, low-rank paramaters ‚Ñì1 ‚àà Rh√ór, ‚Ñì2 ‚àà Rr√óo, parameters\\nŒ±, Œ≤1, Œ≤2\\nOutput: Output tensor y ‚àà Rb√óo\\nÀÜwNFk\\nICQ , sFP8\\n1 , sFP16\\n2\\n, œÑ FP8\\n1\\n, œÑ FP16\\n2\\n‚Üê search process for w\\nÀÜwFP16\\nICQ = ÀÜwNFk\\nICQ dequant(sFP8\\n1 , sFP16\\n2\\n) + dequant(œÑ FP8\\n1\\n, œÑ FP16\\n2\\n)\\ny‚Ä≤\\nICQ = x ÀÜwFP16\\nICQ\\nx‚Ä≤ = U1(x) = x‚Ñì1 + Œ≤1 r\\nh\\nrP\\ni=1\\nx[(i‚àí1) h\\nr :i h\\nr ‚àí1]\\nUIEC(x) = U2(x‚Ä≤) = x‚Ä≤‚Ñì2 + Œ≤2\\nrQ x‚Ä≤[0:r‚àí1]\\ny = y‚Ä≤\\nICQ + Œ±UIEC(x)\\nPipeline of IEC. The targets of IEC are specifically focused on the low-rank adaptations added in linear layers. We modified\\nthe forward function of LoRA, and the current implementation of the forward function is shown in Algorithm 2.\\nThe parameters Œ≤1 and Œ≤2 introduced by the IEC can be integrated into the learned LoRA adapters to avoid additional\\ninference costs. Specificly, by defining the matrix e‚Ñì1 ‚àà Rh√ór and e‚Ñì2 ‚àà Rr√óo as Eq. (16):\\ne‚Ñì1{i,j} =\\n(\\n‚Ñì1{i,j} + Œ≤1\\ngcd(h,r)\\nh\\nif ‚åäi/ gcd(h,r)\\nh\\n‚åã = ‚åäj/ gcd(h, r)‚åã,\\n‚Ñì1{i,j}\\notherwise,\\ne‚Ñì2{i,j} =\\n(\\n‚Ñì2{i,j} + Œ≤2\\ngcd(o,r)\\nr\\nif ‚åäi/ gcd(o,r)\\nr\\n‚åã = ‚åäj/ gcd(o, r)‚åã,\\n‚Ñì2{i,j}\\notherwise,\\n(16)\\nwhere {i, j} is the index of the matrix, and ‚åä¬∑‚åã denotes the floor operation.\\nThe function U1(x) and U2(x) can be further redefined, as Eq. (17):\\nU1(x) = x e‚Ñì1 = x‚Ñì1 + Œ≤1\\nr\\ngcd (h,r)\\nY\\n\\uf8eb\\n\\uf8edgcd (h, r)\\nh\\ngcd (h,r)\\nX\\ni=1\\nx[(i‚àí1)\\nh\\ngcd (h,r) :i\\nh\\ngcd (h,r) ‚àí1]\\n\\uf8f6\\n\\uf8f8 ,\\nU2(x) = x e‚Ñì2 = x‚Ñì2 + Œ≤2\\no\\ngcd (o,r)\\nY\\n\\uf8eb\\n\\uf8edgcd (o, r)\\nr\\ngcd (o,r)\\nX\\ni=1\\nx[(i‚àí1)\\nr\\ngcd(o,r) :i\\nr\\ngcd(o,r) ‚àí1]\\n\\uf8f6\\n\\uf8f8 .\\n(17)\\nTo summarize, ICQ and IEC within IR-QLoRA exhibit remarkable generality and both can be seamlessly integrated into any\\nmethods for LoRA-finetuning quantization of LLMs (e.g.QLoRA (Dettmers et al., 2023), QA-LoRA (Xu et al., 2023b)).\\nMoreover, their implementation incurs minimal overhead.\\n12\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nB. Experiment Settings\\nB.1. Models and Datasets\\nFor models, we establish IR-QLoRA upon the LLaMA (Touvron et al., 2023a) and LLaMA2 (Touvron et al., 2023b) families.\\nSpecifically, we finetune the 7B, 13B, 30B, and 65B models of LLaMA and the 7B and 13B models of LLaMA2.\\nFor datasets, we choose Alpaca (Taori et al., 2023) and FLAN v2 (Longpre et al., 2023) as our finetuning datasets. Alpaca\\ncontains 52K instruction-following data generated from text-davinci-003 (GPT 3.5) (Wang et al., 2022). Flan v2 is a\\ncollection of 1,836 tasks combining the mixture with CoT, Muffin, T0-SF, and NIV2.\\nB.2. NormalFloat Quantization\\nQuantile Quantization (Dettmers et al., 2021) is an information-theoretically optimal data type that ensures each quantization\\nbin has an equal number of values from the input tensor. Essentially, it distributes the data evenly across the available\\nquantization levels, leading to efficient and balanced utilization of the quantized representation.\\nBuilding on Quantile Quantization, NormalFloat (NF) Quantization (Dettmers et al., 2023) introduces the quantization\\nprinciple that the weights conform to a zero-centered normal distribution. By converting all weights into a fixed distribution,\\ntheir distribution is fully adapted to the range of the specified data type. Then the quantile constant can be calculated.\\nFollowing the processing steps in QLoRA (Dettmers et al., 2023), we can get different quantized points according to\\ndifferent quantized bit widths. Table 9-11 shows the NF quantization for 2-4 bits, respectively. Note that we use symmetrical\\nsettings in NF2 to prevent excessive deviation of information.\\nTable 9: The exact values of the NormalFloat 2-bit (NF2) data type\\nIndex\\nValue\\nIndex\\nValue\\n0\\n-1.0\\n2\\n0.2525685131549835\\n1\\n-0.25256848335266113\\n3\\n1.0\\nTable 10: The exact values of the NormalFloat 3-bit (NF3) data type\\nIndex\\nValue\\nIndex\\nValue\\n0\\n-1.0\\n4\\n0.16093020141124725\\n1\\n-0.4786292016506195\\n5\\n0.33791524171829224\\n2\\n-0.217141792178154\\n6\\n0.5626170039176941\\n3\\n0.0\\n7\\n1.0\\nTable 11: The exact values of the NormalFloat 4-bit (NF4) data type\\nIndex\\nValue\\nIndex\\nValue\\n0\\n-1.0\\n8\\n0.07958029955625534\\n1\\n-0.6961928009986877\\n9\\n0.16093020141124725\\n2\\n-0.5250730514526367\\n10\\n0.24611230194568634\\n3\\n-0.39491748809814453\\n11\\n0.33791524171829224\\n4\\n-0.28444138169288635\\n12\\n0.44070982933044434\\n5\\n-0.18477343022823334\\n13\\n0.5626170039176941\\n6\\n-0.09105003625154495\\n14\\n0.7229568362236023\\n7\\n0.0\\n15\\n1.0\\nB.3. Evaluation Metrics\\nFollowing QLoRA (Dettmers et al., 2023) and QA-LoRA (Xu et al., 2023b), we evaluate language understanding capa-\\nbilities of the LLMs on Massively Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2020) and\\ncommonsense reasoning ability on several Common Sense QA datasets.\\n13\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nMMLU includes 57 language tasks, including humanities (Hums.), STEM, social science, etc. We utilize the MMLU evalua-\\ntion script and prompts as outlined in (Hendrycks et al., 2021). Common Sense QA datasets includes HellaSwag (Zellers\\net al., 2019), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), ARC (Clark et al., 2018), BoolQ (Clark et al.,\\n2019), and OpenBookQA (Mihaylov et al., 2018). We adopt lm-evalharness (Gao et al., 2023) to produce the Common\\nSense QA results. Table 12 provides examples of each evaluation dataset. Given the multiple-choice (MC) nature of these\\ntasks, accuracy is selected as the primary metric for evaluation.\\nB.4. Implementation Details\\nAll experiments are conducted on Tesla A100 GPUs. Following (Dettmers et al., 2023), we apply the double quantization\\nmechanism, and set the block size is 64 for quantization and 256 for double quantization. Regarding LoRA parameters, we\\nset r = 64, Œ± = 16, and LoRA dropout of 0.1 for models up to 13B and 0.05 for 33B and 65B models. We employ the\\npaged AdamW optimizer with a beta2 value of 0.999, and a learning rate of 2e-4 for models up to 13B and 1e-4 for 33B and\\n65B models., limiting the maximum gradient norm to 0.3 and adopting a constant learning rate strategy. Fine-tuning was\\nexecuted for 10,000 and 20,000 steps on the Alpaca and FLAN v2 datasets, respectively, utilizing a batch size 16.\\nTable 12: Examples for each the evaluation datasets\\nDataset\\nQuestion\\nAnswer\\nMMLU\\nWhich of the following factors is associated with a decreased risk of Alzheimer‚Äôs?\\nB\\n(A) Being African or Hispanic American\\n(B) Eating fish\\n(C) A lower level of education\\n(D) Being married\\nHellaSwag\\nA man is sitting on a roof. he\\nD\\n(A) is using wrap to wrap a pair of skis.\\n(B) is ripping level tiles off.\\n(C) is holding a rubik‚Äôs cube.\\n(D) starts pulling up roofing on a roof.\\nPIQA\\nHow do I ready a guinea pig cage for it‚Äôs new occupants?\\nA\\n(A) Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper\\nstrips, you will also need to supply it with a water bottle and a food dish.\\n(B) Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans\\nmaterial, you will also need to supply it with a water bottle and a food dish.\\nWinoGrande\\nJohn moved the couch from the garage to the backyard to create space. The\\nis small.\\nA\\n(A) garage\\n(B) backyard\\nARC-Easy\\nWhich factor will most likely cause a person to develop a fever?\\nB\\n(A) a leg muscle relaxing after exercise\\n(B) a bacterial population in the bloodstream\\n(C) several viral particles on the skin\\n(D) carbohydrates being digested in the stomach\\nARC-Challenge\\nGeorge wants to warm his hands quickly by rubbing them. Which skin surface will produce the\\nmost heat?\\nA\\n(A) dry palms\\n(B) wet palms\\n(C) palms covered with oil\\n(D) palms covered with lotion\\nBoolQ\\nPhantom pain sensations are described as perceptions that an individual experiences relating to a\\nlimb or an organ that is not physically part of the body. Limb loss is a result of either removal by\\namputation or congenital limb deficiency. However, phantom limb sensations can also occur\\nfollowing nerve avulsion or spinal cord injury. Is pain experienced in a missing body part or\\nparalyzed area?\\nB\\n(A) True\\n(B) False\\nOpenBookQA\\nA magnet will stick to\\nA\\n(A) a belt buckle\\n(B) a wooden table,\\n(C) a plastic cup\\n(D) a paper plate\\n14\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\n(a) Query Linear Projection\\n(b) Value Linear Projection\\n(c) Output Linear Projection\\n(d) MLP Up Linear Projection\\n(e) MLP Down Linear Projection\\n(f) MLP Gate Linear Projection\\nFigure 5: Entropy comparison of linear projections in 4-bit LLaMA-7B\\nC. Experiment Results\\nC.1. Weight Visualization\\nIn addition to the linear projection for Key mentioned earlier, we also compute the entropy of quantized weights in various\\nother types of layers, as illustrated in Figure 5. This observation confirms that ICQ effectively boosts the information\\nentropy of weights and augments the mutual information between the weights of quantized and original LLMs, consequently\\nalleviating the constraints on representational capacity imposed by quantization.\\nC.2. Efficiency Ablation\\nWe counted the number of parameters and training time for different sizes of LLaMA, as shown in Table 13. It is evident\\nthat IEC introduces a minimal number of additional parameters and does not increase the training time. In the case of ICQ, a\\nsmall number of extra parameters are introduced, and the increase in training time is only incurred once, as the results can\\nbe efficiently cached after ICQ is applied. Thus, IR-QLoRA maintains nearly the same efficiency as Vanilla.\\nTable 13: Efficiency ablation on the different sizes of LLaMA\\nMethod\\n#Bit\\n#Params(GB)\\nTime(h)\\n7B\\n13B\\n30B\\n65B\\n7B\\n13B\\n30B\\n65B\\nLLaMA\\n16\\n12.55\\n24.24\\n60.59\\n121.60\\n-\\n-\\n-\\n-\\nVanilla\\n4\\n2.34\\n4.13\\n9.41\\n18.02\\n15.33\\n26.18\\n42.71\\n118.97\\nICQ\\n4\\n2.39\\n4.22\\n9.65\\n18.50\\n15.40\\n26.26\\n43.07\\n119.38\\nIEC\\n4\\n2.34\\n4.13\\n9.41\\n18.02\\n15.33\\n26.18\\n42.71\\n118.97\\nIR-QLoRA (ours)\\n4\\n2.39\\n4.22\\n9.65\\n18.50\\n15.40\\n26.26\\n43.07\\n119.38\\n15\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nC.3. Case Study\\nThis section offers a qualitative analysis that contrasts the performance of our IR-QLoRA model with that of QLoRA within\\nthe domain of LLM quantization. Utilizing prompts from the Stanford Alpaca dataset (Taori et al., 2023) for text generation,\\nwe indicate definitive answers in blue and unexpected responses in red for clarity. Our qualitative assessment accentuates the\\nenhanced capabilities of IR-QLoRA in intricate understanding and text generation quality, compared to the QLoRA baseline.\\nCognitive Reasoning. The cognitive reasoning aptitude of IR-QLoRA has shown notable advancement. When tasked with\\nchallenges that require logical deduction and computational acumen, such as those presented in arithmetic case studies and\\nfamily relationship inference, as illustrated in Table 14, IR-QLoRA demonstrates a commendable level of proficiency. These\\nresults underscore its inherent reasoning capabilities and shed light on the potential for cognitive reasoning in quantized,\\nsmaller-scale models like QLoRA-7B. Both IR-QLoRA-7B and IR-QLoRA-13B provided correct answers and demonstrated\\na capacity to explain their reasoning to a certain degree. In contrast, QLoRA-7B failed to answer correctly.\\nText Generation Quality. IR-QLoRA demonstrates substantial improvements in the coherence and fluency of generated\\ntext. When tasked with composing narratives for various scenarios, from mundane daily activities to complex extraterrestrial\\ninteractions, as cataloged in Table 14, IR-QLoRA invariably outshines QLoRA. Both IR-QLoRA-7B and IR-QLoRA-13B\\ndemonstrate a remarkable ability to generate narratives with consistent themes and create more comprehensive stories. In\\ncontrast, QLoRA-7B often falters by misinterpreting the subject matter or producing narratives that lack completeness.\\n16\\nAccurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nTable 14: Case study for text generation outputs\\n[1] Input: Alice has 15 apples. She buys 10 more from the market and then gives 7 apples to her friend. How many apples does\\nAlice have now?\\nAnswer: Alice have 18 apples now.\\nModel\\nOutput\\nQLoRA-7B\\n22\\nIR-QLoRA-7B (ours)\\nAlice now has 18 apples.\\nQLoRA-13B\\nAlice had 15 apples. Alice buys 10 more apples. Alice has 15 + 10 = 25 apples. Alice gives 7 apples to\\nher friend. Alice has 25 - 7 = 18 apples.\\nIR-QLoRA-13B (ours)\\nAlice started with 15 apples and then bought 10 apples from the market, so her total apples inventory is\\nnow 25 apples (15 + 10 = 25). Then she gave 7 apples to her friend, so her total apples inventory is now\\n18 apples (25 - 7 = 18). Alice now has 18 apples.\\n[2] Input: If Thomas is Joseph‚Äôs son, what is Thomas to Joseph‚Äôs father?\\nAnswer: Thomas is Joseph‚Äôs father‚Äôs grandson\\nModel\\nOutput\\nQLoRA-7B\\nnephew\\nIR-QLoRA-7B (ours)\\nThomas is grandson of Joseph‚Äôs father.\\nQLoRA-13B\\ngrandson\\nIR-QLoRA-13B (ours)\\ngrandson\\n[3] Input: Imagine you are a historian living in ancient Rome, describe a day in your life.\\nModel\\nOutput\\nQLoRA-7B\\nI wake up in my bed and eat breakfast. I put on my clothes and walk to school. I take my lunch and sit\\nin my desk. We have a history lesson and a math lesson. I walk home and play with my friends.\\nIR-QLoRA-7B (ours)\\nMy day begins with waking up early to tend to my animals. I then head to the forum to listen to the\\ndebates between my fellow Romans. Afterwards, I might head to the library to research some more,\\nbefore I finally return home to my family in the evening.\\nQLoRA-13B\\nYou wake up in your apartment in the center of the city. You roll out of your bed and stretch your arms\\nand legs. You yawn and open your eyes. You sit up and put your feet on the floor. You get up and walk\\nto the bathroom. You wash your face, your hands and your body. You then brush your teeth. You return\\nto your bedroom and get dressed in your clothes. You then put on your shoes and walk out of your\\napartment.\\nIR-QLoRA-13B (ours)\\nAs a historian living in ancient Rome, I would wake up in the morning when my slave would bring me\\ncoffee. Then I would go to the library and read ancient texts to see how Rome developed over time.\\nLater, I would visit a slave market where I would buy my slaves. Then, I went to bed.\\n[4] Input: Tell a story set on an alien planet, featuring an Earth astronaut and a representative of an alien civilization.\\nModel\\nOutput\\nQLoRA-7B\\nThis is an alien planet that has been invaded by an alien civilization.\\nIR-QLoRA-7B (ours)\\nEarth astronaut John Smith landed on the alien planet of Tarsis. The planet Tarsis was inhabited by an\\nalien civilization called the Ranax, who greeted Smith in a traditional Ranaxian ceremony. Smith and\\nhis Ranaxian counterpart, Kxat, bonded over their shared love of exploring space. Together, they set off\\non an adventure to uncover the mysteries of Tarsis, learning more about each other‚Äôs cultures and planet\\nalong the way.\\nQLoRA-13B\\nAs the spacecraft came to a stop on a distant planet, the astronauts got out and began to explore.\\nIR-QLoRA-13B (ours)\\nEarth astronaut Tom Jones landed on Mars and saw a Martian spaceship landing in front of him. Tom\\nJones approached the Martian spaceship and stepped inside. Tom Jones met a Martian representative\\nnamed Martian Smith. Tom Jones asked Martian Smith ‚ÄùAre there any intelligent life forms on Mars?‚Äù\\nMartian Smith replied ‚ÄùYes, there are intelligent life forms on Mars.‚Äù Tom Jones asked Martian Smith\\n‚ÄùHow intelligent are these life forms?‚Äù Martian Smith replied ‚ÄùThese life forms are very intelligent,\\nthey are more intelligent than any life form on Earth.‚Äù\\n17\\n', metadata={'Published': '2024-02-08', 'Title': 'Accurate LoRA-Finetuning Quantization of LLMs via Information Retention', 'Authors': 'Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno', 'Summary': 'The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IRQLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.'}),\n",
              " Document(page_content=\"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large\\nLanguage Model Tuning\\nHossein Rajabzadeh12, Mojtaba Valipour 1, Tianshu Zhu 2, Marzieh Tahaei 2,\\nHyock Ju Kwon 1, Ali Ghodsi 1, Boxing Chen 2, Mehdi Rezagholizadeh 2\\n1 University of Waterloo, 2 Huawei Noah‚Äôs Ark Lab\\n{hossein.rajabzadeh, mojtaba.valipour, hjkwon, ali.ghodsi}@uwaterloo.ca\\n{mehdi.rezagholizadeh, tianshu.zhu, marzieh.tahaei, boxing.chen}@huawei.com\\nAbstract\\nFinetuning large language models requires\\nhuge GPU memory, restricting the choice to\\nacquire Larger models. While the quantized\\nversion of the Low-Rank Adaptation technique,\\nnamed QLoRA, significantly alleviates this is-\\nsue, finding the efficient LoRA rank is still\\nchallenging.\\nMoreover, QLoRA is trained\\non a pre-defined rank and, therefore, cannot\\nbe reconfigured for its lower ranks without\\nrequiring further fine-tuning steps. This pa-\\nper proposes QDyLoRA -Quantized Dynamic\\nLow-Rank Adaptation-, as an efficient quantiza-\\ntion approach for dynamic low-rank adaptation.\\nMotivated by Dynamic LoRA, QDyLoRA is\\nable to efficiently finetune LLMs on a set of\\npre-defined LoRA ranks. QDyLoRA enables\\nfine-tuning Falcon-40b for ranks 1 to 64 on a\\nsingle 32 GB V100-GPU through one round\\nof fine-tuning. Experimental results show that\\nQDyLoRA is competitive to QLoRA and out-\\nperforms when employing its optimal rank.\\n1\\nIntroduction\\nThe popularity of adopting Large Language Models\\n(LLMs) across a diverse range of downstream tasks\\nhas rapidly increased over the past two years. Fine-\\ntuning LLMs has become necessary to enhance\\ntheir performance and introduce desired behaviors\\nwhile preventing undesired outputs (Ding et al.,\\n2023). However, as the size of these models in-\\ncreases, fine-tuning costs become more expensive.\\nThis has led to a large body of research that focuses\\non improving the efficiency of the fine-tuning stage\\n(Liu et al., 2022; Mao et al., 2021; Hu et al., 2021;\\nEdalati et al., 2022; Sung et al., 2022).\\nLow-rank adapter (LoRA) (Hu et al., 2021) is\\na well-known, parameter-efficient tuning (PEFT)\\nmethod that reduces memory requirements during\\nfine-tuning by freezing the base model and updat-\\ning a small set of trainable parameters in form of\\nlow-rank matrix multiplication added to matrices\\nin the base model. However, the memory demand\\nduring fine-tuning remains substantial due to the\\nnecessity of a backward pass through the frozen\\nbase model during stochastic gradient descent.\\nRecent research has thus focused on further re-\\nducing memory usage by designing new parameter-\\nefficient modules that can be tuned without ne-\\ncessitating gradients from the base models (Sung\\net al., 2022). Alternatively, researchers have ex-\\nplored combining other efficiency strategies with\\nparameter-efficient tuning methods (Kwon et al.,\\n2022; Dettmers et al., 2023).\\nAmong these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-\\nlizing LoRA modules is the need to tune their rank\\nas a hyperparameter. Different tasks may require\\nLoRA modules of varying ranks. In fact, it is ev-\\nident from the experimental results in the LoRA\\npaper that the performance of models varies a lot\\nwith different ranks, and there is no clear trend\\nindicating the optimal rank. On the other hand,\\nany hyperparameter tuning for finding the optimal\\nrank contradicts the primary objective of efficient\\ntuning and is not feasible for very large models.\\nMoreover, when deploying a neural network on\\ndiverse devices with varying configurations, the\\nuse of higher ranks can become problematic for\\nhighly sensitive devices due to the increased pa-\\nrameter count. To address this, one typically has to\\nchoose between training multiple models tailored\\nto different device configurations or determining\\nthe optimal rank for each device and task. How-\\never, this process is costly and time-consuming,\\neven when using techniques like LoRA.\\narXiv:2402.10462v1  [cs.LG]  16 Feb 2024\\nTable 1: A comparison between QLoRA and QDyLoRA on the MMLU benchmark, reporting 5-shot test results for\\nLLMs of varying sizes. QDyLoRA is evaluated on ranks [1,2,4,8,16,32,64] and the best rank is reported in brackets.\\nDataset\\nLLaMA-7b\\nLLaMA-13b\\nFalcon-40b\\nQLoRA\\nQDyLoRA\\nQLoRA\\nQDyLoRA\\nQLoRA\\nQDyLoRA\\nAlpaca\\n38.8 [64]\\n39.7 [16]\\n47.8 [64]\\n47.6 [8]\\n55.2 [64]\\n57.1 [4]\\nOASST1\\n36.6 [64]\\n36.8 [16]\\n46.4 [64]\\n47.2 [8]\\n56.3 [64]\\n56.7 [4]\\nSelf-Instruct\\n36.4 [64]\\n37.2 [8]\\n33.3 [64]\\n41.6 [4]\\n51.8 [64]\\n51.1 [4]\\nFLAN-v2\\n44.5 [64]\\n45.9 [4]\\n51.4 [64]\\n52.1 [8]\\n58.3 [64]\\n60.2 [4]\\nDyLoRA (Valipour et al., 2022), is a recent\\nPEFT method that aims to address theses chal-\\nlenges by employing dynamic Low-Rank Adapter\\n(DyLoRA). Inspired by nested dropout, this\\nmethod aims to order the representations of the\\nbottleneck at low-rank adapter modules. Instead of\\ntraining LoRA blocks with a fixed rank, DyLoRA\\nextends training to encompass a spectrum of ranks\\nin a sorted manner. The resulting low-rank PEFT\\nmodules not only provide increased flexibility dur-\\ning inference, allowing for the selection of different\\nranks depending on the context, but also demon-\\nstrate superior performance compared to LoRA, all\\nwithout imposing any additional training time.\\nIn this paper, we employ the DyLoRA PEFT\\nmethod in conjunction with the quantization\\nscheme utilized in the QLoRA work, resulting in\\nQDyLoRA. QDyLoRA has all the aforementioned\\nbenefits of DyLoRA but with significant memory\\nreduction both during training and at inference\\nthrough 4-bit quantization. We utilize QDyLoRA\\nfor efficient fine-tuning of LLaMA-7b, LLaMA-\\n13b, and Falcon-40b models across ranks ranging\\nfrom 1 to 64, all on a single 32GB V100 GPU.\\nOnce tuned, we determine the optimal rank by in-\\nferring the model on the test set. Our results reveal\\nthat the optimal rank can be quite low, yet it outper-\\nforms QLoRA.\\n1.1\\nRelated Work\\nLow-rank PEFT methods\\nThese methods aim\\nto fine-tune pre-trained LLMs for specific tasks\\nwhile minimizing computational and memory re-\\nsources. Low-rank adaptation techniques were in-\\nspired by (Aghajanyan et al., 2020), demonstrating\\nthat pre-trained language models possess a low\\nintrinsic dimension.\\nSince then, several works\\nhave explored the incorporation of trainable param-\\neters in the form of low-rank up-projection/down-\\nprojection during fine-tuning. In (Houlsby et al.,\\n2019), the Adapter module includes a down projec-\\ntion, a non-linear function, an up projection, and\\na residual connection. These modules are sequen-\\ntially inserted after the feed-forward network (FFN)\\nor attention blocks.\\nAdditionally, (He et al., 2021) extends the\\nAdapter concept by introducing trainable mod-\\nules that run in parallel (PA) with the original\\npre-trained language-model (PLM) module. As\\na result of this extension, PA has demonstrated\\nimproved performance compared to the original\\nAdapter method. One notable approach among\\nthese techniques is LoRA (Hu et al., 2021), which\\nintroduces low-rank up-projection/down-projection\\ninto various matrices within a PLM. This method\\noffers efficient inference by seamlessly integrating\\nthe adapter module into the original model‚Äôs weight\\nmatrices.\\nQuantization-aware PEFT methods\\nAlpha-\\nTuning (Kwon et al., 2022), aims to combine\\nparameter-efficient adaptation and model compres-\\nsion. Alpha-Tuning achieves this by employing\\npost-training quantization, which involves convert-\\ning the pre-trained language model‚Äôs full-precision\\nparameters into binary parameters and separate\\nscaling factors. During adaptation, the binary val-\\nues remain fixed for all tasks, while the scaling\\nfactors are fine-tuned for the specific downstream\\ntask.\\nQLoRA (Dettmers et al., 2023) is a more recent\\nquantization-aware PEFT that combines a low-rank\\nadapter with 4-bit NormalFloat (NF4) quantization\\nand Double Quantization (DQ) of the base model\\nto optimize memory usage. NF4 ensures an op-\\ntimal distribution of values in quantization bins,\\nsimplifying the process when input tensors have\\na fixed distribution. DQ further reduces memory\\noverhead by quantizing quantization constants.\\nTo manage memory during gradient checkpoint-\\ning, QLoRA employs Paged Optimizers, utiliz-\\ning NVIDIA‚Äôs unified memory feature for effi-\\nTable 2: Comparing the performance of QLoRA and QDyLoRA across different evaluation ranks. Both models\\nreceives the same training settings. Maximum LoRA rank is set to 64. Falcon-40b is adopted as the base LLM.\\nExact matching and Bleu-score are used as evaluation measurements for GSM8k and Web-GLM, respectively.\\nData\\nMethod\\nRank\\n1\\n2\\n4\\n8\\n16\\n32\\n64\\nWeb-GLM\\nQLoRA\\n19.9\\n19.9\\n19.9\\n33.8\\n35.2\\n52.7\\n54.3\\nQDyLoRA\\n43.3\\n56.0\\n54.9\\n53.3\\n53.3\\n50.5\\n50.2\\nGSM8k\\nQLoRA\\n8.9\\n8.91\\n8.9\\n15.1\\n20.5\\n22.6\\n28.1\\nQDyLoRA\\n21.4\\n25.3\\n28.2\\n30.6\\n29.8\\n28.5\\n27.4\\ncient GPU memory management. These techniques\\ncollectively enable high-fidelity 4-bit fine-tuning\\nwhile effectively handling memory constraints.\\nDynamic\\nPEFT\\nmethods\\nDyLoRA\\npaper\\n(Valipour et al., 2022) introduces a novel approach\\nfor training low-rank modules to work effectively\\nacross a range of ranks simultaneously, eliminating\\nthe need to train separate models for each rank.\\nInspired by the concept of nested dropout, the\\nauthors propose a method for organizing the repre-\\nsentations within low-rank adapter modules. This\\napproach aims to create dynamic low-rank adapters\\nthat can adapt well to various ranks, rather than\\nbeing fixed to a single rank with a set training bud-\\nget. This is achieved by dynamically selecting\\nranks during training, allowing for greater flexibil-\\nity without the need for extensive rank searching\\nand multiple model training sessions.\\nAlgorithm 1 QDyLoRA - Training and Inference\\nRequire: r ‚àà [rmin,rmax]; i: the number of training iterations; Œ±: a scaling\\nfactor; pB: probability distribution function for rank selection; X ‚àà Rd√ón\\n: all input features to LoRA; W0 ‚àà Rm√ód the original frozen pre-trained\\nweight matrix, Wdw ‚àà Rr√ód; Wup ‚àà Rm√ór; Q: Quantizer; LDY\\n‚Üìb :\\nobjective function given truncated weights\\nInitialization:\\nW NF 4\\n0\\n= Q(W0) // Quantize W0 to NF4\\nIterations:\\nwhile t < i do\\nForward:\\nb ‚àº pB(.) // sample a specific rank, during test is given\\nWdw‚Üìb = Wdw[:b,:] // truncate down-projection matrix\\nWup‚Üìb = Wup[:,:b] // truncate up-projection matrix\\nW DDequant‚àíNF 4\\n0\\n=\\nW NF 4\\n0\\ncF P 8\\n2\\n/cF P 32\\n1\\n// dequantized the chunks of\\nthe parameters that are needed\\nh = W DDequant‚àíNF 4\\n0\\nXBF 16 + Œ±\\nb W BF 16\\nup‚Üìb W BF 16\\ndw‚Üìb XBF 16 //\\ncalculate the LoRA output\\nBackward:\\nW BF 16\\ndw‚Üìb ‚Üê W BF 16\\ndw‚Üìb ‚àí Œ∑‚àáW BF 16\\ndw‚Üìb LDY\\n‚Üìb\\nW BF 16\\nup‚Üìb\\n‚Üê Wup‚Üìb ‚àí Œ∑‚àáW BF 16\\nup‚Üìb LDY\\n‚Üìb\\nend while\\n2\\nProposed Method: Quantized DyLoRA\\nFollowing DyLoRA notations (Valipour et al.,\\n2022), we define a truncated weight W‚Üìb ‚àà Rr√ód\\nas W[: b, :]. Assume we have a set of input fea-\\ntures X ‚àà Rd√ón, a set of pre-trained weights W0,\\nand a given range of desired ranks represented by\\nr ‚àà [rmin,rmax] that we want the model to operate\\nwith, and a dynamic objective function LDY\\n‚Üìb\\nthat\\ncan evaluate a truncated sub-model. Then we can\\nuse the following equation to calculate the forward\\npass of the model at each iteration.\\nh = W DDequant‚àíNF4\\n0\\nxBF16\\n+ Œ±\\nb W BF16\\nup‚Üìb W BF16\\ndw‚Üìb xBF16\\n(1)\\nwhere Œ± is the LoRA scalar, and b is the chosen\\nrank by the pB(.) during training stage.\\nFollowing QLoRA (Dettmers et al., 2023), we\\nused 4-bit Normal Float (NF4) for storing the dou-\\nble quantized pre-trained weights. As all the com-\\nputations need to be calculated in BFloat16 pre-\\ncision, DDequant-NF4 will dequantize the stored\\ndata. Similar to (Dettmers et al., 2023), we have:\\nW DDequant‚àíNF4\\n0\\n=\\nW NF4\\n0\\ncFP8\\n2\\n/cFP32\\n1\\n(2)\\nwhere cFP32\\n1\\nand cFP8\\n2\\nare quantization constants\\nintroduced in (Dettmers et al., 2023). After this pro-\\ncess, we can update the dynamic LoRA parameters\\nusing:\\nW BF16\\ndw‚Üìb ‚Üê W BF16\\ndw‚Üìb ‚àí Œ∑‚àáW BF 16\\ndw‚Üìb LDY\\n‚Üìb\\nW BF16\\nup‚Üìb\\n‚Üê Wup‚Üìb ‚àí Œ∑‚àáW BF 16\\nup‚Üìb LDY\\n‚Üìb\\n(3)\\nAlgorithm 1 describes the workflow of our pro-\\nposed QDyLoRA in detail.\\n3\\nExperiments and Evaluation\\nThis section evaluates the efficiency and efficacy\\nof QDyLoRA through several instruct-fine-tuning\\nTable 3: Comparing the performance of DyLoRA, QLoRA and QDyLoRA across different evaluation ranks. all\\nmodels receives the same training settings. Maximum LoRA rank is set to 64. The results are reported in terms of\\nexact matching.\\nData;LLM\\nMethod\\nRank\\n1\\n2\\n4\\n8\\n16\\n32\\n64\\nGSM8K;LLaMA-7b\\nDyLoRA\\n12.96\\n16.91\\n17.06\\n19.94\\n18.50\\n18.35\\n14.94\\nQLoRA\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n12.66\\nQDyLoRA\\n12.59\\n15.09\\n18.50\\n16.76\\n16.91\\n18.65\\n14.71\\nTriviaQA;LLaMA-7b\\nDyLoRA\\n19.27\\n23.20\\n22.99\\n23.32\\n23.25\\n24.12\\n22.43\\nQLoRA\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n15.52\\nQDyLoRA\\n6.66\\n12.49\\n17.16\\n19.51\\n20.09\\n21.65\\n20.27\\nGSM8K;LLaMA2-13b\\nDyLoRA\\nOOM\\nOOM\\nOOM\\nOOM\\nOOM\\nOOM\\nOOM\\nQLoRA\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n21.08\\nQDyLoRA\\n1.90\\n15.01\\n22.97\\n25.55\\n24.26\\n23.81\\n22.08\\ntasks. The first experiment compares QDyLoRA\\nwith QLoRA on Massively Multitask Language Un-\\nderstating (MMLU) benchmark (Hendrycks et al.,\\n2020), consisting of more than 50 different tasks,\\nspanning from fundamental mathematics and U.S.\\nhistory to computer science and law. As shown\\nin Table 11, we finetune LLaMA-7b, LLaMA-13b,\\nLLaMA2-13b, and Falcon40b on different datasets,\\nAlpaca (Taori et al., 2023), OASST1 (K√∂pf et al.,\\n2023), Self-Instruct (Wang et al., 2022), and FLAN-\\nv2 (Chung et al., 2022), using QLoRA and QDy-\\nLoRA techniques. We use the same training bud-\\nget and maximum LoRA rank 2 for each tech-\\nnique. The results consistently show that QDy-\\nLoRA achieves a superior performance by finding\\nthe optimal rank.\\nThe second experiment provides a more in-depth\\ncomparison between QLoRA and QDyLoRA. In\\nparticular, we fairly finetuned Falcon-40b on We-\\nbGLM (Liu et al., 2023) and GSM8k (Cobbe et al.,\\n2021) benchmarks, and compared their test per-\\nformances across different ranks. As described in\\nTable 2, QDyLoRA attains superior performance,\\nnotably when employing its optimal ranks (Rank\\n2 for Web-GLM and Rank 8 for GSM8k). Further-\\nmore, QDyLoRA exhibits consistent superiority\\nover QLoRA, particularly at lower ranks. These\\nfindings emphasize the adaptive nature of QDy-\\nLoRA in dynamically adjusting its focus during\\nfine-tuning, leading to enhanced efficiency and ef-\\nficacy compared to its static counterpart, QLoRA.\\nThe third experiment compares the performance\\nof DyLoRA, QDyLoRA, and QLoRA on GSM8k\\n1The same settings as the original QLoRA work are applied\\nhere.\\n2The maximum LoRA rank is fixed to 64. While QLoRA‚Äôs\\nrank is always fixed, QDyLoRA can split the training across\\nranks in range 1 to 64.\\nand TriviaQA (Joshi et al., 2017) while adopting\\nLLaMA2-13b and LLaMA-7b as LLMs. Table\\n3 reports the results. As the table illustrates, for\\nsmaller-size models, i.e. LLaMA-7b, DyLoRA and\\nQDyLoRA both perform superior than QLoRA.\\nFor larger models, i.e. LLaMA2-13b, DyLoRA\\nfails due to the out-of-memory (OOM) error while\\nQDyLoRA works the best in such situations.\\n4\\nOn the semi-sorted behavior of\\nQDyLoRA\\nAs shown in Table 2, QDyLoRA reveals a semi-\\nsorted performance across ranks. We justify this\\nbehavior by pointing out the limited finetuning bud-\\nget. In a limited budget assumption, QDyLoRA\\nupdates its lower ranks more frequently than its\\nhigher ranks. That is because of the fact that lower\\nranks are also updated when higher ranks are se-\\nlected. In other words, lower ranks have more\\nchance to get updated than higher ranks. Hence,\\nlower ranks are more tuned than higher ranks.\\n5\\nConclusion\\nQDyLoRA offers an efficient and effective tech-\\nnique for LoRA-based fine-tuning LLMs on down-\\nstream tasks. Eliminating the need for fine-tuning\\nmultiple models to find the optimal LoRA rank and\\noffering the possibility of fine-tuning larger LLMs\\nare two main advantages of QDyLoRA. The exper-\\nimental results demonstrated that the optimal rank\\nfor QDyLoRA can be surprisingly low, yet it con-\\nsistently outperforms QLoRA. QDyLoRA provides\\ngreater flexibility for deploying LLMs in various\\ncontexts and represents a promising step towards\\nmaking fine-tuning large language models more\\naccessible and efficient.\\nLimitations\\nWhile the 4-bit QDyLoRA exhibits notable per-\\nformance, it falls short of achieving the perfor-\\nmance levels of full precision fine-tuning. One\\npossible solution could be dynamic quantized Dy-\\nLoRA (DyQDyLoRA), in which the quantization\\nlevel could also vary during finetuning. In particu-\\nlar, the finetuning strategy can dynamically switch\\nbetween different quantization levels based on a\\npredefined learning feedback. Additionally, further\\nresearch is required to investigate the impact of\\nLoRA's scalar and the range of underlying ranks in\\nQDyLoRA.\\nReferences\\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal\\nGupta. 2020. Intrinsic dimensionality explains the\\neffectiveness of language model fine-tuning. arXiv\\npreprint arXiv:2012.13255.\\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\\n2022. Scaling instruction-finetuned language models.\\narXiv preprint arXiv:2210.11416.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\\nNakano, et al. 2021. Training verifiers to solve math\\nword problems. arXiv preprint arXiv:2110.14168.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\\nof quantized llms. arXiv preprint arXiv:2305.14314.\\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei,\\nZonghan Yang, Yusheng Su, Shengding Hu, Yulin\\nChen, Chi-Min Chan, Weize Chen, et al. 2023.\\nParameter-efficient fine-tuning of large-scale pre-\\ntrained language models. Nature Machine Intelli-\\ngence, 5(3):220‚Äì235.\\nAli Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Par-\\ntovi Nia, James J Clark, and Mehdi Rezagholizadeh.\\n2022. Krona: Parameter efficient tuning with kro-\\nnecker adapter. arXiv preprint arXiv:2212.10650.\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\\nKirkpatrick, and Graham Neubig. 2021. Towards a\\nunified view of parameter-efficient transfer learning.\\narXiv preprint arXiv:2110.04366.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2020. Measuring massive multitask language under-\\nstanding. arXiv preprint arXiv:2009.03300.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\\nBruna Morrone, Quentin De Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\\nParameter-efficient transfer learning for nlp. In In-\\nternational Conference on Machine Learning, pages\\n2790‚Äì2799. PMLR.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. 2021.\\nLora: Low-rank adap-\\ntation of large language models.\\narXiv preprint\\narXiv:2106.09685.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. arXiv preprint arXiv:1705.03551.\\nAndreas K√∂pf, Yannic Kilcher, Dimitri von R√ºtte,\\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stan-\\nley, Rich√°rd Nagyfi, et al. 2023.\\nOpenassistant\\nconversations‚Äìdemocratizing large language model\\nalignment. arXiv preprint arXiv:2304.07327.\\nSe Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min\\nYoo, Jin-Hwa Kim, Baeseong Park, Byeongwook\\nKim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee.\\n2022. Alphatuning: Quantization-aware parameter-\\nefficient adaptation of large-scale pre-trained lan-\\nguage models. arXiv preprint arXiv:2210.03858.\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\\nhta, Tenghao Huang, Mohit Bansal, and Colin A Raf-\\nfel. 2022. Few-shot parameter-efficient fine-tuning\\nis better and cheaper than in-context learning. Ad-\\nvances in Neural Information Processing Systems,\\n35:1950‚Äì1965.\\nXiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng,\\nZhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie\\nTang. 2023. Webglm: Towards an efficient web-\\nenhanced question answering system with human\\npreferences. arXiv preprint arXiv:2306.07906.\\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\\nhairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian\\nKhabsa. 2021. Unipelt: A unified framework for\\nparameter-efficient language model tuning. arXiv\\npreprint arXiv:2110.07577.\\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\\nLst: Ladder side-tuning for parameter and memory\\nefficient transfer learning. Advances in Neural Infor-\\nmation Processing Systems, 35:12991‚Äì13005.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model.\\nMojtaba\\nValipour,\\nMehdi\\nRezagholizadeh,\\nIvan\\nKobyzev, and Ali Ghodsi. 2022. Dylora: Parameter\\nefficient tuning of pre-trained models using dynamic\\nsearch-free low-rank adaptation.\\narXiv preprint\\narXiv:2210.07558.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\\nguage model with self generated instructions. arXiv\\npreprint arXiv:2212.10560.\\nA\\nSupplementary Material\\nA.1\\nHyperparameters\\nTable 4 provides an overview of the hyperparam-\\neters and experimental configurations employed\\nin this study, which are crucial configurations that\\ndetermine various aspects of the training process\\nand model behavior in this study. Common key pa-\\nrameters across the experiments include the choice\\nof optimizer, Adam-Beta2 value, maximum gra-\\ndient norm, and warmup ratio, which collectively\\ninfluence how the model adjusts its weights during\\ntraining. LoRA-specific parameters such as LoRA\\ndropout probability, maximum LoRA rank, and\\nalpha value control the behavior of LoRA layers.\\nAdditionally, double quantization and quantization\\ntype impact the precision of numerical representa-\\ntions within the model, which are considered the\\nsame as baselines. Learning rate scheduling and\\nweight decay contribute to the optimization process,\\nhelping to prevent overfitting and stabilize training.\\nRandom seeds ensure reproducibility, while the\\nspecified GPU determines the hardware used for\\ntraining. Each model configuration, whether for\\nthe Web-GLM, GSM8k, or the specific experiment\\noutlined in Table 1, features parameters tailored to\\nthe characteristics of the dataset and the computa-\\ntional resources available. These hyperparameters\\ncollectively shape the training process, ultimately\\ninfluencing the performance and effectiveness of\\nthe models in the study.\\nA.2\\nGenerated Text Quality\\nTo describe the quality of texts generated by QDy-\\nLoRA, a sample query taken from GSM8k is fed\\nto Falcon-40b trained by QDyLoRA. Below, we\\nreport the generated answers for different LoRA\\nranks.\\nQuery: Janet‚Äôs ducks lay 16 eggs per day. She\\neats three for breakfast every morning and bakes\\nmuffins for her friends every day with four. She\\nsells the remainder at the farmers‚Äô market daily for\\n$2 per fresh duck egg. How much in dollars does\\nshe make every day at the farmers‚Äô market?\\nRank=64: Janet has 16 - 3 - 4 = ¬´16-3-4=7¬ª7\\neggs left. She sells 7 - 16 = ¬´7-16=9¬ª9 eggs at the\\nModel\\nParameter\\nValue\\nCommon settings\\nOptimizer\\npaged-adamw-32bit\\nAdam-Beta2\\n0.999\\nMax-Grad-Norm\\n0.3\\nWarmup Ratio\\n0.03\\nLoRA-Dropout\\n0.05\\nLoRA-r\\n64\\nLoRA-Alpha\\n16\\nLoRA-Module\\nall\\nDouble-Quant\\nTrue\\nQuant-Type\\nnf4\\nLR Scheduler\\nConstant\\nLearning Rate (LR)\\n1e-5\\nWeight Decay\\n0.0\\nSeeds\\n[42]\\nGPU\\nTesla V100-PCIE-32GB\\nWeb-GLM\\nMax-Steps\\n2700\\nGradient-Accumulation-Step\\n2\\nSource-Max-Len\\n2048\\nTarget-Max-Len\\n2048\\nBatch Size\\n16\\nGSM8k\\nMax-Steps\\n300\\nGradient-Accumulation-Step\\n2\\nSource-Max-Len\\n2048\\nTarget-Max-Len\\n2048\\nBatch Size\\n16\\nTable 1\\nMax-Steps\\n5000\\nGradient-Accumulation-Step\\n16\\nSource-Max-Len\\n16\\nTarget-Max-Len\\n512\\nBatch Size\\n32\\nTable 4: The list of hyperparameters employed across\\nvarious experiments in our study. The common set-\\ntings are applied to each experiments. If a parameter is\\nnot explicitly mentioned, it means that we use default\\nvalues from HuggingFace Transformers v‚Äô4.28.0.dev0‚Äô\\n(source: https://huggingface.co/docs/transformers).\\nfarmers‚Äô market.She makes 9 * 2 =¬´9*2=18¬ª18 at\\nthe farmers‚Äô market every day.\\nRank=32: She eats 3 eggs for breakfast and\\nbakes 4 muffins per day, so she uses 3 + 4 =\\n¬´3+4=7¬ª7 eggs per day. She has 16 - 7 = ¬´16-\\n7=9¬ª9 eggs left over each day. She sells these eggs\\nat the farmers‚Äô market for $2 per egg, so she makes\\n2 * 9 = ¬´2*9=18¬ª18 per day at the market.\\nRank=28: She eats 3 eggs for breakfast and\\nbakes 4 muffins per day, so she uses 3+4 =\\n¬´3+4=7¬ª7 eggs per day. She sells 16-7 = ¬´16-7=9¬ª9\\neggs per day for $2 per egg. She makes 9*2 =\\n¬´9*2=18¬ª18 per day at the market.\\nRank=16: Janet‚Äôs ducks lay 16 eggs per day\\nand she eats 3 for breakfast every morning so she\\nhas 16-3=13 eggs left. Janet bakes 4 muffins every\\nday and she uses 4 eggs so she has 13-4=9 eggs\\nleft. Janet sells the remaining eggs at the farmers‚Äô\\nmarket for $2 each so she makes 9*2=¬´9*2=18¬ª18\\ndollars a day. Janet makes 18 dollars a day at the\\nfarmers‚Äô market.\\n\", metadata={'Published': '2024-02-16', 'Title': 'QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning', 'Authors': 'Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh', 'Summary': 'Finetuning large language models requires huge GPU memory, restricting the\\nchoice to acquire Larger models. While the quantized version of the Low-Rank\\nAdaptation technique, named QLoRA, significantly alleviates this issue, finding\\nthe efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a\\npre-defined rank and, therefore, cannot be reconfigured for its lower ranks\\nwithout requiring further fine-tuning steps. This paper proposes QDyLoRA\\n-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach\\nfor dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to\\nefficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables\\nfine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one\\nround of fine-tuning. Experimental results show that QDyLoRA is competitive to\\nQLoRA and outperforms when employing its optimal rank.'}),\n",
              " Document(page_content='Speaker Attribution in German Parliamentary Debates with\\nQLoRA-adapted Large Language Models\\nTobias Bornheim1, Niklas Grieger1,2,3, Patrick Gustav Blaneck1 and Stephan Bialonski1,3,*\\n1Department of Medical Engineering and Technomathematics\\nFH Aachen University of Applied Sciences, J¬®ulich, Germany\\n2Department of Information and Computing Sciences\\nUtrecht University, Utrecht, The Netherlands\\n3Institute for Data-Driven Technologies\\nFH Aachen University of Applied Sciences, J¬®ulich, Germany\\n*bialonski@fh-aachen.de\\nAbstract\\nThe growing body of political texts opens up\\nnew opportunities for rich insights into po-\\nlitical dynamics and ideologies but also in-\\ncreases the workload for manual analysis. Au-\\ntomated speaker attribution, which detects who\\nsaid what to whom in a speech event and is\\nclosely related to semantic role labeling, is an\\nimportant processing step for computational\\ntext analysis. We study the potential of the\\nlarge language model family Llama 2 to au-\\ntomate speaker attribution in German parlia-\\nmentary debates from 2017‚Äì2021. We fine-\\ntune Llama 2 with QLoRA, an efficient training\\nstrategy, and observe our approach to achieve\\ncompetitive performance in the GermEval 2023\\nShared Task On Speaker Attribution in German\\nNews Articles and Parliamentary Debates. Our\\nresults shed light on the capabilities of large\\nlanguage models in automating speaker attribu-\\ntion, revealing a promising avenue for compu-\\ntational analysis of political discourse and the\\ndevelopment of semantic role labeling systems.\\n1\\nIntroduction\\nLanguage is central to the study of politics, as\\nit forms the basis for political speech and de-\\nbates (Grimmer and Stewart, 2013). These textual\\nsources offer rich insights into political dynamics\\nand ideologies, yet the analysis of even moderately\\nsized collections has been impeded by prohibitive\\ncosts. Recent innovations from natural language\\nprocessing (NLP) have the potential to significantly\\nreduce the financial burden of scrutinizing exten-\\nsive text corpora (GlavaÀás et al., 2019; Abercrom-\\nbie and Batista-Navarro, 2020). This development\\ncoincides with the availability of a growing body\\nof political texts, including German Parliamentary\\ndata (Barbaresi, 2018; Bl¬®atte and Blessing, 2018;\\nWalter et al., 2021; Rauh and Schwalbach, 2020;\\nAbrami et al., 2022; Rehbein et al., 2023), thus\\nopening new avenues for political research.\\nPolitical texts are usually unstructured, present-\\ning challenges for automated analyses. An ap-\\nproach towards this challenge is automated speaker\\nattribution (Rehbein et al., 2023), which detects\\nwho said what to whom in a speech event. This\\nprocess involves detecting cue words that initiate a\\nspeech event and discerning the different roles (e.g.,\\nsource, message, and addressee) associated with\\neach event. This task is closely related to semantic\\nrole labeling (SRL) that delineates the specific se-\\nmantic relationships among a predicate and its cor-\\nresponding arguments, such as ‚Äúwho‚Äù did ‚Äúwhat‚Äù\\nto ‚Äúwhom‚Äù, ‚Äúwhere‚Äù, ‚Äúwhen‚Äù, and ‚Äúwhy‚Äù (Gildea\\nand Jurafsky, 2002; M`arquez et al., 2008). Seman-\\ntic role labeling is considered a key component\\nfor natural language understanding and has been\\ndemonstrated to enhance systems for various ap-\\nplications including question answering, machine\\ntranslation, and video understanding (Navigli et al.,\\n2022).\\nEarly approaches to SRL relied on syntactic fea-\\ntures (Navigli et al., 2022; Larionov et al., 2019).\\nMore recently, the field has seen a significant tran-\\nsition from such engineered features to features\\nlearned in an end-to-end fashion by models that\\noperate on raw-level input or tokens (Collobert\\net al., 2011). However, such end-to-end models\\nnecessitate large annotated training sets, available\\nfor English but scarce for low-resource languages.\\nThis problem can be mitigated by pretraining on\\nunannotated data. Indeed, the emergence of pre-\\ntrained large language models (LLMs) inspired by\\nThis work was published in J. Lang. Technol. Comput. Linguistics, 2024, available online at\\nhttps://jlcl.org/article/view/244. Please cite as: Bornheim, T., Grieger, N., Blaneck, P. G., & Bialonski, S. (2024).\\nSpeaker Attribution in German Parliamentary debates with QLoRA-adapted Large Language Models. Journal for Language\\nTechnology and Computational Linguistics, 37, 1-13.\\narXiv:2309.09902v2  [cs.CL]  1 Mar 2024\\nthe transformer architecture (Vaswani et al., 2017)\\nled to new state-of-the-art results across various\\nNLP tasks. Among these, encoder-only models\\nlike BERT were demonstrated to improve existing\\nSRL benchmarks (Shi and Lin, 2019). More re-\\ncently, the advent of decoder-only models, such as\\nGPT (Radford and Narasimhan, 2018) and larger\\nmodels like GPT-4 (OpenAI, 2023), Claude 2 (Bai\\net al., 2022), and Llama 2 (Touvron et al., 2023b),\\nhas further propelled the field. These models, with\\ntheir ability to comprehend and execute instruc-\\ntions in natural language for a wide array of tasks,\\nhold potential for SRL and automated speaker attri-\\nbution that is, to the best of our knowledge, largely\\nunexplored.\\nIn this contribution, we study the potential of\\nLlama 2 70B, a model from a recently introduced\\nfamily of large language models, to automati-\\ncally detect speech events and attribute speakers\\nin German parliamentary debates. We instruct and\\nfine-tune Llama 2 to extract cues and roles using\\nQLoRA (Dettmers et al., 2023), a parameter- and\\ncomputationally efficient training strategy. Our ap-\\nproach achieves competitive performance (quanti-\\nfied by F1 scores for cues and roles) on the SpkAtt-\\n2023 dataset of the GermEval 2023 Shared Task on\\nSpeaker Attribution in German News Articles and\\nParliamentary Debates (Rehbein et al., 2023). The\\nimplementation details of our experiments (Team\\n‚ÄúCPAa‚Äù) are available online1.\\n2\\nData and tasks\\nThe dataset of the GermEval 2023 Shared Task on\\nSpeaker Attribution in German News Articles and\\nParliamentary Debates consisted of 267 speeches\\nfrom the German Bundestag (Rehbein et al., 2023).\\nThis dataset included speeches from all seven par-\\nliamentary groups (including independent mem-\\nbers of parliament as a separate group) of the 19th\\nlegislative period of the German Bundestag (see\\nTable 1 for details). To facilitate analysis, each\\nspeech was automatically separated into sentence-\\nlike structures using spaCy, hereafter referred to as\\nsamples (units of analysis). Each sample was then\\nfurther split into elements, i.e., words and punctua-\\ntion marks.\\nHuman annotators followed annotation guide-\\n1https://github.com/dslaborg/\\ngermeval2023\\nParliamentary group\\nSpeeches\\nSamples\\nCDU/CSU\\n77\\n4305\\nSPD\\n57\\n2887\\nAfD\\n39\\n1827\\nFDP\\n34\\n1435\\nDIE LINKE\\n29\\n1356\\nB‚Äô90 / DIE GR ¬®UNEN\\n27\\n1152\\nindependent\\n4\\n125\\nTotal\\n267\\n13087\\nTable 1: Number of speeches and samples per parlia-\\nmentary group in the combined Train, Dev, and Eval\\ndatasets.\\nlines 2 to assign none, one, or multiple annota-\\ntions to each sample. These annotations consisted\\nof cue words that invoke speech events and roles\\n(Addr, Evidence, Medium, Message, Source, Topic,\\nPTC) associated with that event. While the cue is\\nmandatory for each annotation, roles are context-\\ndependent and may be absent. Figure 1 shows\\nexample annotations.\\nThe Shared Task consisted of two subtasks: Full\\nAnnotation (Subtask 1) and Role Detection (Sub-\\ntask 2) (Rehbein et al., 2023). In the Full Annota-\\ntion subtask, the goal was to predict all cues and\\nroles for each sample. In the Role Detection sub-\\ntask, the gold cues were given, and the goal was to\\npredict only the roles for each sample.\\nThe dataset was provided as five sets, namely\\nTrial, Train, Dev, and two Eval sets (see Table 2).\\nWe omitted the Trial set in our experiments, since\\nit was included in the Train set. For training and\\ntuning the final models, we used the Train and Dev\\nsets. The two Eval sets were used by the GermEval\\n2023 organizers to compute the final scores for\\nSubtask 1 (Eval set 1) and Subtask 2 (Eval set\\n2). While the two Eval sets contained the same\\nsamples, the organizers provided gold cues with\\nEval set 2.\\n3\\nMethods\\n3.1\\nModels\\nWe used the Llama 2 model family (Touvron et al.,\\n2023b), a set of large language models pretrained\\non a corpus of two trillion tokens with a context\\n2https://github.com/umanlp/\\nSpkAtt-2023/blob/master/doc/Guidelines_\\nSpeakerAttribution_in_Parliamentary_\\nDebates-SpkAtt-2023_Task1.pdf\\nAnnotation 1\\nVon der AfD wollen wir hier lieber nicht reden; ‚Ä°\\ndenn wir(Source) wissen(Cue): Neben ihren rassistischen\\nPositionen ‚Ä° haben die Rechtsradikalen nicht nur\\nKlimawandelleugnung im Angebot, sie haben auch die\\nrechtspopulistischen Positionen eines Donald Trump\\ngepachtet(Message).\\nAnnotation 2\\nVon der AfD wollen wir hier lieber nicht reden; ‚Ä°\\ndenn wir wissen:\\nNeben ihren rassistischen Posi-\\ntionen(Cue) ‚Ä° haben die Rechtsradikalen nicht nur\\nKlimawandelleugnung im Angebot, sie haben auch die\\nrechtspopulistischen Positionen eines Donald Trump\\ngepachtet.\\nAnnotation 3\\nVon der AfD wollen wir hier lieber nicht reden; ‚Ä°\\ndenn wir(Source) wissen: Neben ihren rassistischen Po-\\nsitionen ‚Ä° haben die Rechtsradikalen nicht nur Kli-\\nmawandelleugnung im Angebot, sie haben auch die\\nrechtspopulistischen Positionen(Cue) eines Donald Trump\\ngepachtet(Message).\\nFigure 1: Sentence from the Train dataset with three\\nannotations. The sentence was split into three samples\\nby spaCy (splitting points are indicated by ‚Ä°). This seg-\\nmentation also occurs at not-punctuated positions, as\\nseen in the example sentence (‚Äú. . . rassistischen Positio-\\nnen ‚Ä° haben die Rechtsradikalen . . . ‚Äù). This behavior is\\ndue to the data provided by ‚ÄúOpen Bundestag‚Äù, where\\ncomments from other members of parliament during\\nan otherwise coherent paragraph force this unintuitive\\nsegmentation into two separate paragraphs (Rehbein\\net al., 2023). As seen in Annotation 2, there can be\\nannotations consisting of only cue word(s). Annotation\\n1 and Annotation 3 show that annotated roles can span\\nmultiple samples.\\nlength of 4096 tokens. The Llama 2 model family\\nincludes both pretrained models and fine-tuned ver-\\nsions optimized for conversational tasks. Since our\\napproach did not require the conversational capabil-\\nities of the fine-tuned models, we chose to use the\\nbase pretrained versions of Llama 2 in our experi-\\nments. These base models were trained without a\\nspecific prompt format and are therefore not biased\\ntoward any particular prompt strategy, allowing us\\nto freely choose our own prompt format.\\nWhile the Llama 2 model family contains mod-\\nels of various sizes, we chose to fine-tune the\\nlargest available model with 70 billion parame-\\nters (Llama 2 70B). The weights of this model\\ncan be obtained upon request using the official\\nGitHub repository3. Once downloaded, we fol-\\n3https://github.com/facebookresearch/\\nllama\\nSplit\\nSpeeches\\nSamples\\nAnnotations\\nDev\\n18\\n927\\n515\\nTrain\\n177\\n9093\\n5399\\nEval\\n72\\n3067\\n1792\\nTotal\\n267\\n13087\\n7706\\nTable 2: Number of speeches, samples (units of analy-\\nsis), and annotations for each dataset. The Trial dataset\\nis completely contained within the Train dataset and is\\ntherefore not shown. The Eval dataset here refers to\\nthe test sets of both Subtask 1 and Subtask 2, since they\\nonly differ in the provided annotations.\\nlowed the provided instructions4 to convert the\\nmodel to the HuggingFace Transformers format\\n(Wolf et al., 2020). This conversion allowed us to\\nload the model using the HuggingFace Transform-\\ners library, which facilitated the fine-tuning and\\ninference steps.\\n3.2\\nPreprocessing\\nFor effective training (see section 3.3) and infer-\\nence (see section 3.4) we preprocessed each sample.\\nWe parsed each annotation into its respective lists\\nof elements. Next, we joined all elements of a sam-\\nple with space characters in between to get each\\nsample‚Äôs text. Since roles can be contained in sam-\\nples different from the one containing the cue, we\\nconcatenated the sample with the next two samples\\nof the same speech, if possible.\\nDuring our experiments, we noticed that our\\nmodels ignored their instructions and generated\\nrandom text if the text of a given sample ended\\nwith a colon. To counteract this behavior, we re-\\nplaced this trailing colon with a period.\\nWe designed prompts for cue prompting (see\\nFigure 2) and role prompting (see Figure 3). We\\nwrote the instructions in our prompt templates in\\nEnglish, because it was observed that the perfor-\\nmance of multilingual models such as Llama 2 is\\nimproved when English prompts are used (Fu et al.,\\n2022; Huang et al., 2023). Also, since a sample\\nmay not contain a cue, or a role may be missing,\\nwe used ‚Äú#UNK#‚Äù to mark such cases.\\n3.3\\nTraining\\nFor our final submission, we fine-tuned two\\nLlama 2 70B models to identify cues and roles,\\nrespectively, using QLoRA (Quantized Low-Rank\\n4https://github.com/facebookresearch/\\nllama-recipes\\nInput:\\nUser: A cue is the lexical items in a sentence that\\nindicate that speech, writing, or thought is being\\nreproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them\\nseparated by commas.\\nIf no cue can be found in the given text, you output the\\nstring #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix ‚ÄúCues: ‚Äù.\\nSentence: denn wir wissen: Neben ihren rassistischen\\nPositionen\\nAssistant:\\nOutput:\\nCues: [wissen], [Positionen]</s>\\nFigure 2: Example cue prompt and desired model re-\\nsponse for the sample ‚Äúdenn wir wissen: Neben ihren\\nrassistischen Positionen‚Äù with the cues ‚Äúwissen‚Äù and\\n‚ÄúPositionen‚Äù. Shaded in gray are the parts of the prompt\\nand response that are sample dependent. The prompt is\\nused as the Input sequence for training and inference,\\nwhile the Output sequence contains the desired response\\nwith the cues. The end-of-sentence token ‚Äú</s>‚Äù is\\nused to indicate the end of the Output sequence.\\nAdaptation) (Dettmers et al., 2023). QLoRA is a\\nhighly efficient fine-tuning technique for large lan-\\nguage models that achieves similar performance to\\nfull fine-tuning while using only a fraction of the\\nmemory. This memory reduction is achieved by\\nquantizing the model weights of an LLM to four\\nbits and adding Low Rank Adapters (LoRA layers)\\nto all linear transformer blocks of the model. Dur-\\ning fine-tuning, only these LoRA layers are trained\\nand the rest of the pretrained model weights remain\\nunaltered. By employing this strategy, QLoRA\\nachieves a significant reduction in memory usage\\nduring fine-tuning, while still allowing the model\\nto adapt to downstream tasks through the trainable\\nLoRA layers.\\nAs described in Section 3.2, we parsed the train-\\ning samples into cue prompts (see Figure 2) that\\nserved as input to the cue model and role prompts\\n(see Figure 3) that served as input to the role model.\\nUtilizing these input prompts, the respective mod-\\nels were trained to predict the desired assistant\\nresponses (defined as Output in Figures 2 and 3).\\nThis approach is consistent with previous research\\nthat has shown improved performance when fine-\\ntuning only on the target response of an instruction\\nset, rather than both the instructions and the desired\\nresponse (Dettmers et al., 2023). By treating the\\ninput and output separately, we can process the\\nInput:\\nUser: Now I give you again the sentence only in\\naddition with the two following sentences, because\\nthe roles can be partially contained in the following\\nsentences.\\nText: denn wir wissen : Neben ihren rassistischen\\nPositionen ‚Ä° haben die Rechtsradikalen nicht nur\\nKlimawandelleugnung im Angebot , sie haben auch die\\nrechtspopulistischen Positionen eines Donald Trump\\ngepachtet . ‚Ä° Als Linke ¬®ubernehmen wir Verantwortung\\n.\\nNow find all roles in the sentence associated with the\\ncue ‚Äòwissen‚Äô you found in the beginning sentence.\\nAssistant:\\nOutput:\\ncue: wissen\\nptc: #UNK#\\nevidence: #UNK#\\nmedium: #UNK#\\ntopic: #UNK#\\naddr: #UNK#\\nmessage: Neben, ihren, rassistischen, Positionen, haben,\\ndie, Rechtsradikalen, nicht, nur, Klimawandelleugnung,\\nim, Angebot, ,, sie, haben, auch, die, rechtspopulistis-\\nchen, Positionen, eines, Donald, Trump, gepachtet\\nsource: wir</s>\\nFigure 3: Example role prompt and desired model re-\\nsponse for the sample ‚Äúdenn wir wissen: Neben ihren\\nrassistischen Positionen‚Äù with the cue ‚Äúwissen‚Äù. Since\\nroles can be contained in samples different from the\\none containing the cue, we concatenated the sample\\nwith the next two samples of the same speech (transi-\\ntions between samples are indicated by ‚Ä°). Shaded in\\ngray are the parts of the prompt and response that are\\nsample dependent. Similar to the cue prompt, the role\\nprompt is used as the Input sequence for training and in-\\nference, while the Output sequence contains the desired\\nresponse. We append the end-of-sentence token ‚Äú</s>‚Äù\\nto the Output.\\ntwo sequences with different maximum sequence\\nlengths. Specifically, for the model used to identify\\ncues, we set the maximum length of the input to\\n256 tokens (with seven samples of the training data\\ntruncated) and the maximum length of the output\\nto 64 tokens (no samples truncated). For the model\\nused to identify roles, we truncated the input to\\n640 tokens (with six samples of the training data\\ntruncated) and the output to 256 tokens (with one\\nsample truncated).\\nExcept for the maximum number of tokens in the\\ninput and output sequences, we largely followed\\nthe training strategy proposed in Dettmers, Pagnoni,\\nHoltzman, and Zettlemoyer (2023). Although their\\nspecific experiments did not involve a Llama 2\\n70B model, they successfully fine-tuned a similarly\\nsized LLaMA model (predecessor to Llama 2) with\\n65 billion parameters (Touvron et al., 2023a). We\\nadopted most parameters from this 65B model fine-\\ntuning, such as a constant learning rate of Œ∑ =\\n0.0001 with linear warmup over the first 3% of\\ntraining steps and a dropout of 0.05 for the LoRA\\nlayers. The main hyperparameter we adjusted was\\nthe number of training steps to prevent overfitting.\\nFor the cues model, we trained for 2000 steps with\\na batch size of 16 and no gradient accumulation.\\nFor the roles model, we used 2500 steps with a\\nbatch size of eight and gradient accumulation over\\ntwo steps, i.e., an effective batch size of 16.\\nFine-tuning was carried out on a DGX A100\\nserver, with a total training time of about seven\\nhours for the cues model and 17 hours for the roles\\nmodel. To optimize memory usage, we experi-\\nmented with reducing the batch size to one while\\nincreasing the gradient accumulation steps to 16\\n(i.e., maintaining the same effective batch size).\\nWith these parameters, both models were able to\\noperate within a GPU memory limit of less than 60\\nGB.\\n3.4\\nInference\\nPrompting our fine-tuned models was a two-step\\nprocess. In the first step, we prompted our cue\\nmodel for all cues in a sample using our prompt\\ntemplate for cues (see Figure 2). We postprocessed\\nthe output of the model (see section 3.5) into a\\nlist of cues. In the second step, for each cue, we\\nprompted for the roles with our role model. To do\\nthis, we prepended the complete cue prompt and its\\noutput to the role prompt template before querying\\nthe model (see Figure 3).\\nTo ensure reproducibility of results, we config-\\nured our models to generate output deterministi-\\ncally. For a given input sequence, large language\\nmodels obtain a probability distribution over all\\npossible tokens. We chose to always select the to-\\nken with the highest assigned probability as the\\nnext output token, thereby fixing the output for a\\ngiven input sequence.\\n3.5\\nPostprocessing and evaluation metrics\\nSeveral postprocessing steps were necessary to\\nevaluate the models‚Äô output in a structured way.\\nEnforcing the output format.\\nIf the models‚Äô out-\\nput did not follow our strict output format (see Fig-\\nures 2 and 3), we mapped the output to the marker\\n#UNK# (unknown).\\nPreventing overlapping cues.\\nIf our cue model\\ndetected multiple but overlapping cues, we com-\\nbined them into a single cue.\\nIgnoring made-up words.\\nIf the output of the\\nmodel contained words for cues or roles that were\\nnot in the given sample, and no other word with a\\nLevenshtein distance of 1 was found in the sample,\\nwe ignored those words. Then, if the output was\\nempty, we mapped the output to the marker #UNK#\\n(unknown).\\nResolving ambiguities.\\nA word may occur more\\nthan once in a sample. When a model outputs such\\na word as a cue or a role, it is unclear to which\\noccurrence of the word in the sample it should\\nbe attributed. To resolve this ambiguity, for each\\noccurrence of the word, we counted how many\\nelements around that word (in the range of two\\nelements to the left and right) were part of the cue\\nor role, and chose the occurrence with the highest\\ncount.\\nIncluding surrounded punctuation.\\nRoles of-\\nten contained punctuation marks such as colons\\nor commas. We observed that our models ignored\\nthese punctuation marks most of the time. If a punc-\\ntuation mark was surrounded by words that were\\nselected for this role, we added that punctuation\\nmark to the role as well.\\nEvaluating metrics.\\nTo evaluate the perfor-\\nmance of our models, we used the proportional F1\\nscore as proposed for opinion role labeling (Johans-\\nson and Moschitti, 2010). This score is defined\\nas the harmonic mean of the proportional preci-\\nsion and recall. Proportional precision quantifies\\nthe proportion of overlap between a predicted cue\\n(role) and an overlapping true cue (role). Propor-\\ntional recall quantifies the proportion of overlap\\nbetween a true cue (role) and an overlapping pre-\\ndicted cue (role; see Rehbein, Petersen-Frey, Brun-\\nner, Ruppenhofer, Biemann, and Ponzetto (2023)\\nfor further details on how the proportional F1 score\\nis calculated).\\n4\\nResults\\nWe used the same fine-tuned Llama 2 70B models\\nfor both Subtask 1 and Subtask 2 of GermEval\\n2023 Shared Task 1 ‚Äì a cues model to identify cues\\nin a given sentence and a roles model to predict\\nthe roles associated with the identified cues. While\\nthe cues model was used exclusively in Subtask 1,\\nPrecision\\nRecall\\nF1\\nSubtask 1\\nCues\\n0.889\\n0.889\\n0.889\\nRoles\\n0.787\\n0.822\\n0.804\\nCues & Roles\\n0.798\\n0.829\\n0.813\\nSubtask 2\\nRoles\\n0.910\\n0.873\\n0.891\\nTable 3: Proportional precision, recall, and F1 scores ob-\\ntained for predicting cues and roles on the Eval dataset.\\nThe joint scores for predicting both cues and roles (Sub-\\ntask 1 of GermEval 2023 Shared Task 1) are shown in\\nthe third row. The last row shows the results obtained\\nfor predicting roles on the Eval dataset when the true\\ncues were given (Subtask 2).\\nas the cues were provided in Subtask 2, the roles\\nmodel was used in both subtasks. It leveraged\\neither the predicted cues from Subtask 1 or the gold\\ncues from Subtask 2 to predict the roles associated\\nwith each cue, as described in section 3.4. By using\\nthe same fine-tuned roles model for both subtasks,\\nwe were able to analyze the impact of using gold\\ncues versus predicted cues on role identification\\nperformance.\\nTable 3 shows the final results of our submissions\\non the Eval dataset, as reported by the organizers\\nof the GermEval 2023 Shared Task. For Subtask 1,\\nthe fine-tuned cues model achieved an F1 score\\nof 0.889 for predicting cues. Using the predicted\\ncues from this model, the fine-tuned roles model\\nachieved an F1 score of 0.804 for predicting roles.\\nCombining both predictions, our models achieved\\nan overall F1 score of 0.813 for predicting cues\\nand roles in Subtask 1. In Subtask 2, where gold\\ncues were provided, the same roles model used in\\nSubtask 1 achieved a higher F1 score of 0.891 for\\npredicting roles. Interestingly, the improvement of\\nthe roles model using gold cues was greater in pre-\\ncision, which increased from 0.787 to 0.910, than\\nin recall, which increased from 0.822 to 0.873. This\\nincrease in precision suggests that the cues model\\nin Subtask 1 overpredicted sentences as containing\\ncues when they actually had no cues, resulting in\\ntoo many false positive role predictions.\\nIn summary, our results demonstrate that our\\nfine-tuned models are effective at reliably predict-\\ning cues and roles. Additionally, the results high-\\nlight the importance of accurate cue prediction, as\\nerrors of the cues model propagate to the roles\\nmodel, reducing its performance.\\n5\\nConclusion\\nWe demonstrated that fine-tuned Llama 2 language\\nmodels can successfully predict cues and roles in\\nGerman parliamentary debates, achieving compet-\\nitive performance on the GermEval2023 Shared\\nTask without relying on traditional linguistic fea-\\ntures. These results highlight the feasibility of auto-\\nmated speaker attribution by fine-tuning models on\\nprompt templates that task them with identifying\\ncues and roles. The similarity between automated\\nspeaker attribution and semantic role labeling sug-\\ngests that this strategy may pave the way for new\\nstate-of-the-art results in various semantic role la-\\nbeling tasks.\\nLimitations\\nWe did not study risks that may or may not arise\\nwhen our fine-tuned large language models are used\\nfor other application scenarios than ours. In our\\napproach, users can neither manipulate the prompts\\nnor read the generated texts produced by our mod-\\nels. Instead, the generated outputs are processed\\nand mapped back to the words from the parliamen-\\ntary speeches used as input. Therefore, we consider\\nthe risks associated with our approach to be lim-\\nited. We recommend security testing if our trained\\nmodels are to be used in other scenarios.\\nAcknowledgements\\nWe are grateful to M. Rei√üel and V. Sander for\\nproviding us with computing resources.\\nReferences\\nGavin Abercrombie and Riza Batista-Navarro. 2020.\\nSentiment and position-taking analysis of parliamen-\\ntary debates: A systematic literature review. Journal\\nof Computational Social Science, 3(1):245‚Äì270.\\nGiuseppe Abrami, Mevl¬®ut Bagci, Leon Hammerla, and\\nAlexander Mehler. 2022. German parliamentary cor-\\npus (GerParCor). In Proceedings of the Thirteenth\\nLanguage Resources and Evaluation Conference,\\nLREC 2022, Marseille, France, 20-25 June 2022,\\npages 1900‚Äì1906. European Language Resources\\nAssociation.\\nYuntao Bai,\\nSaurav Kadavath,\\nSandipan Kundu,\\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\\npher Olah, Danny Hernandez, Dawn Drain, Deep\\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\\nLandau, Kamal Ndousse, Kamile Lukosiute, Liane\\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\\nSchiefer, Noem¬¥ƒ± Mercado, Nova DasSarma, Robert\\nLasenby, Robin Larson, Sam Ringer, Scott John-\\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\\nNicholas Joseph, Sam McCandlish, Tom Brown, and\\nJared Kaplan. 2022. Constitutional AI: Harmlessness\\nfrom AI feedback. CoRR, abs/2212.08073.\\nAdrien Barbaresi. 2018. A corpus of German political\\nspeeches from the 21st century. In Proceedings of\\nthe Eleventh International Conference on Language\\nResources and Evaluation (LREC 2018), Miyazaki,\\nJapan. European Language Resources Association\\n(ELRA).\\nAndreas Bl¬®atte and Andr¬¥e Blessing. 2018. The Germa-\\nParl corpus of parliamentary protocols. In Proceed-\\nings of the Eleventh International Conference on\\nLanguage Resources and Evaluation, LREC 2018,\\nMiyazaki, Japan, May 7-12, 2018. European Lan-\\nguage Resources Association (ELRA).\\nRonan Collobert, Jason Weston, L¬¥eon Bottou, Michael\\nKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.\\n2011. Natural language processing (almost) from\\nscratch. J. Mach. Learn. Res., 12:2493‚Äì2537.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\\nLuke Zettlemoyer. 2023. QLoRA: Efficient finetun-\\ning of quantized LLMs. CoRR, abs/2305.14314.\\nJinlan Fu, See-Kiong Ng, and Pengfei Liu. 2022. Poly-\\nglot Prompt: Multilingual multitask prompt training.\\nIn Proceedings of the 2022 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n9919‚Äì9935, Abu Dhabi, United Arab Emirates. As-\\nsociation for Computational Linguistics.\\nDaniel Gildea and Daniel Jurafsky. 2002. Automatic\\nlabeling of semantic roles.\\nComput. Linguistics,\\n28(3):245‚Äì288.\\nGoran GlavaÀás, Federico Nanni, and Simone Paolo\\nPonzetto. 2019. Computational analysis of political\\ntexts: Bridging research efforts across communities.\\nIn Proceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics: Tutorial\\nAbstracts. Association for Computational Linguis-\\ntics.\\nJustin Grimmer and Brandon M. Stewart. 2013. Text\\nas data: The promise and pitfalls of automatic con-\\ntent analysis methods for political texts. Political\\nAnalysis, 21(3):267‚Äì297.\\nHaoyang Huang, Tianyi Tang, Dongdong Zhang,\\nWayne Xin Zhao, Ting Song, Yan Xia, and Furu\\nWei. 2023. Not all languages are created equal in\\nLLMs: Improving multilingual capability by cross-\\nlingual-thought prompting. CoRR, abs/2305.07004.\\nRichard Johansson and Alessandro Moschitti. 2010.\\nSyntactic and semantic structure for opinion expres-\\nsion detection. In Proceedings of the Fourteenth Con-\\nference on Computational Natural Language Learn-\\ning, pages 67‚Äì76, Uppsala, Sweden. Association for\\nComputational Linguistics.\\nDaniil Larionov, Artem Shelmanov, Elena Chistova, and\\nIvan V. Smirnov. 2019. Semantic role labeling with\\npretrained language models for known and unknown\\npredicates. In Proceedings of the International Con-\\nference on Recent Advances in Natural Language\\nProcessing, RANLP 2019, Varna, Bulgaria, Septem-\\nber 2-4, 2019, pages 619‚Äì628. INCOMA Ltd.\\nLlu¬¥ƒ±s M`arquez, Xavier Carreras, Kenneth C. Litkowski,\\nand Suzanne Stevenson. 2008. Semantic role label-\\ning: An introduction to the special issue. Comput.\\nLinguistics, 34(2):145‚Äì159.\\nRoberto Navigli, Edoardo Barba, Simone Conia, and\\nRexhina Blloshmi. 2022. A tour of explicit multi-\\nlingual semantics: Word sense disambiguation, se-\\nmantic role labeling and semantic parsing. In Pro-\\nceedings of the 2nd Conference of the Asia-Pacific\\nChapter of the Association for Computational Lin-\\nguistics and the 12th International Joint Conference\\non Natural Language Processing: Tutorial Abstracts,\\npages 35‚Äì43, Taipei. Association for Computational\\nLinguistics.\\nOpenAI. 2023.\\nGPT-4 technical report.\\nCoRR,\\nabs/2303.08774.\\nAlec Radford and Karthik Narasimhan. 2018.\\nIm-\\nproving language understanding by generative pre-\\ntraining.\\nChristian Rauh and Jan Schwalbach. 2020. The Parl-\\nSpeech V2 data set: Full-text corpora of 6.3 million\\nparliamentary speeches in the key legislative cham-\\nbers of nine representative democracies.\\nInes Rehbein, Fynn Petersen-Frey, Annelen Brun-\\nner, Josef Ruppenhofer, Chris Biemann, and Si-\\nmone Paolo Ponzetto. 2023. Overview of the Germ-\\nEval 2023 Shared Task on Speaker Attribution in\\nNewswire and Parliamentary Debates. In The Germ-\\nEval 2023 Shared Task at KONVENS 2023, Ingol-\\nstadt, Germany.\\nPeng Shi and Jimmy Lin. 2019. Simple BERT models\\nfor relation extraction and semantic role labeling.\\nCoRR, abs/1904.05255.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth¬¥ee Lacroix,\\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aur¬¥elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023a.\\nLLaMA:\\nOpen and efficient foundation language models.\\nCoRR, abs/2302.13971.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aur¬¥elien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023b. Llama 2: Open foundation and\\nfine-tuned chat models. CoRR, abs/2307.09288.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\\nKaiser, and Illia Polosukhin. 2017.\\nAttention is\\nall you need. In Annual Conf. Neural Information\\nProcessing Systems 2017, pages 5998‚Äì6008, Long\\nBeach, CA, USA.\\nTobias Walter, Celina Kirschner, Steffen Eger, Goran\\nGlavas, Anne Lauscher, and Simone Paolo Ponzetto.\\n2021. Diachronic analysis of German parliamentary\\nproceedings: Ideological shifts through the lens of\\npolitical biases. In ACM/IEEE Joint Conference on\\nDigital Libraries, JCDL 2021, Champaign, IL, USA,\\nSeptember 27-30, 2021, pages 51‚Äì60. IEEE.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, R¬¥emi Louf, Morgan Funtowicz,\\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\\nScao, Sylvain Gugger, Mariama Drame, Quentin\\nLhoest, and Alexander M. Rush. 2020. Transformers:\\nState-of-the-art natural language processing. In Proc.\\n2020 Conf. on Empirical Methods in Natural Lan-\\nguage Processing: System Demonstrations, pages\\n38‚Äì45, Online. Association for Computational Lin-\\nguistics.\\n', metadata={'Published': '2024-03-01', 'Title': 'Speaker attribution in German parliamentary debates with QLoRA-adapted large language models', 'Authors': 'Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck, Stephan Bialonski', 'Summary': 'The growing body of political texts opens up new opportunities for rich\\ninsights into political dynamics and ideologies but also increases the workload\\nfor manual analysis. Automated speaker attribution, which detects who said what\\nto whom in a speech event and is closely related to semantic role labeling, is\\nan important processing step for computational text analysis. We study the\\npotential of the large language model family Llama 2 to automate speaker\\nattribution in German parliamentary debates from 2017-2021. We fine-tune Llama\\n2 with QLoRA, an efficient training strategy, and observe our approach to\\nachieve competitive performance in the GermEval 2023 Shared Task On Speaker\\nAttribution in German News Articles and Parliamentary Debates. Our results shed\\nlight on the capabilities of large language models in automating speaker\\nattribution, revealing a promising avenue for computational analysis of\\npolitical discourse and the development of semantic role labeling systems.'}),\n",
              " Document(page_content='VIZ: A QLORA-BASED COPYRIGHT MARKETPLACE FOR\\nLEGALLY COMPLIANT GENERATIVE AI\\nA PREPRINT\\nDipankar Sarkar\\nTerraprompt AI\\nme@dipankar.name\\nJanuary 2, 2024\\nABSTRACT\\nThis paper aims to introduce and analyze the Viz system in a comprehensive way, a novel system\\narchitecture that integrates Quantized Low-Rank Adapters (QLoRA) to fine-tune large language\\nmodels (LLM) within a legally compliant and resource efficient marketplace. Viz represents a\\nsignificant contribution to the field of artificial intelligence, particularly in addressing the challenges\\nof computational efficiency, legal compliance, and economic sustainability in the utilization and\\nmonetization of LLMs. The paper delineates the scholarly discourse and developments that have\\ninformed the creation of Viz, focusing primarily on the advancements in LLM models, copyright\\nissues in AI training The New York Times Company [2023], and the evolution of model fine-tuning\\ntechniques, particularly low-rank adapters and quantized low-rank adapters, to create a sustainable\\nand economically compliant framework for LLM utilization.The economic model it proposes benefits\\ncontent creators, AI developers, and end-users, delineating a harmonious integration of technology,\\neconomy, and law, offering a comprehensive solution to the complex challenges of today‚Äôs AI\\nlandscape.\\nKeywords Copyright ¬∑ LLM ¬∑ QLoRa ¬∑ Economics ¬∑ Marketplace\\n1\\nIntroduction\\nThe realm of artificial intelligence (AI), particularly in the field of large language models (LLMs), has seen a substantial\\nevolution in recent years, driven by innovations in model architectures, training methodologies, and application scopes.\\nHowever, this rapid advancement brings to the fore significant challenges, notably in the domains of computational\\nefficiency, economic viability, and legal and ethical concerns, particularly relating to copyright issues [The New York\\nTimes Company, 2023] in the pre-training of these models. This paper introduces Viz, a novel system architecture\\ndesigned to address these challenges by leveraging recent advances in model fine-tuning techniques, specifically QLoRA\\n(Quantized Low Rank Adapters), to create a sustainable and legally compliant framework for LLM utilization.\\nLanguage models, such as the GPT (Generative Pre-trained Transformer) series developed by OpenAI, have demon-\\nstrated remarkable capabilities in natural language understanding and generation, underpinning various applications\\nfrom chatbots to content creation [Radford et al., 2019][Brown et al., 2020]. Despite their impressive performance,\\nthese models often require extensive computational resources for training and fine-tuning, which poses significant\\neconomic and environmental concerns [Strubell et al., 2019]. Furthermore, the pre-training of such models typically\\ninvolves large datasets sourced from the Internet, raising critical copyright issues as witnessed in recent legal challenges\\n[Gaon, 2021].\\nThe introduction of LoRA (Low-Rank Adaptation) presented a paradigm shift in the fine-tuning of large models,\\noffering a more resource-efficient approach [Hu et al., 2021]. Building upon this, QLoRA emerges as a groundbreaking\\ntechnique, allowing for the fine-tuning of models with tens of billions of parameters on comparatively modest hardware\\nwhile maintaining performance levels. The QLoRA methodology, as elucidated in its seminal paper, introduces\\nseveral innovative techniques such as 4-bit NormalFloat quantization and Double Quantization, which significantly\\narXiv:2401.00503v1  [cs.LG]  31 Dec 2023\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\nreduce memory usage without sacrificing task performance [Dettmers et al., 2023]. The Guanaco family of models,\\nfine-tuned using QLoRA, demonstrates exemplary performance on the Vicuna benchmark, achieving near parity with\\nstate-of-the-art models like ChatGPT with markedly reduced resource requirements.\\nViz leverages these advances to create a collaborative ecosystem involving content providers, AI developers, and\\nend-users. At its core, Viz proposes a model where LLMs are initially pretrained on non-copyrighted datasets. Content\\nproviders can then utilize QLoRA to create fine-tuned models specific to their content, which are subsequently offered\\nin a marketplace. This marketplace functions akin to digital content platforms like Spotify, allowing users to access and\\ncombine multiple fine-tunes for their specific needs, with usage tracked and monetized accordingly.\\nThis paper aims to detail the architecture of Viz, its implementation using QLoRA, and the economic model it proposes,\\nalong with a thorough discussion of the legal, ethical, and practical implications of such a system. Through Viz, we\\nenvision an economically viable, legally compliant, and resource-efficient paradigm for the utilization and monetization\\nof large-language models, heralding a new era in the field of artificial intelligence. Here are the contribution of the\\npapers.\\n‚Ä¢ Innovatively integrates Quantized Low-Rank Adapters (QLoRA) within a marketplace framework, revolution-\\nizing the accessibility and efficiency of large language models (LLMs).\\n‚Ä¢ Provides a comprehensive design of the Viz system that addresses computational efficiency, copyright compli-\\nance, and economic sustainability in AI.\\n‚Ä¢ Proposes a sustainable economic model for AI technology, ensuring legal and ethical compliance, crucial for\\nresponsible AI deployment.\\n‚Ä¢ Contributes to the discussion on legal and ethical considerations in AI, particularly in copyright compliance\\nand data privacy.\\n2\\nLiterature Review\\nThe foundation of the Viz system is based on the growing field of large-language models (LLMs) and their methods of\\nfinetuning, as well as the legal and ethical dilemmas associated with them. This review of existing literature aims to\\noutline the academic discussions and progress that have influenced the development of Viz. The primary focus is on the\\nadvancements in LLMs, copyright concerns in AI training, and the evolution of fine-tuning techniques, specifically\\nLow-Rank Adapters (LoRA) and Quantized Low-Rank Adapters (QLoRA).\\nGenesis and Evolution of LLMs\\nBeginning with the introduction of the Transformer architecture [Vaswani et al.,\\n2023], the field has seen a rapid expansion, most notably with models like BERT [Devlin et al., 2019] and GPT series\\n[Radford et al., 2019][Brown et al., 2020]. These models have revolutionized natural language processing (NLP),\\noffering unprecedented capabilities in language understanding and generation.\\nThe versatility of LLMs in various applications ranging from text generation to conversational AI has been extensively\\ndocumented [Wolf et al., 2020]. Their impact extends beyond technical domains, influencing fields such as law,\\neducation, and creative industries.\\nData Sourcing and Copyright Challenges\\nThe reliance of LLMs on vast datasets, often scraped from the web, raises\\nsignificant copyright concerns [Gaon, 2021]. The legal discourse around data usage for AI training is a burgeoning\\nfield, with scholars debating the balance between innovation and copyright protection [Cohen, 2019].\\nThe necessity for adhering to data usage practices in line with regulations is underscored by recent legal actions and\\nexamination in the field of AI. The Oracle v. Google (2021) lawsuit sheds light on the intricate legal considerations\\nassociated with utilizing copyrighted content for software development [Wikipedia contributors, 2023].\\nAdvent of LoRA\\nThe introduction of LoRA by [Hu et al., 2021] marked a significant shift in fine-tuning practices.\\nBy applying low-rank matrices to modify pre-trained weights, LoRA offered a more efficient alternative to full model\\nretraining, allowing for resource-efficient customization of LLMs.\\nQLoRA, an extension of LoRA, is a notable improvement in the efficiency of fine-tuning. The foundational paper\\ndescribes its techniques, including 4-bit NormalFloat quantization and Double Quantization, which allow for the fine-\\ntuning of larger models (such as the 65B parameter models) on constrained hardware while maintaining performance\\n[Dettmers et al., 2023].\\n2\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\nEconomic Models in AI and Content Monetization\\nThe monetization of AI technologies, especially in the realm of\\nLLMs, presents unique challenges and opportunities. The work of [Agrawal et al., 2019] on the economics of artificial\\nintelligence provides a comprehensive overview of these dynamics.\\nThe economic model proposed by Viz parallels existing digital content platforms. Studies on platforms like Spotify\\n[Aguiar and Waldfogel, 2018] offer insights into user-based monetization models, which are analogous to Viz‚Äôs approach\\nto monetizing fine-tuned LLMs.\\nThis review highlights the dynamic nature of LLMs, which are characterized by rapid innovation and intricate challenges.\\nThe Viz system, positioned at the convergence of these advancements, seeks to leverage the capabilities of LLMs while\\ntackling the computational, economic, and legal obstacles that are commonly encountered in this domain.\\n3\\nViz System Architecture\\nThe design of the Viz system is highly complex in order to promote a collaborative interaction among content providers,\\ngenerative AI developers, and end-users. This is achieved by utilizing the latest advancements in fine-tuning large\\nlanguage models (LLMs) through Quantized Low Rank Adapters (QLoRA).\\nIn this section, we will provide an overview of the key elements 1, technical requirements, and general process of the\\nViz system, elucidating how it addresses the computational, financial, and legal challenges in the existing AI landscape.\\nFigure 1: Viz system components\\n3.1\\nFundamentals\\nViz is conceptualized as a platform that integrates a marketplace for AI models fine-tuned through QLoRA, providing a\\nlegally compliant and economically viable avenue for content creators and users to interact with LLMs.\\nThe system aims to reduce computational overhead, ensure copyright compliance in training datasets, and create a\\nsustainable economic model for all stakeholders.\\nPre-training on Non-copyrighted Corpus\\nThe initial stage involves training LLMs on a vast corpus of data that is\\nfree from copyright restrictions. This approach aligns with the legal frameworks discussed by Gaon [2021], further\\n3\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\naddressing the challenges highlighted in Oracle v. Google (2021) Wikipedia contributors [2023] and NYT case (2023)\\nThe New York Times Company [2023].\\nQLoRA for Content-specific Fine-tuning\\nBuilding on the efficiency of QLoRA, as expounded by Hu et al. [2021],\\ncontent providers can fine-tune models specific to their content. This process utilizes the innovations in QLoRA, such\\nas 4-bit NormalFloat quantization, to manage large model parameters effectively.\\nMarketplace for Finetune Modules\\nA digital platform akin to Aguiar and Waldfogel [2018] study on Spotify,\\nwhere fine-tuned models are available for purchase or rent. This marketplace facilitates the monetization of AI models,\\naligning with the economic models proposed by Agrawal et al. [2019].\\nUser Interface for Module Application and Tracking\\nAn intuitive interface allows end users to apply multiple\\nfine-tunings to their LLM usage, with an integrated tracking system for usage monitoring and billing, reminiscent of\\ndigital content platforms.\\nFigure 2: Consumer workflow\\nHere 2 step-by-step description of the training of base LLMs and their subsequent fine-tuning using QLoRA. We have 3\\nexplanation of how content providers upload and price their fine-tuned models and how users can access and utilize\\nthese models. It also details the mechanism for tracking the usage of fine-tunes and the subsequent billing process.\\nThe design of the Viz system offers a holistic and inventive approach to the existing obstacles in the artificial intelligence\\nfield, specifically in the application of LLMs. Through the incorporation of advanced methods such as QLoRA for\\neffective model refinement and the establishment of a marketplace that advantages all parties involved, Viz establishes a\\nstandard for upcoming advancements in the AI sector and the monetization of content.\\n3.2\\nQLoRA importance in Viz\\nThe incorporation of Quantized Low Rank Adapters (QLoRA) into the Viz system architecture represents a notable\\nprogress in the efficient and successful fine-tuning of large language models (LLMs). This section thoroughly describes\\nthe application of QLoRA in Viz, clarifying the technical complexities and operational benefits that come with this\\nintegration, particularly in terms of resource utilization and performance improvement.\\nQLoRA‚Äôs Core Principles\\nAt the heart of QLoRA, as elaborated in its seminal paper, lies the concept of using\\nlow-rank matrices in conjunction with quantization techniques to fine-tune LLMs. This method significantly reduces\\nthe computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\nFigure 3: Content owner workflow\\nAdapting QLoRA for Viz\\nThe implementation of QLoRA within Viz should be carefully tailored to align with the\\nsystem‚Äôs objective of providing a scalable and legally compliant platform for LLM utilization. This adaptation involves\\noptimizing QLoRA‚Äôs existing capabilities to suit the diverse requirements of content providers and end-users in the Viz\\nmarketplace.\\nIn Viz, QLoRA is employed to create a modular fine-tuning framework. This approach allows content providers to\\nfine-tune base LLMs with their specific datasets, leveraging QLoRA‚Äôs efficiency to handle diverse content types and\\nrequirements.\\nEfficient Resource Utilization\\nBy integrating QLoRA, Viz significantly lowers the barriers to entry for content\\nproviders, enabling them to fine-tune LLMs on comparatively modest hardware setups, thereby democratizing access to\\nadvanced AI capabilities.\\nEnhanced Model Performance\\nThe QLoRA fine-tuned models in Viz, benchmarked on standards such as the Vicuna\\nbenchmark, demonstrate that this integration does not compromise on performance, ensuring that users have access to\\nhigh-quality AI models.\\nThe implementation of QLoRA within the Viz system represents a paradigm shift in the fine-tuning of LLMs, offering a\\nsolution that is both resource-efficient and performance-oriented. This integration not only enhances the capabilities of\\nViz as a platform for AI model monetization but also contributes significantly to the broader field of AI and machine\\nlearning by demonstrating a practical and scalable approach to model customization.\\n4\\nMarketplace Design and Economics\\nThe Viz system integrates an innovative marketplace for distributing and earning money from finely-tuned large\\nlanguage models (LLMs). This marketplace utilizes the advancements made through Quantized Low Rank Adapters\\n(QLoRA). In this section, we explore the architectural design of the marketplace, its economic foundations, and the\\nproposed revenue models. We draw comparisons to existing digital platforms and economic theories in the field of\\ndigital goods and AI.\\n5\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\n4.1\\nStructural Design\\nThe digital platform in Viz is organized as a marketplace, where content providers can upload and offer their QLoRA\\nfine-tuned LLMs for sale or lease. This structure is inspired by the model seen in digital content platforms such as\\nSpotify, as discussed by Aguiar and Waldfogel [2018].\\nModels in the marketplace are categorized based on various criteria, such as application domain, language, and\\nperformance metrics, facilitating easy access and selection by end-users.\\n4.2\\nEconomic Model for Content Providers and Users\\nThe marketplace employs a dual monetization strategy. Content providers can opt to either sell their models outright or\\noffer them on a subscription basis. This approach aligns with contemporary digital economic models as discussed by\\nAgrawal et al. [2019].\\nPricing in the marketplace is dynamic, allowing content providers to set prices based on factors like model performance,\\nuniqueness, and demand. The Viz system also incorporates algorithms to suggest optimal pricing strategies based on\\nmarket trends.\\n4.3\\nRevenue Sharing Models\\nThe income generated from the sales and subscriptions of models is divided between the content providers and the\\nViz platform, following a pre-established agreement for revenue sharing. This approach mirrors the revenue-sharing\\npractices observed in digital marketplaces, as examined in research conducted by Rochet and Tirole [2003].\\nViz incentivizes content providers through a reward system that recognizes high-performing and popular models,\\nencouraging continuous improvement and innovation in model development.\\n4.4\\nComparison with Existing Models\\nThe economic structure of the Viz marketplace is compared with existing digital platforms, such as Spotify and Netflix,\\nhighlighting similarities and differences in terms of user engagement, pricing, and revenue models.\\nThe marketplace‚Äôs design is analyzed through the lens of economic theories pertinent to digital goods, such as those\\nproposed by Varian [1995], focusing on aspects like pricing, copyright, and distribution in digital economies.\\nThe marketplace within the Viz system represents a novel and economically viable avenue for the distribution and\\nmonetization of AI models. By incorporating advanced AI technologies like QLoRA and integrating sound economic\\nprinciples, the marketplace not only fosters innovation in AI model development but also presents a sustainable\\neconomic model for content providers and users alike.\\n5\\nLegal and Ethical Considerations\\nEnsuring a deep comprehension and strict adherence to legal and ethical standards is of utmost importance in the\\ncreation and functioning of the Viz system, particularly due to the intricate nature of large language models (LLMs) and\\nAI technology. This section highlights the various legal and ethical factors that are essential to the Viz system, with\\nspecific emphasis on copyright adherence, data privacy, AI ethics, and user safeguarding.\\nThe primary objective of the Viz system is to guarantee adherence to global copyright regulations, especially when it\\ncomes to utilizing data for training LLMs. In light of recent legal disputes such as Oracle v. Google (2021) [Wikipedia\\ncontributors, 2023], which highlight the intricate legal aspects of incorporating copyrighted material into software, it\\nis essential to ensure compliance. To ensure this, content providers in the Viz marketplace must verify that the data\\nused for fine-tuning models through QLoRA does not violate any copyright laws, following the guidelines presented by\\nGaon [2021].\\nThe Viz system incorporates strong data security measures to protect user data, in accordance with international data\\nprivacy regulations like the General Data Protection Regulation (GDPR). The system‚Äôs protocols for data privacy are\\ndesigned to prevent unauthorized access and misuse of user data. Viz will maintain transparency regarding data usage,\\nensuring that users are informed about how their data is utilized within the system, as recommended in privacy-focused\\nliterature [Cavoukian, 2009].\\nThe Viz system subscribes to the principles of ethical AI, including fairness, accountability, and transparency. These\\nprinciples are critical in mitigating biases and ensuring equitable AI outcomes, as discussed by [Dignum, 2019]. Viz\\n6\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\nimplements a governance framework for AI models in the marketplace, overseeing their development and deployment\\nto ensure they adhere to ethical standards and do not propagate harmful content or biases.\\nViz is committed to the principles of fair use, which means that it values the intellectual property rights of others\\nwhen using models and their outputs. This is crucial for maintaining a legally compliant and ethically responsible AI\\necosystem. Viz has implemented measures within its system to prevent the inappropriate use of AI technology, such\\nas creating misleading or harmful content. These measures align with the ethical guidelines put forth by Jobin et al.\\n[2019].\\nThe economic models employed in the Viz marketplace are scrutinized for legal implications, ensuring that the\\nmonetization of AI models aligns with copyright and trade laws. The marketplace adheres to ethical monetization\\npractices, promoting fairness and avoiding exploitation, as outlined in the economic and ethical discourse by Lanier\\n[2014].\\nThe legal and ethical framework of the Viz system is a cornerstone of its design and operation, ensuring that it not only\\nadvances technological innovation but also upholds the highest standards of legality, ethics, and user protection. This\\nframework is instrumental in building trust among users, content providers, and stakeholders in the AI community.\\n6\\nDiscussion\\nIn this section, we conduct a thorough examination of the Viz system, with a specific emphasis on its influence on the\\nAI and content industry. We also explore the potential obstacles it may face and discuss its future advancements, such\\nas the possibility of decentralization. The purpose of this discussion is to place the Viz system within the larger context\\nof technological advancements, economic models, and evolving legal frameworks.\\nThe Viz system, with its unique utilization of QLoRA and the marketplace model, is a notable advancement in the AI\\nand content sector. However, for its future development, which may involve decentralization, it is crucial to navigate the\\ntechnical, legal, and ethical aspects with caution. The system‚Äôs progression will undoubtedly contribute to the ongoing\\ndiscussions in the fields of AI, digital economy, and technology governance.\\n6.1\\nImpact on AI and Content Industry\\nBy incorporating QLoRA, Viz effectively reduces the obstacles to developing and customizing AI models, making it\\nmore accessible to a wider group of individuals. This aligns with the notion put forth by Benker [2006] in their research\\non the networked information economy, emphasizing the democratization of access.\\nThe Viz marketplace model introduces a new economic paradigm in the AI industry, aligning with the shifts toward\\ndigital economies and platform-based models ([Parker et al., 2016]. It fosters a competitive environment where\\ninnovation is incentivized, potentially leading to rapid advancements in AI applications.\\n6.2\\nDecentralization\\nImplementing decentralized structures for data management could enhance security and user control over data, resonating\\nwith the principles of Web3.0.\\nA decentralized marketplace could offer greater transparency in transactions, fairer revenue distribution, and enhanced\\ntrust among users and content providers, as per the decentralization ethos articulated by Benker [2006].\\n6.3\\nSocietal and Ethical Implications\\nThe widespread adoption of systems like Viz could significantly impact labor markets, education, and information\\ndissemination, necessitating a societal discourse on the role and regulation of AI.\\nWhile decentralization offers numerous benefits, it also raises ethical concerns, such as the potential for unchecked\\ndissemination of biased or harmful AI models, necessitating robust governance structures.\\n6.4\\nChallenges and Limitations\\nDespite the efficiencies introduced by QLoRA, the computational demands of LLMs remain a challenge, particularly\\nfor smaller entities with limited resources.\\n7\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\nNavigating the legal and ethical landscape, especially concerning copyright compliance and AI ethics, remains a\\ncomplex and evolving challenge, as discussed by Lessig [2006] in the context of internet regulation.\\n6.5\\nFuture Developments and Research Directions\\nContinuous research in AI finetuning, like advancements beyond QLoRA, could further optimize model efficiency and\\neffectiveness.\\nExploring the potential of decentralizing aspects of the Viz system, such as using blockchain technology for transparent\\nand secure transactions in the marketplace, could address issues like data privacy and user trust, as suggested by Tapscott\\nand Tapscott [2016] in their exploration of blockchain revolution.\\n7\\nConclusion\\nThe purpose of this paper is to provide a thorough introduction and analysis of the Viz system. The Viz system is an\\ninnovative architecture that combines Quantized Low Rank Adapters (QLoRA) to fine-tune large language models\\n(LLMs) in a way that is both legally compliant and economically feasible. This system makes a noteworthy contribution\\nto the field of artificial intelligence by addressing the issues of computational efficiency, legal compliance, and economic\\nsustainability when using LLMs. The following are the distinct contributions of this paper:\\n‚Ä¢ Innovative Incorporation of QLoRA The article emphasizes the unique utilization of QLoRA in Viz,\\nillustrating how this method significantly reduces the computational resources needed for fine-tuning LLMs\\nwhile still maintaining high performance levels. This contribution is particularly relevant in the context of\\nmaking advanced AI technologies more accessible to a wider range of users and developers.\\n‚Ä¢ Creation of a Sustainable Economic Model By proposing a marketplace for fine-tuned AI models, the article\\noutlines an economic model that benefits content creators, AI developers, and end-users. This model not only\\nfosters a competitive and innovative environment but also addresses the economic challenges traditionally\\nassociated with AI development and deployment.\\n‚Ä¢ Legal and Ethical Framework The article contributes to the discussion on legal and ethical considerations\\nin AI by explaining how the Viz system adheres to copyright laws, data privacy regulations, and ethical AI\\nprinciples. This aspect of the system is particularly important in an era where the societal and legal impacts of\\nAI are increasingly scrutinized.\\n‚Ä¢ Potential for Decentralization The discussion on incorporating decentralization into the Viz system introduces\\na forward-thinking perspective on how blockchain technology and decentralized governance could enhance\\ntransparency, data security, and user trust in AI marketplaces.\\nAs the field of artificial intelligence (AI) progresses, systems such as Viz are positioned to have a significant impact on\\nthe future of AI development and application. Viz stands out with its unique methods for refining models, designing\\nmarketplaces, and ensuring legal compliance. This system sets a precedent for future advancements in the field, as\\nit combines technological innovation, economic insight, and legal caution. By addressing current challenges and\\npaving the way for future explorations in AI technology, Viz represents a convergence of various factors that shape the\\nlandscape of AI.\\nTo sum up, the Viz system demonstrates a successful merging of technology, economy, and law, providing a holistic\\nanswer to the intricate issues present in the current AI environment. It serves as evidence of the possibilities that arise\\nfrom collaborative and inventive methods in utilizing AI for the advantage of various stakeholders.\\nReferences\\nThe New York Times Company. Complaint - Jury Trial Demanded. United States District Court Southern District\\nof New York, 2023. URL https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf.\\nCivil Action No. Case 1:23-cv-11195.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised\\nmultitask learners. 2019. URL https://api.semanticscholar.org/CorpusID:160025533.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark\\n8\\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nA PREPRINT\\nChen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\\nNLP. In Anna Korhonen, David Traum, and Llu√≠s M√†rquez, editors, Proceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics, pages 3645‚Äì3650, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi:10.18653/v1/P19-1355. URL https://aclanthology.org/P19-1355.\\nAviv Gaon. The future of copyright in the age of artificial intelligence / aviv h. gaon., 2021.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models, 2021.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms,\\n2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need, 2023.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding, 2019.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\\nRault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,\\nJulien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nHuggingface‚Äôs transformers: State-of-the-art natural language processing, 2020.\\nJulie E. Cohen. Between Truth and Power: The Legal Constructions of Informational Capitalism. Oxford University\\nPress, 10 2019. ISBN 9780190246693. doi:10.1093/oso/9780190246693.001.0001. URL https://doi.org/10.\\n1093/oso/9780190246693.001.0001.\\nWikipedia contributors. Google llc v. oracle america, inc. ‚Äî Wikipedia, the free encyclopedia, 2023. URL https://en.\\nwikipedia.org/w/index.php?title=Google_LLC_v._Oracle_America,_Inc.&oldid=1187395454. [On-\\nline; accessed 29-December-2023].\\nAjay Agrawal, Joshua Gans, and Avi Goldfarb. The Economics of Artificial Intelligence: An Agenda. University of\\nChicago Press, 05 2019. ISBN 9780226613338. doi:10.7208/chicago/9780226613475.001.0001. URL https:\\n//doi.org/10.7208/chicago/9780226613475.001.0001.\\nLuis Aguiar and Joel Waldfogel. Platforms, promotion, and product discovery: Evidence from spotify playlists. Working\\nPaper 24713, National Bureau of Economic Research, June 2018. URL http://www.nber.org/papers/w24713.\\nJean-Charles Rochet and Jean Tirole. Platform Competition in Two-Sided Markets. Journal of the European Economic\\nAssociation, 1(4):990‚Äì1029, 06 2003. ISSN 1542-4766. doi:10.1162/154247603322493212. URL https://doi.\\norg/10.1162/154247603322493212.\\nHal R Varian. Pricing information goods, 1995.\\nAnn Cavoukian. Privacy by design. 2009.\\nVirginia Dignum. Responsible artificial intelligence: how to develop and use AI in a responsible way, volume 2156.\\nSpringer, 2019.\\nAnna Jobin, Marcello Ienca, and Effy Vayena. The global landscape of ai ethics guidelines. Nature machine intelligence,\\n1(9):389‚Äì399, 2019.\\nJaron Lanier. Who owns the future? Simon and Schuster, 2014.\\nYochai Benker. The wealth of networks. How social production transforms markets and freedom. New Haven and\\nLondon: Yale University Press. Recuperado de http://www. benkler. org/Benkler_Wealth_Of_Networks. pdf, 2006.\\nGeoffrey G Parker, Marshall W Van Alstyne, and Sangeet Paul Choudary. Platform revolution: How networked markets\\nare transforming the economy and how to make them work for you. WW Norton & Company, 2016.\\nLawrence Lessig. Code: Version 2.0, 2006.\\nDon Tapscott and Alex Tapscott. Blockchain revolution: how the technology behind bitcoin is changing money, business,\\nand the world. Penguin, 2016.\\n9\\n', metadata={'Published': '2023-12-31', 'Title': 'Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI', 'Authors': 'Dipankar Sarkar', 'Summary': \"This paper aims to introduce and analyze the Viz system in a comprehensive\\nway, a novel system architecture that integrates Quantized Low-Rank Adapters\\n(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\\nresource efficient marketplace. Viz represents a significant contribution to\\nthe field of artificial intelligence, particularly in addressing the challenges\\nof computational efficiency, legal compliance, and economic sustainability in\\nthe utilization and monetization of LLMs. The paper delineates the scholarly\\ndiscourse and developments that have informed the creation of Viz, focusing\\nprimarily on the advancements in LLM models, copyright issues in AI training\\n(NYT case, 2023), and the evolution of model fine-tuning techniques,\\nparticularly low-rank adapters and quantized low-rank adapters, to create a\\nsustainable and economically compliant framework for LLM utilization. The\\neconomic model it proposes benefits content creators, AI developers, and\\nend-users, delineating a harmonious integration of technology, economy, and\\nlaw, offering a comprehensive solution to the complex challenges of today's AI\\nlandscape.\"})]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "91d53d2c-bae2-477e-98c1-ac399438bc84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ‚ùì Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "Because of the context window for the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "e5b4adeb-ff47-40c9-cb7e-9f8a7e792bb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Among these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-'),\n",
              " Document(page_content='the computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4')]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ‚ùì Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "Yes it does matter. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "27e7b28a-a840-421c-bab7-95d29b9c243f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that is made widely and easily accessible by large corporations that do not release models or source code for auditing.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "13ce411f-2756-4815-93c0-2dd15b2de778"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "import getpass\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "6d34a358-2587-4510-b4bc-ddbc6dcd9b3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that is widely accessible and easy to use, even in the hands of large corporations that do not release models or source code for auditing. It is based on the concept of using low-rank matrices in conjunction with quantization techniques to reduce the computational requirements of LLMs, making them more efficient and easier to fine-tune.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "299 tokens\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "5.11 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "14ff5de8-0a5e-4425-908d-e03e3da8aa0c"
      },
      "outputs": [],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=False,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v3\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
